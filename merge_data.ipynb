{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import mygene\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# MyGene API test\n",
    "\n",
    "Manual MyGene API test for checking if data is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check mygene for specific gene IDs\n",
    "def check_mygene(gene_ids):\n",
    "    mg = mygene.MyGeneInfo()\n",
    "    results = mg.querymany(gene_ids, scopes='ensemblgene', fields='symbol', species='human')\n",
    "    # return {result['query']: result.get('symbol', None) for result in results}\n",
    "    return results\n",
    "\n",
    "genes = [\"ENSG00000223972\",\"ENSG00000227232\",\"ENSG00000278267\",\"ENSG00000243485\",\"ENSG00000237613\"]\n",
    "genes2 = [\"ENSG00000000003\",\"ENSG00000000005\",\"ENSG00000000419\",\"ENSG00000000457\",\"ENSG00000000460\"]\n",
    "gene_results = check_mygene(genes)\n",
    "gene_results2 = check_mygene(genes2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Gene symbols found:\")\n",
    "for res in gene_results:\n",
    "    print(res)\n",
    "    \n",
    "print(\"\\n-================-\")    \n",
    "\n",
    "for res in gene_results2:\n",
    "    print(res)\n",
    "    \n",
    "# Use getgenes to fetch gene information\n",
    "# def getgenes(gene_ids):\n",
    "#     mg = mygene.MyGeneInfo()\n",
    "#     results = mg.getgenes(gene_ids, fields='symbol')\n",
    "#     return results\n",
    "\n",
    "# Example usage of getgenes\n",
    "# gene_info = getgenes(genes)\n",
    "# print(\"Gene information fetched:\")\n",
    "# for gene in gene_info:\n",
    "#     print(f\"Gene ID: {gene['query']}, Symbol: {gene.get('symbol', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Test ordering of columns while preserving sample data.\n",
    "\n",
    "Testing if ordering the columns preserves the sample data correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({\n",
    "    'sample_id': ['patient1', 'patient2', 'patient3'],\n",
    "    'ENSG00000227232.5': [40, 50, 60],\n",
    "    'ENSG00000000003.15': [100, 200, 300],\n",
    "    'ENSG00000278267.1': [7, 8, 9]\n",
    "})\n",
    "\n",
    "print(\"=== ORIGINAL DATAFRAME ===\")\n",
    "print(test_df)\n",
    "\n",
    "gene_column_prefix = \"ENSG\"\n",
    "original_to_base = {}\n",
    "base_gene_ids = set()\n",
    "\n",
    "for column in test_df.columns:\n",
    "    if isinstance(column, str) and column.startswith(gene_column_prefix):\n",
    "        base_gene_id = column.split('.')[0]\n",
    "        if base_gene_id not in base_gene_ids:\n",
    "            original_to_base[column] = base_gene_id\n",
    "            base_gene_ids.add(base_gene_id)\n",
    "\n",
    "renamed_df = test_df.rename(columns=original_to_base)\n",
    "\n",
    "non_gene_cols = [col for col in renamed_df.columns \n",
    "                if not (isinstance(col, str) and col.startswith(gene_column_prefix))]\n",
    "common_gene_base_ids = sorted(base_gene_ids)\n",
    "final_columns = non_gene_cols + common_gene_base_ids\n",
    "reordered_df = renamed_df[final_columns].copy()\n",
    "\n",
    "print(\"\\n=== FINAL REORDERED DATAFRAME ===\")\n",
    "print(reordered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Convert .gct to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = 'data/GTEx_Analysis_2022-06-06_v10_RNASeQCv2.4.2_gene_reads.gct'\n",
    "output_file = 'data/healthy_data.csv'\n",
    "chunk_size = 8000\n",
    "first_chunk = True\n",
    "\n",
    "for chunk in pd.read_csv(dataset_file, sep='\\t', skiprows=2, chunksize=chunk_size):\n",
    "    if first_chunk:\n",
    "        # For the first chunk, write with header\n",
    "        chunk.to_csv(output_file, index=False, mode='w')\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        # For subsequent chunks, append without header\n",
    "        chunk.to_csv(output_file, index=False, mode='a', header=False)\n",
    "    print(f\"Processed and wrote a chunk of {len(chunk)} rows to {output_file}\")\n",
    "\n",
    "print(f\"Successfully converted {dataset_file} to {output_file} by processing in chunks.\")\n",
    "\n",
    "# Delete the original GCT file to save space\n",
    "if os.path.exists(dataset_file):\n",
    "    os.remove(dataset_file)\n",
    "    print(f\"Deleted the original GCT file: {dataset_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Transpose dataset\n",
    "\n",
    "Flip dataset so that the rows become the columns and vice versa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv_path = 'data/healthy_data_processed.csv'\n",
    "output_transposed_csv_path = 'data/healthy_data_transposed.csv'\n",
    "\n",
    "print(f\"Starting transposition of {input_csv_path} to {output_transposed_csv_path}...\")\n",
    "\n",
    "# 1. Get gene IDs (from the 'Name' column, these will be the new header columns)\n",
    "# Reads the first column 'Name' which contains gene Ensembl IDs\n",
    "gene_ids_series = pd.read_csv(input_csv_path, usecols=['Name']).iloc[:, 0]\n",
    "gene_ids_list = gene_ids_series.tolist()\n",
    "print(f\"Read {len(gene_ids_list)} gene IDs to be used as columns.\")\n",
    "\n",
    "# 2. Get sample IDs (these are the current column headers, excluding 'Name' and 'Description')\n",
    "# Read only the header row of the input CSV to get column names\n",
    "header_df = pd.read_csv(input_csv_path, nrows=0)\n",
    "original_column_names = header_df.columns.tolist()\n",
    "# The first two columns are 'Name' and 'Description', the rest are sample IDs\n",
    "sample_ids_list = original_column_names[2:] # Assuming 'Name' is first, 'Description' is second\n",
    "print(f\"Found {len(sample_ids_list)} sample IDs to be used as rows.\")\n",
    "\n",
    "# 3. Write the new transposed CSV\n",
    "with open(output_transposed_csv_path, 'w', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    # Write the header for the transposed file: 'sample_id' followed by all gene IDs\n",
    "    transposed_header = ['sample_id'] + gene_ids_list\n",
    "    writer.writerow(transposed_header)\n",
    "    print(\"Written header to output file.\")\n",
    "\n",
    "    # 4. Iterate through sample IDs (original columns) in chunks for memory efficiency\n",
    "    sample_chunk_size = 3000  # Adjust based on memory; number of samples to process at once\n",
    "    num_samples = len(sample_ids_list)\n",
    "\n",
    "    for i in range(0, num_samples, sample_chunk_size):\n",
    "        current_sample_ids_chunk = sample_ids_list[i:i + sample_chunk_size]\n",
    "        \n",
    "        if not current_sample_ids_chunk:\n",
    "            continue\n",
    "\n",
    "        chunk_df = pd.read_csv(input_csv_path, usecols=current_sample_ids_chunk)\n",
    "\n",
    "        for sample_id in current_sample_ids_chunk:\n",
    "            sample_values_list = chunk_df[sample_id].tolist()\n",
    "            \n",
    "            output_row = [sample_id] + sample_values_list\n",
    "            writer.writerow(output_row)\n",
    "        \n",
    "        print(f\"Processed and wrote samples from index {i} to {min(i + sample_chunk_size - 1, num_samples - 1)}\")\n",
    "\n",
    "print(f\"Successfully transposed {input_csv_path} to {output_transposed_csv_path}\")\n",
    "\n",
    "# Delete the original GCT file to save space\n",
    "if os.path.exists(input_csv_path):\n",
    "    os.remove(input_csv_path)\n",
    "    print(f\"Deleted the original GCT file: {input_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Load CSV in chunks\n",
    "\n",
    "Use this method to load the csv datasets in chunks so that it doesn't crash the devcontainer :)\n",
    "\n",
    "Also use this to get the `chunk_iterator` for the dataset and use that in combination with the other methods created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_in_chunks(file_path, chunk_size=8000, **kwargs):\n",
    "    \"\"\"\n",
    "    Loads a CSV file in chunks to avoid memory issues.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "        chunk_size (int): The number of rows per chunk.\n",
    "        **kwargs: Additional keyword arguments to pass to pd.read_csv()\n",
    "                  (e.g., sep=',', header=0, index_col=None, usecols=None).\n",
    "\n",
    "    Returns:\n",
    "        A pandas TextFileReader object (iterator) that yields DataFrame chunks\n",
    "        if the file exists, otherwise None.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Preparing to load {file_path} in chunks of size {chunk_size}...\")\n",
    "    try:\n",
    "        chunk_iterator = pd.read_csv(file_path, chunksize=chunk_size, **kwargs)\n",
    "        return chunk_iterator\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV file {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Save data to new csv file\n",
    "\n",
    "Use this to save the preprocessed data as a new csv file, can be a pandas `dataframe` or a `chunk_iterator`.\n",
    "\n",
    "Optional: Can also delete old csv file. Can be usefull for final dataset when we know everything is setup properly. Otherwise, keep old dataset so that we work in a sort of non-destructive way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_csv(data_to_save, output_file_path, delete_original_path=False, index=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Saves a pandas DataFrame or an iterator of DataFrame chunks to a CSV file.\n",
    "    Optionally deletes the original file after successful saving.\n",
    "\n",
    "    Args:\n",
    "        data_to_save (pd.DataFrame or iterator): DataFrame or iterator yielding DataFrames.\n",
    "        output_file_path (str): Path to save the new CSV file.\n",
    "        delete_original_path (str, optional): Path to the original file to delete. Defaults to False.\n",
    "        index (bool, optional): Whether to write DataFrame index. Defaults to False.\n",
    "        **kwargs: Additional keyword arguments to pass to df.to_csv().\n",
    "    \"\"\"\n",
    "    mode = kwargs.pop('mode', 'w')\n",
    "    header = kwargs.pop('header', True)\n",
    "\n",
    "    try:\n",
    "        if isinstance(data_to_save, pd.DataFrame):\n",
    "            print(f\"Saving DataFrame to {output_file_path}...\")\n",
    "            data_to_save.to_csv(output_file_path, index=index, mode=mode, header=header, **kwargs)\n",
    "        else:\n",
    "            print(f\"Saving data in chunks to {output_file_path}...\")\n",
    "            for i, chunk_df in enumerate(data_to_save):\n",
    "                chunk_mode = 'w' if i == 0 else 'a'\n",
    "                chunk_header = header if i == 0 else False\n",
    "                \n",
    "                chunk_df.to_csv(output_file_path, index=index, mode=chunk_mode, header=chunk_header, **kwargs)\n",
    "                \n",
    "                if i == 0:\n",
    "                    print(f\"Written first chunk to {output_file_path}\")\n",
    "                else:\n",
    "                    print(f\"Appended chunk {i+1} to {output_file_path}\")\n",
    "        \n",
    "        print(f\"Successfully saved data to {output_file_path}\")\n",
    "        \n",
    "        # Handle file deletion if requested\n",
    "        if delete_original_path and delete_original_path != output_file_path:\n",
    "            if os.path.exists(delete_original_path):\n",
    "                os.remove(delete_original_path)\n",
    "                print(f\"Successfully deleted original file: {delete_original_path}\")\n",
    "            else:\n",
    "                print(f\"Warning: Original file not found at {delete_original_path}\")\n",
    "        elif delete_original_path == output_file_path:\n",
    "            print(f\"Warning: Original and output paths are the same. Original file not deleted.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to {output_file_path}: {e}\")\n",
    "        if os.path.exists(output_file_path):\n",
    "            print(f\"Partial data might have been written to {output_file_path}.\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Modify Dataframe\n",
    "\n",
    "Use this method to perform different dataframe manipulations in a memory safe way.\n",
    "\n",
    "\n",
    "`drop_row`: Drops a row by its index label.\n",
    "\n",
    "\n",
    "`column_header`: Renames a column.\n",
    "\n",
    "\n",
    "`index_header`: Sets/renames the index name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_dataframe_element(df_chunk, new_value, row_label=None, old_name=None, element_type=None):\n",
    "    \"\"\"\n",
    "    Modifies DataFrame elements in a memory-safe way.\n",
    "    \n",
    "    Args:\n",
    "        df_chunk (pd.DataFrame): The DataFrame chunk to modify.\n",
    "        new_value (any): The new value or name to set.\n",
    "        row_label (any, optional): Row index label for 'drop_row' operation.\n",
    "        old_name (any, optional): Current column/index name for reference.\n",
    "        element_type (str): Type of modification:\n",
    "                           'drop_row': Drops row by index label\n",
    "                           'column_header': Renames column \n",
    "                           'index_header': Sets/renames index name\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Modified DataFrame chunk.\n",
    "    \"\"\"\n",
    "    \n",
    "    if element_type == 'drop_row':\n",
    "        if row_label is None:\n",
    "            print(\"Error: 'row_label' required for 'drop_row' operation.\")\n",
    "            return df_chunk\n",
    "        if row_label not in df_chunk.index:\n",
    "            print(f\"Warning: Row '{row_label}' not found. No changes made.\")\n",
    "            return df_chunk\n",
    "        print(f\"Dropping row: {row_label}\")\n",
    "        return df_chunk.drop(index=row_label)\n",
    "    \n",
    "    elif element_type == 'column_header':\n",
    "        if old_name is None:\n",
    "            print(\"Error: 'old_name' required for 'column_header' operation.\")\n",
    "            return df_chunk\n",
    "        if old_name not in df_chunk.columns:\n",
    "            print(f\"Warning: Column '{old_name}' not found. No changes made.\")\n",
    "            return df_chunk\n",
    "        print(f\"Renaming column: '{old_name}' → '{new_value}'\")\n",
    "        df_chunk.rename(columns={old_name: new_value}, inplace=True)\n",
    "        return df_chunk\n",
    "    \n",
    "    elif element_type == 'index_header':\n",
    "        current_name = df_chunk.index.name\n",
    "        print(f\"Setting index name: '{current_name}' → '{new_value}'\")\n",
    "        df_chunk.index.name = new_value\n",
    "        return df_chunk\n",
    "    \n",
    "    else:\n",
    "        print(f\"Error: Invalid element_type '{element_type}'. Use: 'drop_row', 'column_header', or 'index_header'.\")\n",
    "        return df_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Convert gene IDs to corrosponding symbol\n",
    "\n",
    "Use this to remove the version suffix from the gene IDs (.XX) and convert the gene IDs to the symbols.\n",
    "\n",
    "We could keep using the gene IDs without the version suffix, but in general the symbols are better for readibility and should be more consistent between GENCODE versions.\n",
    "\n",
    "Also removes duplicate genes found in the dataset, only keeping the first and removing the columns of the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_gene_columns_simple(healthy_chunk_iterator, unhealthy_chunk_iterator, \n",
    "                              gene_column_prefix=\"ENSG\"):\n",
    "    \"\"\"\n",
    "    Aligns gene columns between healthy and unhealthy datasets.\n",
    "    \n",
    "    Args:\n",
    "        healthy_chunk_iterator: Iterator for healthy dataset chunks\n",
    "        unhealthy_chunk_iterator: Iterator for unhealthy dataset chunks\n",
    "        gene_column_prefix (str): Prefix to identify gene columns (default: \"ENSG\")\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (aligned_healthy_chunks, aligned_unhealthy_chunks, alignment_info)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting gene column alignment...\")\n",
    "    \n",
    "    print(\"\\nLoading first chunks from both datasets...\")\n",
    "    try:\n",
    "        first_healthy_chunk = next(healthy_chunk_iterator)\n",
    "        print(f\"   ✓ Loaded healthy dataset first chunk: {first_healthy_chunk.shape}\")\n",
    "        \n",
    "        first_unhealthy_chunk = next(unhealthy_chunk_iterator)\n",
    "        print(f\"   ✓ Loaded unhealthy dataset first chunk: {first_unhealthy_chunk.shape}\")\n",
    "        \n",
    "    except StopIteration:\n",
    "        print(\"ERROR: One or both datasets are empty!\")\n",
    "        return [], [], {}\n",
    "    \n",
    "\n",
    "    print(\"\\nProcessing gene columns and stripping version suffixes...\")\n",
    "    def extract_gene_info(chunk, dataset_name):\n",
    "        \"\"\"Extract gene columns and create mapping from original_column to base_id\"\"\"\n",
    "\n",
    "        print(f\"Processing {dataset_name} dataset...\")\n",
    "\n",
    "        original_to_base = {}\n",
    "        base_gene_ids = set()\n",
    "        total_gene_columns = 0\n",
    "        duplicate_count = 0\n",
    "        \n",
    "        for column in chunk.columns:\n",
    "            if isinstance(column, str) and column.startswith(gene_column_prefix):\n",
    "                total_gene_columns += 1\n",
    "                base_gene_id = column.split('.')[0]\n",
    "                \n",
    "                # Handle duplicates (keep first occurrence)\n",
    "                if base_gene_id not in base_gene_ids:\n",
    "                    original_to_base[column] = base_gene_id\n",
    "                    base_gene_ids.add(base_gene_id)\n",
    "                else:\n",
    "                    duplicate_count += 1\n",
    "                    print(f\"Warning: Duplicate base gene {base_gene_id} found, skipping {column}\")\n",
    "        \n",
    "        print(f\"Total gene columns found: {total_gene_columns}\")\n",
    "        print(f\"Unique base gene IDs: {len(base_gene_ids)}\")\n",
    "        if duplicate_count > 0:\n",
    "            print(f\"Duplicates removed: {duplicate_count}\")\n",
    "\n",
    "        return original_to_base, base_gene_ids\n",
    "    \n",
    "    # Process both datasets\n",
    "    healthy_rename_mapping, healthy_base_genes = extract_gene_info(first_healthy_chunk, \"HEALTHY\")\n",
    "    unhealthy_rename_mapping, unhealthy_base_genes = extract_gene_info(first_unhealthy_chunk, \"UNHEALTHY\")\n",
    "\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"   Healthy dataset: {len(healthy_base_genes)} unique genes\")\n",
    "    print(f\"   Unhealthy dataset: {len(unhealthy_base_genes)} unique genes\")\n",
    "\n",
    "    print(\"\\nFinding common genes between datasets...\")\n",
    "    common_base_genes = healthy_base_genes & unhealthy_base_genes\n",
    "\n",
    "    if not common_base_genes:\n",
    "        print(\"ERROR: No common genes found between datasets!\")\n",
    "        return [], [], {}\n",
    "\n",
    "    print(f\"Common genes: {len(common_base_genes)}\")\n",
    "    print(f\"Genes exclusive to healthy dataset: {len(healthy_base_genes - common_base_genes)}\")\n",
    "    print(f\"Genes exclusive to unhealthy dataset: {len(unhealthy_base_genes - common_base_genes)}\")\n",
    "\n",
    "    print(\"\\nProcessing all chunks and renaming to base gene IDs...\")\n",
    "\n",
    "    def process_all_chunks(chunk_iterator, first_chunk, rename_mapping, common_genes, dataset_name):\n",
    "        \"\"\"Process all chunks: rename gene columns to base IDs and keep only common genes\"\"\"\n",
    "        print(f\"   Processing {dataset_name} dataset chunks...\")\n",
    "        \n",
    "        # Rename columns in first chunk\n",
    "        renamed_first_chunk = first_chunk.rename(columns=rename_mapping)\n",
    "        \n",
    "        # Build final column list\n",
    "        non_gene_cols = [col for col in renamed_first_chunk.columns \n",
    "                        if not (isinstance(col, str) and col.startswith(gene_column_prefix))]\n",
    "        common_gene_base_ids = sorted(common_genes)  # Sorted base gene IDs\n",
    "        final_columns = non_gene_cols + common_gene_base_ids\n",
    "        \n",
    "        print(f\"      Non-gene columns: {len(non_gene_cols)}\")\n",
    "        print(f\"      Common gene columns (base IDs): {len(common_gene_base_ids)}\")\n",
    "        print(f\"      Total columns to keep: {len(final_columns)}\")\n",
    "        \n",
    "        # Keep only desired columns\n",
    "        aligned_chunks = [renamed_first_chunk[final_columns].copy()]\n",
    "        chunk_count = 1\n",
    "        print(f\"✓ Processed chunk 1 - shape: {aligned_chunks[0].shape}\")\n",
    "\n",
    "        # Process remaining chunks\n",
    "        for chunk in chunk_iterator:\n",
    "            renamed_chunk = chunk.rename(columns=rename_mapping)\n",
    "            aligned_chunks.append(renamed_chunk[final_columns].copy())\n",
    "            chunk_count += 1\n",
    "            \n",
    "            if chunk_count % 3 == 0:\n",
    "                print(f\"✓ Processed {chunk_count} chunks so far...\")\n",
    "\n",
    "        print(f\"{dataset_name} processing complete: {chunk_count} chunks processed\")\n",
    "        return aligned_chunks\n",
    "\n",
    "    # Process both datasets\n",
    "    print(f\"\\nProcessing HEALTHY dataset...\")\n",
    "    aligned_healthy = process_all_chunks(healthy_chunk_iterator, first_healthy_chunk,\n",
    "                                    healthy_rename_mapping, common_base_genes, \"HEALTHY\")\n",
    "\n",
    "    print(f\"\\nProcessing UNHEALTHY dataset...\")\n",
    "    aligned_unhealthy = process_all_chunks(unhealthy_chunk_iterator, first_unhealthy_chunk,\n",
    "                                        unhealthy_rename_mapping, common_base_genes, \"UNHEALTHY\")\n",
    "\n",
    "    alignment_info = {\n",
    "        'total_healthy_genes_original': len(healthy_rename_mapping),\n",
    "        'total_unhealthy_genes_original': len(unhealthy_rename_mapping),\n",
    "        'common_genes': len(common_base_genes),\n",
    "        'genes_only_in_healthy': len(healthy_base_genes - common_base_genes),\n",
    "        'genes_only_in_unhealthy': len(unhealthy_base_genes - common_base_genes),\n",
    "        'healthy_chunks_processed': len(aligned_healthy),\n",
    "        'unhealthy_chunks_processed': len(aligned_unhealthy),\n",
    "        'final_healthy_columns': len([col for col in aligned_healthy[0].columns if not isinstance(col, str) or not col.startswith(gene_column_prefix)]) + len(common_base_genes),\n",
    "        'final_unhealthy_columns': len([col for col in aligned_unhealthy[0].columns if not isinstance(col, str) or not col.startswith(gene_column_prefix)]) + len(common_base_genes)\n",
    "    }\n",
    "\n",
    "    print(\"\\nAlignment complete!\")\n",
    "    print(\"=\" * 25)\n",
    "    print(f\"Final Summary:\")\n",
    "    print(f\"Original genes - Healthy: {alignment_info['total_healthy_genes_original']}\")\n",
    "    print(f\"Original genes - Unhealthy: {alignment_info['total_unhealthy_genes_original']}\")\n",
    "    print(f\"Common genes kept: {alignment_info['common_genes']}\")\n",
    "    print(f\"Healthy chunks processed: {alignment_info['healthy_chunks_processed']}\")\n",
    "    print(f\"Unhealthy chunks processed: {alignment_info['unhealthy_chunks_processed']}\")\n",
    "    print(\"=\" * 25)\n",
    "\n",
    "    return aligned_healthy, aligned_unhealthy, alignment_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_gene_columns_between_datasets(healthy_chunk_iterator, unhealthy_chunk_iterator, \n",
    "                                       gene_column_prefix=\"ENSG\"):\n",
    "    \"\"\"\n",
    "    Aligns gene columns between healthy and unhealthy datasets by keeping only\n",
    "    genes present in both datasets. Handles GENCODE version differences.\n",
    "    \n",
    "    Args:\n",
    "        healthy_chunk_iterator: Iterator yielding healthy DataFrame chunks\n",
    "        unhealthy_chunk_iterator: Iterator yielding unhealthy DataFrame chunks  \n",
    "        gene_column_prefix (str): Prefix to identify gene columns (e.g., \"ENSG\")\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (aligned_healthy_chunks, aligned_unhealthy_chunks, alignment_info)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting gene column alignment...\")\n",
    "    \n",
    "    # Get first chunks\n",
    "    try:\n",
    "        first_healthy_chunk = next(healthy_chunk_iterator)\n",
    "        first_unhealthy_chunk = next(unhealthy_chunk_iterator)\n",
    "    except StopIteration:\n",
    "        print(\"Error: One or both datasets are empty\")\n",
    "        return [], [], {}\n",
    "    \n",
    "    def extract_and_rename_gene_columns(chunk, dataset_name):\n",
    "        \"\"\"Extract gene columns, strip version suffixes, and rename columns\"\"\"\n",
    "        gene_mapping = {}  # Maps base_gene_id -> original_column_name\n",
    "        column_rename_mapping = {}  # Maps original_column_name -> base_gene_id\n",
    "        \n",
    "        for column in chunk.columns:\n",
    "            if isinstance(column, str) and column.startswith(gene_column_prefix):\n",
    "                base_gene_id = column.split('.')[0]  # Remove version suffix\n",
    "                \n",
    "                if base_gene_id not in gene_mapping:\n",
    "                    gene_mapping[base_gene_id] = column\n",
    "                    column_rename_mapping[column] = base_gene_id  # NEW: Map to base ID\n",
    "                else:\n",
    "                    print(f\"Warning: Duplicate gene {base_gene_id} in {dataset_name} dataset\")\n",
    "        \n",
    "        return gene_mapping, column_rename_mapping\n",
    "    \n",
    "    healthy_genes, healthy_rename_map = extract_and_rename_gene_columns(first_healthy_chunk, \"healthy\")\n",
    "    unhealthy_genes, unhealthy_rename_map = extract_and_rename_gene_columns(first_unhealthy_chunk, \"unhealthy\")\n",
    "    \n",
    "    print(f\"Healthy dataset: {len(healthy_genes)} unique genes\")\n",
    "    print(f\"Unhealthy dataset: {len(unhealthy_genes)} unique genes\")\n",
    "    \n",
    "    # Find common genes (intersection)\n",
    "    common_genes = set(healthy_genes.keys()) & set(unhealthy_genes.keys())\n",
    "    \n",
    "    \n",
    "    if not common_genes:\n",
    "        print(\"Error: No common genes found between datasets!\")\n",
    "        return [], [], {}\n",
    "    \n",
    "    print(f\"Common genes found: {len(common_genes)}\")\n",
    "    \n",
    "    def process_chunks_with_renaming(chunk_iterator, first_chunk, rename_map, common_genes, dataset_name):\n",
    "        \"\"\"Process chunks: rename columns to base gene IDs and keep only common genes\"\"\"\n",
    "        # Rename columns in first chunk\n",
    "        renamed_chunk = first_chunk.rename(columns=rename_map)\n",
    "        \n",
    "        # Keep non-gene columns + common gene columns\n",
    "        non_gene_cols = [col for col in renamed_chunk.columns \n",
    "                        if not (isinstance(col, str) and col.startswith(gene_column_prefix))]\n",
    "        common_gene_cols = [gene_id for gene_id in sorted(common_genes)]\n",
    "        final_columns = non_gene_cols + common_gene_cols\n",
    "        \n",
    "        aligned_chunks = [renamed_chunk[final_columns].copy()]\n",
    "        chunk_count = 1\n",
    "        \n",
    "        for chunk in chunk_iterator:\n",
    "            renamed_chunk = chunk.rename(columns=rename_map)\n",
    "            aligned_chunks.append(renamed_chunk[final_columns].copy())\n",
    "            chunk_count += 1\n",
    "            \n",
    "            if chunk_count % 5 == 0:\n",
    "                print(f\"  Processed {chunk_count} {dataset_name} chunks...\")\n",
    "        \n",
    "        print(f\"  Completed {chunk_count} {dataset_name} chunks\")\n",
    "        return aligned_chunks\n",
    "    \n",
    "    print(\"Processing healthy dataset chunks...\")\n",
    "    aligned_healthy = process_chunks_with_renaming(\n",
    "        healthy_chunk_iterator, first_healthy_chunk, healthy_rename_map, common_genes, \"healthy\"\n",
    "    )\n",
    "    \n",
    "    print(\"Processing unhealthy dataset chunks...\")\n",
    "    aligned_unhealthy = process_chunks_with_renaming(\n",
    "        unhealthy_chunk_iterator, first_unhealthy_chunk, unhealthy_rename_map, common_genes, \"unhealthy\"\n",
    "    )\n",
    "    \n",
    "    # Generate summary statistics\n",
    "    alignment_info = {\n",
    "        'total_healthy_genes': len(healthy_genes),\n",
    "        'total_unhealthy_genes': len(unhealthy_genes),\n",
    "        'common_genes': len(common_genes),\n",
    "        'genes_only_in_healthy': len(healthy_genes) - len(common_genes),\n",
    "        'genes_only_in_unhealthy': len(unhealthy_genes) - len(common_genes),\n",
    "        'healthy_chunks_processed': len(aligned_healthy),\n",
    "        'unhealthy_chunks_processed': len(aligned_unhealthy)\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== Gene Alignment Summary ===\")\n",
    "    print(f\"✓ Common genes: {alignment_info['common_genes']}\")\n",
    "    print(f\"• Healthy-only genes: {alignment_info['genes_only_in_healthy']}\")\n",
    "    print(f\"• Unhealthy-only genes: {alignment_info['genes_only_in_unhealthy']}\")\n",
    "    print(\"=============================\\n\")\n",
    "    \n",
    "    return aligned_healthy, aligned_unhealthy, alignment_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_gene_columns(chunk_iterator, mygene_client, id_column_prefix=\"ENSG\"):\n",
    "    \"\"\"\n",
    "    Convert gene IDs to symbols using corrected MyGene API call.\n",
    "    \"\"\"\n",
    "    first_chunk = next(chunk_iterator)\n",
    "    original_columns = first_chunk.columns.tolist()\n",
    "    \n",
    "    # Find gene columns (should already be base IDs without version suffixes)\n",
    "    gene_columns = [col for col in original_columns \n",
    "                   if isinstance(col, str) and col.startswith(id_column_prefix)]\n",
    "    \n",
    "    if not gene_columns:\n",
    "        print(\"No gene columns found for symbol conversion\")\n",
    "        return pd.concat([first_chunk] + list(chunk_iterator), ignore_index=False)\n",
    "    \n",
    "    print(f\"Converting {len(gene_columns)} gene IDs to symbols...\")\n",
    "    gene_symbols = {}\n",
    "    \n",
    "    try:\n",
    "        results = mygene_client.querymany(\n",
    "            gene_columns, \n",
    "            scopes='ensembl.gene',\n",
    "            fields='symbol', \n",
    "            species='human', \n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Process results\n",
    "        for result in results:\n",
    "            if 'symbol' in result and result['symbol']:\n",
    "                gene_symbols[result['query']] = result['symbol']\n",
    "            else:\n",
    "                # Keep original ID if no symbol found\n",
    "                gene_symbols[result['query']] = result['query']\n",
    "                print(f\"No symbol found for {result['query']}, keeping original ID\")\n",
    "                \n",
    "        print(f\"Successfully converted {len([v for v in gene_symbols.values() if not v.startswith('ENSG')])} genes to symbols\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"MyGene API error: {e}\")\n",
    "        print(\"Using original gene IDs as fallback\")\n",
    "        gene_symbols = {gid: gid for gid in gene_columns}\n",
    "    \n",
    "    # Build column mapping\n",
    "    column_mapping = {}\n",
    "    for col in original_columns:\n",
    "        if col in gene_columns:\n",
    "            column_mapping[col] = gene_symbols.get(col, col)\n",
    "        else:\n",
    "            column_mapping[col] = col\n",
    "    \n",
    "    # Apply mapping and combine chunks\n",
    "    print(\"Applying symbol mapping to all chunks...\")\n",
    "    all_chunks = [first_chunk.rename(columns=column_mapping)]\n",
    "    \n",
    "    for chunk in chunk_iterator:\n",
    "        all_chunks.append(chunk.rename(columns=column_mapping))\n",
    "    \n",
    "    final_df = pd.concat(all_chunks, ignore_index=False)\n",
    "    print(f\"Final dataset shape: {final_df.shape}\")\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_condition_labels_to_chunks(chunk_iterator, condition_label, dataset_name):\n",
    "    \"\"\"\n",
    "    Add condition labels to dataset chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunk_iterator: Iterator yielding DataFrame chunks\n",
    "        condition_label (int): Binary label (0 for healthy, 1 for unhealthy)\n",
    "        dataset_name (str): Name for logging purposes\n",
    "    \n",
    "    Returns:\n",
    "        list: List of labeled DataFrame chunks\n",
    "    \"\"\"\n",
    "    print(f\"Adding label '{condition_label}' to {dataset_name} dataset...\")\n",
    "    \n",
    "    labeled_chunks = []\n",
    "    for chunk in chunk_iterator:\n",
    "        chunk['condition'] = condition_label\n",
    "        labeled_chunks.append(chunk)\n",
    "    \n",
    "    print(f\"✅ Completed {len(labeled_chunks)} {dataset_name} chunks\")\n",
    "    return labeled_chunks\n",
    "\n",
    "def merge_labeled_datasets(healthy_chunks, unhealthy_chunks):\n",
    "    \"\"\"\n",
    "    Merge healthy and unhealthy dataset chunks into one DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        healthy_chunks (list): List of healthy DataFrame chunks\n",
    "        unhealthy_chunks (list): List of unhealthy DataFrame chunks\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Merged dataset\n",
    "    \"\"\"\n",
    "    print(\"Merging datasets...\")\n",
    "    \n",
    "    all_chunks = healthy_chunks + unhealthy_chunks\n",
    "    merged_dataset = pd.concat(all_chunks, axis=0, ignore_index=False)\n",
    "    \n",
    "    print(f\"✅ Merged dataset: {len(merged_dataset)} samples, {len(merged_dataset.columns)} features\")\n",
    "    print(f\"   Healthy (0): {(merged_dataset['condition'] == 0).sum()}\")\n",
    "    print(f\"   Unhealthy (1): {(merged_dataset['condition'] == 1).sum()}\")\n",
    "    \n",
    "    return merged_dataset\n",
    "\n",
    "def convert_gene_ids_to_symbols(dataset_chunk_iterator, mygene_client, gene_column_prefix=\"ENSG\"):\n",
    "    \"\"\"\n",
    "    Convert gene IDs to gene symbols using MyGene API in a memory-efficient way.\n",
    "    Only calls MyGene API once for all gene columns from the first chunk.\n",
    "    \n",
    "    Args:\n",
    "        dataset_chunk_iterator: Iterator yielding DataFrame chunks\n",
    "        mygene_client: MyGene client instance\n",
    "        gene_column_prefix (str): Prefix to identify gene columns\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Complete dataset with gene symbols as column names\n",
    "    \"\"\"\n",
    "    print(\"Converting gene IDs to symbols (chunked processing)...\")\n",
    "    \n",
    "    # Get first chunk to determine gene columns and create mapping\n",
    "    try:\n",
    "        first_chunk = next(dataset_chunk_iterator)\n",
    "        print(f\"Processing first chunk: {first_chunk.shape}\")\n",
    "    except StopIteration:\n",
    "        print(\"❌ Error: Dataset is empty\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Identify gene columns from first chunk\n",
    "    gene_columns = [col for col in first_chunk.columns \n",
    "                   if isinstance(col, str) and col.startswith(gene_column_prefix)]\n",
    "    \n",
    "    if not gene_columns:\n",
    "        print(\"No gene columns found - returning original data\")\n",
    "        # Concatenate all chunks and return\n",
    "        all_chunks = [first_chunk]\n",
    "        for chunk in dataset_chunk_iterator:\n",
    "            all_chunks.append(chunk)\n",
    "        return pd.concat(all_chunks, axis=0, ignore_index=False)\n",
    "    \n",
    "    print(f\"Found {len(gene_columns)} gene columns\")\n",
    "    \n",
    "    # Single MyGene API call for all gene IDs\n",
    "    gene_id_to_symbol = {}\n",
    "    symbols_found = 0\n",
    "    \n",
    "    try:\n",
    "        print(\"Making single MyGene API call...\")\n",
    "        import warnings\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            results = mygene_client.querymany(\n",
    "                gene_columns,\n",
    "                scopes='ensembl.gene',\n",
    "                fields='symbol',\n",
    "                species='human',\n",
    "                verbose=False,\n",
    "                silent=True\n",
    "            )\n",
    "        \n",
    "        # Process API results\n",
    "        for result in results:\n",
    "            gene_id = result['query']\n",
    "            if 'symbol' in result and result['symbol']:\n",
    "                gene_id_to_symbol[gene_id] = result['symbol']\n",
    "                symbols_found += 1\n",
    "            else:\n",
    "                gene_id_to_symbol[gene_id] = gene_id\n",
    "        \n",
    "        print(f\"✅ Successfully converted {symbols_found}/{len(gene_columns)} genes to symbols\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ MyGene API error: {e}. Using original gene IDs\")\n",
    "        gene_id_to_symbol = {gene_id: gene_id for gene_id in gene_columns}\n",
    "    \n",
    "    # Create final column mapping (preserve non-gene columns like 'condition')\n",
    "    final_column_mapping = {}\n",
    "    for col in first_chunk.columns:\n",
    "        if col == 'condition':\n",
    "            final_column_mapping[col] = col\n",
    "        elif col in gene_id_to_symbol:\n",
    "            final_column_mapping[col] = gene_id_to_symbol[col]\n",
    "        else:\n",
    "            final_column_mapping[col] = col\n",
    "    \n",
    "    print(\"Applying gene symbol mapping to all chunks...\")\n",
    "    \n",
    "    # Process first chunk\n",
    "    renamed_first_chunk = first_chunk.rename(columns=final_column_mapping)\n",
    "    processed_chunks = [renamed_first_chunk]\n",
    "    chunk_count = 1\n",
    "    \n",
    "    # Process remaining chunks with same mapping\n",
    "    for chunk in dataset_chunk_iterator:\n",
    "        renamed_chunk = chunk.rename(columns=final_column_mapping)\n",
    "        processed_chunks.append(renamed_chunk)\n",
    "        chunk_count += 1\n",
    "        \n",
    "        if chunk_count % 3 == 0:\n",
    "            print(f\"  ✓ Processed {chunk_count} chunks...\")\n",
    "    \n",
    "    print(f\"Concatenating {chunk_count} processed chunks...\")\n",
    "    final_dataset = pd.concat(processed_chunks, axis=0, ignore_index=False)\n",
    "    \n",
    "    print(f\"✅ Final dataset shape: {final_dataset.shape}\")\n",
    "    print(f\"   Symbols converted: {symbols_found}/{len(gene_columns)}\")\n",
    "    \n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# Actual preprocessing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths for the healthy and unhealthy datasets\n",
    "unhealthy_data_path = 'data/rna_seq_unstranded.csv'\n",
    "output_unhealthy_processed_path = 'data/unhealthy_data_processed.csv'\n",
    "\n",
    "healthy_data_path = 'data/healthy_data_preprocessed.csv'\n",
    "output_healthy_processed_path = 'data/healthy_data_processed.csv'\n",
    "\n",
    "mg = mygene.MyGeneInfo() # Essential for process_gene_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Unhealthy Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the unhealthy dataset in chunks\n",
    "df_unhealthy_chunk_iterator = load_csv_in_chunks(\n",
    "    unhealthy_data_path,\n",
    "    header=0,         # Use the first row (Ensembl IDs) as header\n",
    "    skiprows=[1, 2],  # Skip the `gene_name` and `gene_type` rows of the original file\n",
    "    index_col=0       # `Sample IDs` becomes the index\n",
    ")\n",
    "\n",
    "# Process the unhealthy dataset chunks\n",
    "if df_unhealthy_chunk_iterator:\n",
    "    processed_unhealthy_chunks = []\n",
    "    for i, df_chunk in enumerate(df_unhealthy_chunk_iterator):\n",
    "        if df_chunk.index.name != 'sample_id':\n",
    "            df_chunk = modify_dataframe_element(df_chunk, new_value='sample_id', old_name=df_chunk.index.name, element_type='index_header')\n",
    "        processed_unhealthy_chunks.append(df_chunk)\n",
    "    \n",
    "    # if processed_unhealthy_chunks:\n",
    "    #     print(\"Efficiently processing unhealthy dataset gene columns with single MyGene API call...\")\n",
    "    #     df_unhealthy_processed = preprocess_gene_columns(\n",
    "    #         iter(processed_unhealthy_chunks), \n",
    "    #         mg, \n",
    "    #         id_column_prefix=\"ENSG\"\n",
    "    #     )\n",
    "        \n",
    "    #     print(f\"Unhealthy dataset processing complete. Final shape: {df_unhealthy_processed.shape}\")\n",
    "    save_data_to_csv(processed_unhealthy_chunks, output_unhealthy_processed_path, index=True)\n",
    "    # else:\n",
    "    #     print(\"No unhealthy data chunks were processed.\")\n",
    "else:\n",
    "    print(f\"Failed to load unhealthy data chunks from {unhealthy_data_path}. Check file path and format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Healthy Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Transpose the healthy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the healthy dataset\n",
    "current_input_csv_path = 'data/healthy_data_preprocessed.csv'\n",
    "current_output_transposed_csv_path = 'data/healthy_data_transformed.csv'\n",
    "feature_id_col_name = 'Name'\n",
    "columns_to_exclude = ['Description']\n",
    "\n",
    "print(f\"Starting transposition of {current_input_csv_path} to {current_output_transposed_csv_path} using preferred logic...\")\n",
    "\n",
    "try:\n",
    "    gene_ids_series = pd.read_csv(current_input_csv_path, usecols=[feature_id_col_name]).iloc[:, 0]\n",
    "    gene_ids_list = gene_ids_series.tolist()\n",
    "    print(f\"Read {len(gene_ids_list)} gene IDs to be used as columns.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error reading gene IDs: {e}. Ensure '{feature_id_col_name}' exists in '{current_input_csv_path}'.\")\n",
    "    gene_ids_list = []\n",
    "\n",
    "if gene_ids_list:\n",
    "    header_df = pd.read_csv(current_input_csv_path, nrows=0)\n",
    "    original_column_names = header_df.columns.tolist()\n",
    "\n",
    "    sample_ids_list = [\n",
    "        col for col in original_column_names\n",
    "        if col not in [feature_id_col_name] + columns_to_exclude\n",
    "    ]\n",
    "    print(f\"Found {len(sample_ids_list)} sample IDs to be used as rows.\")\n",
    "\n",
    "    if sample_ids_list:\n",
    "        with open(current_output_transposed_csv_path, 'w', newline='') as outfile:\n",
    "            writer = csv.writer(outfile)\n",
    "            transposed_header = ['sample_id'] + gene_ids_list\n",
    "            writer.writerow(transposed_header)\n",
    "            print(\"Written header to output file.\")\n",
    "\n",
    "            sample_processing_batch_size = 500\n",
    "            num_samples = len(sample_ids_list)\n",
    "\n",
    "            for i in range(0, num_samples, sample_processing_batch_size):\n",
    "                current_sample_ids_batch = sample_ids_list[i:i + sample_processing_batch_size]\n",
    "                \n",
    "                if not current_sample_ids_batch:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    batch_df = pd.read_csv(current_input_csv_path, usecols=current_sample_ids_batch)\n",
    "\n",
    "                    for sample_id_in_batch in current_sample_ids_batch:\n",
    "                        sample_values_list = batch_df[sample_id_in_batch].tolist()\n",
    "                        \n",
    "                        output_row = [sample_id_in_batch] + sample_values_list\n",
    "                        writer.writerow(output_row)\n",
    "                    \n",
    "                    print(f\"Processed and wrote samples from index {i} to {min(i + sample_processing_batch_size - 1, num_samples - 1)}\")\n",
    "                except Exception as e_batch:\n",
    "                    print(f\"Error processing batch of samples starting at index {i}: {e_batch}\")\n",
    "                    print(f\"Problematic sample IDs in batch might be: {current_sample_ids_batch[:5]}\")\n",
    "\n",
    "\n",
    "        print(f\"Successfully transposed {current_input_csv_path} to {current_output_transposed_csv_path}\")\n",
    "\n",
    "        df_healthy_transposed_sample = pd.read_csv(current_output_transposed_csv_path, nrows=5)\n",
    "        print(df_healthy_transposed_sample.head())\n",
    "    else:\n",
    "        print(f\"No sample IDs found to process in {current_input_csv_path} after excluding specified columns.\")\n",
    "else:\n",
    "    if os.path.exists(current_input_csv_path): # Only print if input file was valid but no gene_ids\n",
    "        print(f\"No gene IDs found in column '{feature_id_col_name}' in {current_input_csv_path}. Transposition aborted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Reduce the samples of the healthy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_for_sampling_path = 'data/healthy_data_transposed.csv'\n",
    "output_sampled_path = 'data/healthy_data_transposed_sampled.csv'\n",
    "num_samples_to_keep = 1000\n",
    "sampling_read_chunk_size = 1000\n",
    "\n",
    "print(f\"Starting sampling of {input_for_sampling_path} to select {num_samples_to_keep} samples.\")\n",
    "\n",
    "# Reduce the sample size for the healthy dataset to be more equivalent to the unhealthy dataset\n",
    "if not os.path.exists(input_for_sampling_path):\n",
    "    print(f\"Error: Input file for sampling not found: {input_for_sampling_path}\")\n",
    "else:\n",
    "    try:\n",
    "        # 1. Get all sample IDs from the 'sample_id' column of the transformed file\n",
    "        print(\"Reading all sample IDs from the transformed healthy dataset...\")\n",
    "        # The first column of healthy_data_transposed.csv is 'sample_id'\n",
    "        all_sample_ids_df = pd.read_csv(input_for_sampling_path, usecols=['sample_id'])\n",
    "        all_sample_ids = all_sample_ids_df['sample_id'].tolist()\n",
    "        print(f\"Found {len(all_sample_ids)} total samples in {input_for_sampling_path}.\")\n",
    "\n",
    "        if not all_sample_ids:\n",
    "            print(\"Error: No sample IDs found. Cannot proceed with sampling.\")\n",
    "        elif len(all_sample_ids) < num_samples_to_keep:\n",
    "            print(f\"Warning: Total samples ({len(all_sample_ids)}) is less than desired samples ({num_samples_to_keep}). Using all available samples.\")\n",
    "            selected_sample_ids = all_sample_ids\n",
    "        else:\n",
    "            # 2. Randomly select N sample IDs\n",
    "            random.seed(5) # for reproducibility\n",
    "            selected_sample_ids = random.sample(all_sample_ids, num_samples_to_keep)\n",
    "            print(f\"Randomly selected {len(selected_sample_ids)} sample IDs.\")\n",
    "\n",
    "        if selected_sample_ids:\n",
    "            # 3. Create the new CSV with only the selected samples\n",
    "            print(f\"Writing {len(selected_sample_ids)} selected samples to {output_sampled_path}...\")\n",
    "            \n",
    "            # The transposed file has 'sample_id' as the first column, which becomes the index\n",
    "            chunk_iterator_for_sampling = load_csv_in_chunks(\n",
    "                input_for_sampling_path,\n",
    "                chunk_size=sampling_read_chunk_size,\n",
    "                index_col='sample_id' \n",
    "            )\n",
    "\n",
    "            if chunk_iterator_for_sampling:\n",
    "                first_save_chunk = True\n",
    "                for i_chunk, df_chunk_original in enumerate(chunk_iterator_for_sampling):\n",
    "                    # Filter the chunk to keep only rows whose index (sample_id) is in selected_sample_ids\n",
    "                    df_chunk_filtered = df_chunk_original[df_chunk_original.index.isin(selected_sample_ids)]\n",
    "\n",
    "                    if not df_chunk_filtered.empty:\n",
    "                        if first_save_chunk:\n",
    "                            save_data_to_csv(df_chunk_filtered, output_sampled_path, index=True, header=True, mode='w')\n",
    "                            first_save_chunk = False\n",
    "                        else:\n",
    "                            save_data_to_csv(df_chunk_filtered, output_sampled_path, index=True, header=False, mode='a')\n",
    "                        print(f\"  Processed original chunk {i_chunk+1}, wrote {len(df_chunk_filtered)} sampled rows to {output_sampled_path}.\")\n",
    "                print(f\"Finished writing sampled data to {output_sampled_path}\")\n",
    "            else:\n",
    "                print(f\"Failed to load chunks from {input_for_sampling_path} for sampling.\")\n",
    "        else:\n",
    "            print(\"No samples were selected. Output file will not be created.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during sampling: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Rename index name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replace the existing healthy data processing cell with this more efficient version\n",
    "# sampled_healthy_data_input_path = 'data/healthy_data_transformed_sampled.csv'\n",
    "# processing_chunk_size = 1000\n",
    "\n",
    "# print(f\"Loading sampled healthy data from: {sampled_healthy_data_input_path}\")\n",
    "# df_healthy_chunk_iterator = load_csv_in_chunks(\n",
    "#     sampled_healthy_data_input_path,\n",
    "#     header=0,\n",
    "#     index_col=0,\n",
    "#     chunk_size=processing_chunk_size\n",
    "# )\n",
    "\n",
    "# # Process the healthy dataset efficiently with single MyGene API call\n",
    "# if df_healthy_chunk_iterator:\n",
    "#     # First ensure all chunks have proper index name\n",
    "#     processed_chunks = []\n",
    "#     for i, df_chunk in enumerate(df_healthy_chunk_iterator):\n",
    "#         if df_chunk.index.name != 'sample_id':\n",
    "#             df_chunk = modify_dataframe_element(df_chunk, new_value='sample_id', old_name=df_chunk.index.name, element_type='index_header')\n",
    "#         processed_chunks.append(df_chunk)\n",
    "    \n",
    "#     # Save the processed healthy data\n",
    "#     save_data_to_csv(df_healthy_processed, output_healthy_processed_path, index=True)\n",
    "# else:\n",
    "#     print(f\"Failed to load healthy data chunks from {sampled_healthy_data_input_path}. Check file path and format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Prepare datasets for merge\n",
    "\n",
    "Remove version suffix from gene IDs.\n",
    "\n",
    "Process datasets so that they only have the genes that are present in both datasets.\n",
    "\n",
    "Translate the gene IDs to their corresponding symbols for interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_data_path = 'data/healthy_data_transposed_sampled.csv'\n",
    "unhealthy_data_path = 'data/unhealthy_data_processed.csv'\n",
    "output_healthy_path = 'data/healthy_data_aligned.csv'\n",
    "output_unhealthy_path = 'data/unhealthy_data_aligned.csv'\n",
    "chunk_size = 1000\n",
    "\n",
    "print(\"🚀 STARTING DATASET ALIGNMENT PROCESS\")\n",
    "\n",
    "# Load datasets\n",
    "print(\"\\n📂 LOADING DATASETS...\")\n",
    "print(f\"   Healthy data: {healthy_data_path}\")\n",
    "print(f\"   Unhealthy data: {unhealthy_data_path}\")\n",
    "print(f\"   Chunk size: {chunk_size}\")\n",
    "\n",
    "healthy_chunks = load_csv_in_chunks(healthy_data_path, chunk_size=chunk_size, index_col=0)\n",
    "unhealthy_chunks = load_csv_in_chunks(unhealthy_data_path, chunk_size=chunk_size, index_col=0)\n",
    "\n",
    "if healthy_chunks is None or unhealthy_chunks is None:\n",
    "    print(\"❌ ERROR: Failed to load one or both datasets\")\n",
    "else:\n",
    "    print(\"✅ Both datasets loaded successfully\")\n",
    "    \n",
    "    # Align gene columns (strips version suffixes and keeps intersection only)\n",
    "    print(\"\\n🔄 STARTING GENE ALIGNMENT...\")\n",
    "    aligned_healthy, aligned_unhealthy, alignment_info = align_gene_columns_simple(\n",
    "        healthy_chunks, unhealthy_chunks\n",
    "    )\n",
    "    \n",
    "    if aligned_healthy and aligned_unhealthy:\n",
    "        # Save results\n",
    "        print(\"\\n💾 SAVING ALIGNED DATASETS...\")\n",
    "        print(f\"   Saving healthy dataset to: {output_healthy_path}\")\n",
    "        save_data_to_csv(aligned_healthy, output_healthy_path, index=True)\n",
    "        \n",
    "        print(f\"   Saving unhealthy dataset to: {output_unhealthy_path}\")\n",
    "        # save_data_to_csv(aligned_unhealthy, output_unhealthy_path, index=True)\n",
    "        \n",
    "        # Final verification\n",
    "        print(\"\\n🔍 FINAL VERIFICATION...\")\n",
    "        print(\"   Loading saved files to verify structure...\")\n",
    "        \n",
    "        # Check first few rows of each saved file\n",
    "        healthy_check = pd.read_csv(output_healthy_path, nrows=3, index_col=0)\n",
    "        # unhealthy_check = pd.read_csv(output_unhealthy_path, nrows=3, index_col=0)\n",
    "        \n",
    "        print(f\"   ✅ Healthy dataset saved - Shape: {healthy_check.shape}\")\n",
    "        print(f\"      Sample columns: {list(healthy_check.columns[:5])}...\")\n",
    "        \n",
    "        # print(f\"   ✅ Unhealthy dataset saved - Shape: {unhealthy_check.shape}\")  \n",
    "        # print(f\"      Sample columns: {list(unhealthy_check.columns[:5])}...\")\n",
    "        \n",
    "        print(\"\\n🎉 ALIGNMENT PROCESS COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"   Both datasets now contain only common genes with version suffixes stripped.\")\n",
    "        \n",
    "    else:\n",
    "        print(\"ERROR: Gene alignment failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚀 FINAL DATASET PREPARATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Configuration\n",
    "healthy_aligned_path = 'data/healthy_data_aligned.csv'\n",
    "unhealthy_aligned_path = 'data/unhealthy_data_aligned.csv'\n",
    "output_healthy_path = 'data/healthy_data_labeled.csv'\n",
    "output_unhealthy_path = 'data/unhealthy_data_labeled.csv'\n",
    "output_merged_path = 'data/merged_dataset.csv'\n",
    "final_path = 'data/final_dataset.csv'\n",
    "chunk_size = 1000\n",
    "\n",
    "# Load datasets\n",
    "healthy_chunks = load_csv_in_chunks(healthy_aligned_path, chunk_size=chunk_size, index_col=0)\n",
    "unhealthy_chunks = load_csv_in_chunks(unhealthy_aligned_path, chunk_size=chunk_size, index_col=0)\n",
    "\n",
    "if healthy_chunks is None or unhealthy_chunks is None:\n",
    "    print(\"❌ ERROR: Failed to load one or both datasets\")\n",
    "else:\n",
    "    print(\"✅ Both datasets loaded successfully\")\n",
    "\n",
    "# Label datasets\n",
    "healthy_labeled = add_condition_labels_to_chunks(healthy_chunks, 0, 'HEALTHY')\n",
    "unhealthy_labeled = add_condition_labels_to_chunks(unhealthy_chunks, 1, 'UNHEALTHY')\n",
    "\n",
    "print(\"Saving labeled datasets...\")\n",
    "save_data_to_csv(healthy_labeled, output_healthy_path, index=True)\n",
    "# save_data_to_csv(unhealthy_labeled, output_unhealthy_path, index=True)\n",
    "\n",
    "# # Merge datasets\n",
    "# merged_dataset = merge_labeled_datasets(healthy_labeled, unhealthy_labeled)\n",
    "\n",
    "# print(f\"   Saving merged dataset to: {output_merged_path}\")\n",
    "# save_data_to_csv(merged_dataset, output_merged_path, index=True)\n",
    "\n",
    "# merged_dataset_chunks = load_csv_in_chunks(output_merged_path, chunk_size=chunk_size, index_col=0)\n",
    "# if merged_dataset_chunks is not None:\n",
    "#     # Step 2: Convert gene IDs to symbols\n",
    "#     print(\"\\n🧬 CONVERTING GENE IDS TO SYMBOLS\")\n",
    "#     print(\"-\" * 35)\n",
    "\n",
    "#     final_dataset = convert_gene_ids_to_symbols(merged_dataset_chunks, mg)\n",
    "\n",
    "#     # Step 3: Save final dataset\n",
    "#     print(\"\\n💾 SAVING FINAL DATASET\")\n",
    "#     print(\"-\" * 25)\n",
    "    \n",
    "#     print(f\"Saving to: {final_path}\")\n",
    "#     try:\n",
    "#         save_data_to_csv(final_dataset, final_path, index=True)\n",
    "#         print(f\"✅ Successfully saved!\")\n",
    "        \n",
    "#         # Final summary\n",
    "#         print(f\"\\n📊 FINAL DATASET SUMMARY\")\n",
    "#         print(\"-\" * 25)\n",
    "#         print(f\"Shape: {final_dataset.shape}\")\n",
    "#         print(f\"Binary labels: {final_dataset['condition'].value_counts().sort_index().to_dict()}\")\n",
    "#         print(f\"Sample columns: {list(final_dataset.columns[:5])}...\")\n",
    "        \n",
    "#         print(\"\\n🎉 DATASET READY FOR ML TRAINING!\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error saving: {e}\")\n",
    "# else:\n",
    "#     print(\"❌ Cannot proceed - failed to load merged dataset chunks for gene ID conversion.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
