{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Imports and modular functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import mygene\n",
    "import csv\n",
    "import random\n",
    "import gc\n",
    "\n",
    "\n",
    "def load_csv_in_chunks(file_path, chunk_size=8000, **kwargs):\n",
    "    \"\"\"\n",
    "    Loads a CSV file in chunks to avoid memory issues.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "        chunk_size (int): The number of rows per chunk.\n",
    "        **kwargs: Additional keyword arguments to pass to pd.read_csv()\n",
    "                  (e.g., sep=',', header=0, index_col=None, usecols=None).\n",
    "\n",
    "    Returns:\n",
    "        A pandas TextFileReader object (iterator) that yields DataFrame chunks\n",
    "        if the file exists, otherwise None.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Preparing to load {file_path} in chunks of size {chunk_size}...\")\n",
    "    try:\n",
    "        chunk_iterator = pd.read_csv(file_path, chunksize=chunk_size, **kwargs)\n",
    "        return chunk_iterator\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_data_to_csv(data_to_save, output_file_path, delete_original_path=False, index=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Saves a pandas DataFrame or an iterator of DataFrame chunks to a CSV file.\n",
    "    Optionally deletes the original file after successful saving.\n",
    "\n",
    "    Args:\n",
    "        data_to_save (pd.DataFrame or iterator): DataFrame or iterator yielding DataFrames.\n",
    "        output_file_path (str): Path to save the new CSV file.\n",
    "        delete_original_path (str, optional): Path to the original file to delete. Defaults to False.\n",
    "        index (bool, optional): Whether to write DataFrame index. Defaults to False.\n",
    "        **kwargs: Additional keyword arguments to pass to df.to_csv().\n",
    "    \"\"\"\n",
    "    mode = kwargs.pop('mode', 'w')\n",
    "    header = kwargs.pop('header', True)\n",
    "\n",
    "    try:\n",
    "        if isinstance(data_to_save, pd.DataFrame):\n",
    "            print(f\"Saving DataFrame to {output_file_path}...\")\n",
    "            data_to_save.to_csv(output_file_path, index=index, mode=mode, header=header, **kwargs)\n",
    "        else:\n",
    "            print(f\"Saving data in chunks to {output_file_path}...\")\n",
    "            for i, chunk_df in enumerate(data_to_save):\n",
    "                chunk_mode = 'w' if i == 0 else 'a'\n",
    "                chunk_header = header if i == 0 else False\n",
    "                \n",
    "                chunk_df.to_csv(output_file_path, index=index, mode=chunk_mode, header=chunk_header, **kwargs)\n",
    "                \n",
    "                if i == 0:\n",
    "                    print(f\"Written first chunk to {output_file_path}\")\n",
    "                else:\n",
    "                    print(f\"Appended chunk {i+1} to {output_file_path}\")\n",
    "        \n",
    "        print(f\"Successfully saved data to {output_file_path}\")\n",
    "        \n",
    "        # Handle file deletion if requested\n",
    "        if delete_original_path and delete_original_path != output_file_path:\n",
    "            if os.path.exists(delete_original_path):\n",
    "                os.remove(delete_original_path)\n",
    "                print(f\"Successfully deleted original file: {delete_original_path}\")\n",
    "            else:\n",
    "                print(f\"Warning: Original file not found at {delete_original_path}\")\n",
    "        elif delete_original_path == output_file_path:\n",
    "            print(f\"Warning: Original and output paths are the same. Original file not deleted.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to {output_file_path}: {e}\")\n",
    "        if os.path.exists(output_file_path):\n",
    "            print(f\"Partial data might have been written to {output_file_path}.\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def transpose_csv(input_csv, output_csv, feature_id_col='Name', exclude_cols=['Description'], \n",
    "                           batch_size=1000):\n",
    "    \"\"\"\n",
    "    Optimized version of transpose_csv that reduces file reads and improves performance.\n",
    "    \n",
    "    Args:\n",
    "        input_csv (str): Path to the input CSV file\n",
    "        output_csv (str): Path where the transposed CSV will be saved\n",
    "        feature_id_col (str): Name of the column containing feature IDs (e.g., gene names)\n",
    "        exclude_cols (list): List of column names to exclude from transposition\n",
    "        batch_size (int): Number of samples to process in each batch\n",
    "    \"\"\"\n",
    "    print(f\"Reading header and identifying sample columns...\")\n",
    "    with open(input_csv, 'r') as f:\n",
    "        header = next(csv.reader(f))\n",
    "    \n",
    "    # Find indices for sample columns and the feature_id column\n",
    "    sample_cols = [i for i, col in enumerate(header) if col not in [feature_id_col] + exclude_cols]\n",
    "    sample_ids = [header[i] for i in sample_cols]\n",
    "    feature_id_idx = header.index(feature_id_col)\n",
    "    print(f\"    Found {len(sample_cols)} samples to transpose.\")\n",
    "    \n",
    "    # First pass: collect gene IDs\n",
    "    print(f\"Reading gene IDs from column '{feature_id_col}'...\")\n",
    "    gene_ids = []\n",
    "    with open(input_csv, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            if row and len(row) > feature_id_idx:\n",
    "                gene_ids.append(row[feature_id_idx])\n",
    "    \n",
    "    print(f\"    Found {len(gene_ids)} genes!\")\n",
    "    \n",
    "    # Create output file with header\n",
    "    with open(output_csv, 'w', newline='') as out_file:\n",
    "        writer = csv.writer(out_file)\n",
    "        writer.writerow(['sample_id'] + gene_ids)\n",
    "    \n",
    "    # Process samples in batches to balance memory usage and speed\n",
    "    print(f\"Writing data to {output_csv} in batches of {batch_size} samples...\")\n",
    "    for batch_start in range(0, len(sample_cols), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(sample_cols))\n",
    "        current_batch_cols = sample_cols[batch_start:batch_end]\n",
    "        current_batch_ids = sample_ids[batch_start:batch_end]\n",
    "        \n",
    "        print(f\"    Processing batch {batch_start//batch_size + 1}/{(len(sample_cols)-1)//batch_size + 1} \"\n",
    "              f\"(samples {batch_start+1}-{batch_end})...\")\n",
    "        \n",
    "        # For each batch, read the input file once and extract all columns for this batch\n",
    "        sample_values = [[] for _ in range(len(current_batch_cols))]\n",
    "        \n",
    "        with open(input_csv, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)  # Skip header\n",
    "            for row in reader:\n",
    "                if row:\n",
    "                    # For each gene row, extract values for all samples in current batch\n",
    "                    for i, col_idx in enumerate(current_batch_cols):\n",
    "                        if col_idx < len(row):\n",
    "                            sample_values[i].append(row[col_idx])\n",
    "                        else:\n",
    "                            sample_values[i].append(\"\")\n",
    "        \n",
    "        # Now write each sample in the batch to the output file\n",
    "        with open(output_csv, 'a', newline='') as out_file:\n",
    "            writer = csv.writer(out_file)\n",
    "            for i, sample_id in enumerate(current_batch_ids):\n",
    "                writer.writerow([sample_id] + sample_values[i])\n",
    "        \n",
    "        # Force garbage collection after writing each batch\n",
    "        sample_values = None\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"ðŸ¥¶âœ… Transposed {len(sample_ids)} samples Ã— {len(gene_ids)} genes!\")\n",
    "\n",
    "\n",
    "def convert_gene_ids_to_symbols(dataset_chunk_iterator, mygene_client, gene_column_prefix=\"ENSG\"):\n",
    "    \"\"\"\n",
    "    Convert gene IDs to gene symbols using MyGene API in a memory-efficient way.\n",
    "    Only calls MyGene API once for all gene columns from the first chunk.\n",
    "    \n",
    "    Args:\n",
    "        dataset_chunk_iterator: Iterator yielding DataFrame chunks\n",
    "        mygene_client: MyGene client instance\n",
    "        gene_column_prefix (str): Prefix to identify gene columns\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Complete dataset with gene symbols as column names\n",
    "    \"\"\"\n",
    "    print(\"Converting gene IDs to symbols (chunked processing)...\")\n",
    "    \n",
    "    # Get first chunk to determine gene columns and create mapping\n",
    "    try:\n",
    "        first_chunk = next(dataset_chunk_iterator)\n",
    "        print(f\"Processing first chunk: {first_chunk.shape}\")\n",
    "    except StopIteration:\n",
    "        print(\"âŒ Error: Dataset is empty\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Identify gene columns from first chunk\n",
    "    gene_columns = [col for col in first_chunk.columns \n",
    "                   if isinstance(col, str) and col.startswith(gene_column_prefix)]\n",
    "    \n",
    "    if not gene_columns:\n",
    "        print(\"No gene columns found - returning original data\")\n",
    "        # Concatenate all chunks and return\n",
    "        all_chunks = [first_chunk]\n",
    "        for chunk in dataset_chunk_iterator:\n",
    "            all_chunks.append(chunk)\n",
    "        return pd.concat(all_chunks, axis=0, ignore_index=False)\n",
    "    \n",
    "    print(f\"Found {len(gene_columns)} gene columns\")\n",
    "    \n",
    "    # Single MyGene API call for all gene IDs\n",
    "    gene_id_to_symbol = {}\n",
    "    symbols_found = 0\n",
    "    \n",
    "    try:\n",
    "        print(\"Making single MyGene API call...\")\n",
    "        import warnings\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            results = mygene_client.querymany(\n",
    "                gene_columns,\n",
    "                scopes='ensembl.gene',\n",
    "                fields='symbol',\n",
    "                species='human',\n",
    "                verbose=False,\n",
    "                silent=True\n",
    "            )\n",
    "        \n",
    "        # Process API results\n",
    "        for result in results:\n",
    "            gene_id = result['query']\n",
    "            if 'symbol' in result and result['symbol']:\n",
    "                gene_id_to_symbol[gene_id] = result['symbol']\n",
    "                symbols_found += 1\n",
    "            else:\n",
    "                gene_id_to_symbol[gene_id] = gene_id\n",
    "        \n",
    "        print(f\"âœ… Successfully converted {symbols_found}/{len(gene_columns)} genes to symbols\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ MyGene API error: {e}. Using original gene IDs\")\n",
    "        gene_id_to_symbol = {gene_id: gene_id for gene_id in gene_columns}\n",
    "    \n",
    "    # Create final column mapping (preserve non-gene columns like 'condition')\n",
    "    final_column_mapping = {}\n",
    "    for col in first_chunk.columns:\n",
    "        if col == 'condition':\n",
    "            final_column_mapping[col] = col\n",
    "        elif col in gene_id_to_symbol:\n",
    "            final_column_mapping[col] = gene_id_to_symbol[col]\n",
    "        else:\n",
    "            final_column_mapping[col] = col\n",
    "    \n",
    "    print(\"Applying gene symbol mapping to all chunks...\")\n",
    "    \n",
    "    # Process first chunk\n",
    "    renamed_first_chunk = first_chunk.rename(columns=final_column_mapping)\n",
    "    processed_chunks = [renamed_first_chunk]\n",
    "    chunk_count = 1\n",
    "    \n",
    "    # Process remaining chunks with same mapping\n",
    "    for chunk in dataset_chunk_iterator:\n",
    "        renamed_chunk = chunk.rename(columns=final_column_mapping)\n",
    "        processed_chunks.append(renamed_chunk)\n",
    "        chunk_count += 1\n",
    "        \n",
    "        if chunk_count % 3 == 0:\n",
    "            print(f\"  âœ“ Processed {chunk_count} chunks...\")\n",
    "    \n",
    "    print(f\"Concatenating {chunk_count} processed chunks...\")\n",
    "    final_dataset = pd.concat(processed_chunks, axis=0, ignore_index=False)\n",
    "    \n",
    "    print(f\"âœ… Final dataset shape: {final_dataset.shape}\")\n",
    "    print(f\"   Symbols converted: {symbols_found}/{len(gene_columns)}\")\n",
    "    \n",
    "    return final_dataset\n",
    "\n",
    "\n",
    "def add_condition_labels_to_chunks(chunk_iterator, condition_label, dataset_name):\n",
    "    \"\"\"\n",
    "    Add condition labels to dataset chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunk_iterator: Iterator yielding DataFrame chunks\n",
    "        condition_label (int): Binary label (0 for healthy, 1 for unhealthy)\n",
    "        dataset_name (str): Name for logging purposes\n",
    "    \n",
    "    Returns:\n",
    "        list: List of labeled DataFrame chunks\n",
    "    \"\"\"\n",
    "    print(f\"Adding label '{condition_label}' to {dataset_name} dataset...\")\n",
    "    \n",
    "    labeled_chunks = []\n",
    "    for chunk in chunk_iterator:\n",
    "        chunk['condition'] = condition_label\n",
    "        labeled_chunks.append(chunk)\n",
    "    \n",
    "    print(f\"âœ… Completed {len(labeled_chunks)} {dataset_name} chunks\")\n",
    "    return labeled_chunks\n",
    "\n",
    "\n",
    "def merge_labeled_datasets(healthy_chunks, unhealthy_chunks):\n",
    "    \"\"\"\n",
    "    Merge healthy and unhealthy dataset chunks into one DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        healthy_chunks (list): List of healthy DataFrame chunks\n",
    "        unhealthy_chunks (list): List of unhealthy DataFrame chunks\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Merged dataset\n",
    "    \"\"\"\n",
    "    print(\"Merging datasets...\")\n",
    "    \n",
    "    all_chunks = healthy_chunks + unhealthy_chunks\n",
    "    merged_dataset = pd.concat(all_chunks, axis=0, ignore_index=False)\n",
    "    \n",
    "    print(f\"âœ… Merged dataset: {len(merged_dataset)} samples, {len(merged_dataset.columns)} features\")\n",
    "    print(f\"   Healthy (0): {(merged_dataset['condition'] == 0).sum()}\")\n",
    "    print(f\"   Unhealthy (1): {(merged_dataset['condition'] == 1).sum()}\")\n",
    "    \n",
    "    return merged_dataset\n",
    "\n",
    "\n",
    "def clean_duplicate_nans(chunk_iterator):\n",
    "    \"\"\"\n",
    "    Processes an iterator of DataFrame chunks to drop duplicates and NaNs.\n",
    "    Returns a list of cleaned DataFrame chunks.\n",
    "    \"\"\"\n",
    "    cleaned_chunks_list = []\n",
    "    for i, chunk_df in enumerate(chunk_iterator):\n",
    "        original_rows = len(chunk_df)\n",
    "        chunk_df.drop_duplicates(inplace=True)\n",
    "        if original_rows > len(chunk_df):\n",
    "            print(f\"        Chunk {i+1}: Dropped {original_rows - len(chunk_df)} duplicate data rows.\")\n",
    "        original_rows = len(chunk_df)\n",
    "        chunk_df.dropna(inplace=True)\n",
    "        if original_rows > len(chunk_df):\n",
    "            print(f\"        Chunk {i+1}: Dropped {original_rows - len(chunk_df)} rows with null values.\")\n",
    "        if not chunk_df.empty:\n",
    "            cleaned_chunks_list.append(chunk_df)\n",
    "        else:\n",
    "            del chunk_df\n",
    "        gc.collect()\n",
    "    print(f\"    Finished dup/NaN cleaning. {len(cleaned_chunks_list)} non-empty chunks remaining.\")\n",
    "    return cleaned_chunks_list\n",
    "\n",
    "\n",
    "def drop_column(chunk_iterator, column_name_to_drop):\n",
    "    \"\"\"\n",
    "    Processes an iterator of DataFrame chunks to drop a specified column.\n",
    "    Returns a list of processed DataFrame chunks.\n",
    "    \"\"\"\n",
    "    processed_chunks_list = []\n",
    "    print(f\"    Attempting to drop column: '{column_name_to_drop}' from all chunks...\")\n",
    "    for i, chunk_df in enumerate(chunk_iterator):\n",
    "        if column_name_to_drop in chunk_df.columns:\n",
    "            chunk_df.drop(columns=[column_name_to_drop], inplace=True)\n",
    "            if i == 0:\n",
    "                 print(f\"        Dropped '{column_name_to_drop}' column (first occurrence in chunk {i+1}).\")\n",
    "        elif i == 0:\n",
    "            print(f\"        Warning: Column '{column_name_to_drop}' not found in the first chunk ({i+1}).\")\n",
    "        if not chunk_df.empty:\n",
    "            processed_chunks_list.append(chunk_df)\n",
    "        else:\n",
    "            del chunk_df\n",
    "        gc.collect()\n",
    "    print(f\"    Finished column drop attempt. {len(processed_chunks_list)} chunks remaining.\")\n",
    "    return processed_chunks_list\n",
    "\n",
    "\n",
    "def rename_index(chunk_iterator, new_index_name):\n",
    "    \"\"\"\n",
    "    Processes an iterator of DataFrame chunks to rename the index.\n",
    "    Returns a list of processed DataFrame chunks.\n",
    "    \"\"\"\n",
    "    processed_chunks_list = []\n",
    "    print(f\"    Attempting to rename index to: '{new_index_name}' for all chunks...\")\n",
    "    for i, chunk_df in enumerate(chunk_iterator):\n",
    "        if chunk_df.index.name != new_index_name:\n",
    "            chunk_df.index.name = new_index_name\n",
    "            if i == 0:\n",
    "                print(f\"        Renamed index to '{new_index_name}' (first occurrence in chunk {i+1}).\")\n",
    "        if not chunk_df.empty:\n",
    "            processed_chunks_list.append(chunk_df)\n",
    "        else:\n",
    "            del chunk_df\n",
    "        gc.collect()\n",
    "    print(f\"    Finished index renaming. {len(processed_chunks_list)} chunks remaining.\")\n",
    "    return processed_chunks_list\n",
    "\n",
    "\n",
    "def filter_rows(chunk_iterator):\n",
    "    \"\"\"\n",
    "    Processes an iterator of DataFrame chunks to filter out rows where all data columns are zero.\n",
    "    Returns a list of filtered DataFrame chunks.\n",
    "    \"\"\"\n",
    "    filtered_chunks_list = []\n",
    "    print(f\"    Attempting to filter rows with all zero counts from all chunks...\")\n",
    "    for i, chunk_df in enumerate(chunk_iterator):\n",
    "        if not chunk_df.empty:\n",
    "            original_rows = len(chunk_df)\n",
    "            chunk_df_filtered = chunk_df.loc[(chunk_df != 0).any(axis=1)]\n",
    "            if original_rows > len(chunk_df_filtered):\n",
    "                print(f\"        Chunk {i+1}: Dropped {original_rows - len(chunk_df_filtered)} rows with all zero counts.\")\n",
    "            if not chunk_df_filtered.empty:\n",
    "                filtered_chunks_list.append(chunk_df_filtered)\n",
    "            else:\n",
    "                del chunk_df\n",
    "                if chunk_df_filtered is not chunk_df:\n",
    "                    del chunk_df_filtered\n",
    "        gc.collect()\n",
    "    print(f\"    Finished zero-row filtering. {len(filtered_chunks_list)} non-empty chunks remaining.\")\n",
    "    return filtered_chunks_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Initial Preprocessing Unhealthy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Initial Preprocessing Healthy Dataset\n",
    "## Convert .gct to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = 'data/GTEx_Analysis_2022-06-06_v10_RNASeQCv2.4.2_gene_reads.gct'\n",
    "output_file = 'data/healthy_data.csv'\n",
    "chunk_size = 8000\n",
    "\n",
    "gct_chunk_iterator = load_csv_in_chunks(\n",
    "    file_path=dataset_file,\n",
    "    chunk_size=chunk_size,\n",
    "    sep='\\t',\n",
    "    skiprows=2\n",
    ")\n",
    "\n",
    "if gct_chunk_iterator:\n",
    "    save_data_to_csv(\n",
    "        data_to_save=gct_chunk_iterator,\n",
    "        output_file_path=output_file,\n",
    "        delete_original_path=dataset_file,\n",
    "        index=False\n",
    "    )\n",
    "else:\n",
    "    print(f\"Failed to load {dataset_file} using load_csv_in_chunks.\")\n",
    "    \n",
    "# Clean up\n",
    "if 'gct_chunk_iterator' in locals() or 'gct_chunk_iterator' in globals():\n",
    "    del gct_chunk_iterator\n",
    "if 'dataset_file' in locals() or 'dataset_file' in globals():\n",
    "    del dataset_file\n",
    "if 'output_file' in locals() or 'output_file' in globals():\n",
    "    del output_file\n",
    "if 'read_chunk_size' in locals() or 'read_chunk_size' in globals():\n",
    "    del chunk_size\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Transpose healthy dataset\n",
    "\n",
    "Run this seperately. After it is done transposing, restart python kernel, otherwise it will most likely crash if you try to run the next cell (If that cell would use a memory intensive task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_data_input = 'data/healthy_data.csv'\n",
    "healthy_data_output = 'data/healthy_data_transposed.csv'\n",
    "feature_id_col = 'Name'\n",
    "exclude_cols = ['Description']\n",
    "batch_size = 3000\n",
    "\n",
    "transpose_csv(\n",
    "    input_csv=healthy_data_input,\n",
    "    output_csv=healthy_data_output,\n",
    "    feature_id_col=feature_id_col,\n",
    "    exclude_cols=exclude_cols,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Load and clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_dataset_file = 'data/healthy_data_transposed.csv'\n",
    "healthy_output_file = 'data/healthy_data_preprocessed.csv'\n",
    "config_chunk_size = 1000\n",
    "\n",
    "chunk_iterator = load_csv_in_chunks(file_path=healthy_dataset_file, chunk_size=config_chunk_size, index_col=0)\n",
    "print(f\"Starting preprocessing for Healthy Dataset: {healthy_dataset_file}\")\n",
    "\n",
    "chunk_iterator = clean_duplicate_nans(chunk_iterator)\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Dropping \")\n",
    "chunk_iterator = drop_column(chunk_iterator, column_name_to_drop='Unnamed: 0')\n",
    "gc.collect()\n",
    "\n",
    "chunk_iterator = rename_index(chunk_iterator, new_index_name='gene_id')\n",
    "gc.collect()\n",
    "\n",
    "chunk_iterator = filter_rows(chunk_iterator)\n",
    "gc.collect()\n",
    "\n",
    "save_data_to_csv(\n",
    "    data_to_save=chunk_iterator,\n",
    "    output_file_path=healthy_output_file,\n",
    "    index=True\n",
    ")\n",
    "\n",
    "if 'healthy_dataset_file' in locals(): del healthy_dataset_file\n",
    "if 'healthy_output_file' in locals(): del healthy_output_file\n",
    "if 'config_chunk_size' in locals(): del config_chunk_size\n",
    "if 'chunk_iterator' in locals(): del chunk_iterator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_dataset_file = 'data/healthy_data_transposed.csv'\n",
    "healthy_output_file = 'data/healthy_data_preprocessed_1.csv'\n",
    "config_chunk_size = 500\n",
    "\n",
    "chunk_iterator = load_csv_in_chunks(file_path=healthy_dataset_file, chunk_size=config_chunk_size, index_col=0)\n",
    "print(f\"Starting preprocessing for Healthy Dataset: {healthy_dataset_file}\")\n",
    "\n",
    "print(f\"Cleaning duplicate NaNs and filtering rows...\")\n",
    "chunk_iterator = clean_duplicate_nans(chunk_iterator)\n",
    "\n",
    "print(\"saving cleaned data to CSV...\")\n",
    "save_data_to_csv(\n",
    "    data_to_save=chunk_iterator,\n",
    "    output_file_path=healthy_output_file,\n",
    "    index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_dataset_file = 'data/healthy_data_preprocessed_1.csv'\n",
    "healthy_output_file = 'data/healthy_data_preprocessed_2.csv'\n",
    "config_chunk_size = 1000\n",
    "\n",
    "chunk_iterator = load_csv_in_chunks(file_path=healthy_dataset_file, chunk_size=config_chunk_size, index_col=0)\n",
    "print(f\"Starting preprocessing for Healthy Dataset: {healthy_dataset_file}\")\n",
    "\n",
    "chunk_iterator = drop_column(chunk_iterator, column_name_to_drop='Unnamed: 0')\n",
    "\n",
    "save_data_to_csv(\n",
    "    data_to_save=chunk_iterator,\n",
    "    output_file_path=healthy_output_file,\n",
    "    index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_dataset_file = 'data/healthy_data_preprocessed_2.csv'\n",
    "healthy_output_file = 'data/healthy_data_preprocessed_3.csv'\n",
    "config_chunk_size = 1000\n",
    "\n",
    "chunk_iterator = load_csv_in_chunks(file_path=healthy_dataset_file, chunk_size=config_chunk_size, index_col=0)\n",
    "print(f\"Starting preprocessing for Healthy Dataset: {healthy_dataset_file}\")\n",
    "\n",
    "chunk_iterator = rename_index(chunk_iterator, new_index_name='gene_id')\n",
    "\n",
    "save_data_to_csv(\n",
    "    data_to_save=chunk_iterator,\n",
    "    output_file_path=healthy_output_file,\n",
    "    index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_dataset_file = 'data/healthy_data_preprocessed_3.csv'\n",
    "healthy_output_file = 'data/healthy_data_preprocessed_4.csv'\n",
    "config_chunk_size = 1000\n",
    "\n",
    "chunk_iterator = load_csv_in_chunks(file_path=healthy_dataset_file, chunk_size=config_chunk_size, index_col=0)\n",
    "print(f\"Starting preprocessing for Healthy Dataset: {healthy_dataset_file}\")\n",
    "\n",
    "chunk_iterator = filter_rows(chunk_iterator)\n",
    "\n",
    "save_data_to_csv(\n",
    "    data_to_save=chunk_iterator,\n",
    "    output_file_path=healthy_output_file,\n",
    "    index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
