{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Imports and modular functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Lock, Process\n",
    "import os\n",
    "import pandas as pd\n",
    "import mygene\n",
    "import csv\n",
    "import pickle\n",
    "import random\n",
    "import gc\n",
    "\n",
    "def load_csv_in_chunks(file_path, chunk_size=8000, **kwargs):\n",
    "    \"\"\"\n",
    "    Loads a CSV file in chunks to avoid memory issues.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "        chunk_size (int): The number of rows per chunk.\n",
    "        **kwargs: Additional keyword arguments to pass to pd.read_csv()\n",
    "                  (e.g., sep=',', header=0, index_col=None, usecols=None).\n",
    "\n",
    "    Returns:\n",
    "        A pandas TextFileReader object (iterator) that yields DataFrame chunks\n",
    "        if the file exists, otherwise None.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Preparing to load {file_path} in chunks of size {chunk_size}...\")\n",
    "    try:\n",
    "        chunk_iterator = pd.read_csv(file_path, chunksize=chunk_size, **kwargs)\n",
    "        return chunk_iterator\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_pickle_in_chunks(file_path, chunk_size=8000):\n",
    "    \"\"\"\n",
    "    Loads a pickle file containing a pandas DataFrame in chunks to avoid memory issues.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the pickle file.\n",
    "        chunk_size (int): The number of rows per chunk.\n",
    "\n",
    "    Yields:\n",
    "        pandas.DataFrame: DataFrame chunks of the specified size.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Preparing to load {file_path} in chunks of size {chunk_size}...\")\n",
    "    try:\n",
    "        df = pd.read_pickle(file_path)\n",
    "        print(f\"Loaded DataFrame with {len(df)} rows. Yielding chunks...\", flush=True)\n",
    "        \n",
    "        # Yield chunks of the DataFrame\n",
    "        for start_idx in range(0, len(df), chunk_size):\n",
    "            end_idx = min(start_idx + chunk_size, len(df))\n",
    "            chunk = df.iloc[start_idx:end_idx].copy()\n",
    "            yield chunk\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pickle file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_data_to_csv(data_to_save, output_file_path, delete_original_path=False, index=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Saves a pandas DataFrame or an iterator of DataFrame chunks to a CSV file.\n",
    "    Optionally deletes the original file after successful saving.\n",
    "\n",
    "    Args:\n",
    "        data_to_save (pd.DataFrame or iterator): DataFrame or iterator yielding DataFrames.\n",
    "        output_file_path (str): Path to save the new CSV file.\n",
    "        delete_original_path (str, optional): Path to the original file to delete. Defaults to False.\n",
    "        index (bool, optional): Whether to write DataFrame index. Defaults to False.\n",
    "        **kwargs: Additional keyword arguments to pass to df.to_csv().\n",
    "    \"\"\"\n",
    "    mode = kwargs.pop('mode', 'w')\n",
    "    header = kwargs.pop('header', True)\n",
    "\n",
    "    try:\n",
    "        if isinstance(data_to_save, pd.DataFrame):\n",
    "            print(f\"Saving DataFrame to {output_file_path}...\")\n",
    "            data_to_save.to_csv(output_file_path, index=index, mode=mode, header=header, **kwargs)\n",
    "        else:\n",
    "            print(f\"Saving data in chunks to {output_file_path}...\")\n",
    "            for i, chunk_df in enumerate(data_to_save):\n",
    "                chunk_mode = 'w' if i == 0 else 'a'\n",
    "                chunk_header = header if i == 0 else False\n",
    "                \n",
    "                chunk_df.to_csv(output_file_path, index=index, mode=chunk_mode, header=chunk_header, **kwargs)\n",
    "                \n",
    "                if i == 0:\n",
    "                    print(f\"Written first chunk to {output_file_path}\")\n",
    "                else:\n",
    "                    print(f\"Appended chunk {i+1} to {output_file_path}\")\n",
    "        \n",
    "        print(f\"Successfully saved data to {output_file_path}\")\n",
    "        \n",
    "        # Handle file deletion if requested\n",
    "        if delete_original_path and delete_original_path != output_file_path:\n",
    "            if os.path.exists(delete_original_path):\n",
    "                os.remove(delete_original_path)\n",
    "                print(f\"Successfully deleted original file: {delete_original_path}\")\n",
    "            else:\n",
    "                print(f\"Warning: Original file not found at {delete_original_path}\")\n",
    "        elif delete_original_path == output_file_path:\n",
    "            print(f\"Warning: Original and output paths are the same. Original file not deleted.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to {output_file_path}: {e}\")\n",
    "        if os.path.exists(output_file_path):\n",
    "            print(f\"Partial data might have been written to {output_file_path}.\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def save_data_as_pickle(chunk_iterator, output_pickle_path, process_func=None):\n",
    "    \"\"\"\n",
    "    Process CSV chunks and save the final result as a pickle file.\n",
    "    Uses pandas' built-in to_pickle() method for better performance.\n",
    "    \n",
    "    Args:\n",
    "        chunk_iterator: Iterator yielding DataFrame chunks from load_csv_in_chunks\n",
    "        output_pickle_path (str): Path to save the pickle file\n",
    "        process_func (callable, optional): Function to apply to each chunk\n",
    "    \"\"\"\n",
    "    print(f\"Processing chunks and saving to {output_pickle_path}...\")\n",
    "\n",
    "    processed_chunks = []\n",
    "    for i, chunk in enumerate(chunk_iterator):\n",
    "        if process_func:\n",
    "            chunk = process_func(chunk)\n",
    "        \n",
    "        if chunk.empty:\n",
    "            continue\n",
    "            \n",
    "        processed_chunks.append(chunk)\n",
    "    \n",
    "    if processed_chunks:\n",
    "        print(\"Concatenating all processed chunks...\")\n",
    "        final_df = pd.concat(processed_chunks, axis=0, ignore_index=False)\n",
    "        \n",
    "        print(f\"Saving final dataset to {output_pickle_path}...\")\n",
    "        final_df.to_pickle(output_pickle_path)\n",
    "        \n",
    "        print(f\"✅ Successfully saved {final_df.shape[0]} rows, {final_df.shape[1]} columns to pickle\")\n",
    "        \n",
    "        del processed_chunks, final_df\n",
    "        gc.collect()\n",
    "        \n",
    "    else:\n",
    "        print(\"No data to save!\")\n",
    "\n",
    "\n",
    "def transpose_csv(input_csv, output_csv, feature_id_col='sample_id', exclude_cols=[], \n",
    "                           batch_size=1000):\n",
    "    \"\"\"\n",
    "    Optimized version of transpose_csv that reduces file reads and improves performance.\n",
    "    \n",
    "    Args:\n",
    "        input_csv (str): Path to the input CSV file\n",
    "        output_csv (str): Path where the transposed CSV will be saved\n",
    "        feature_id_col (str): Name of the column containing feature IDs (e.g., gene names)\n",
    "        exclude_cols (list): List of column names to exclude from transposition\n",
    "        batch_size (int): Number of samples to process in each batch\n",
    "    \"\"\"\n",
    "    print(f\"Reading header and identifying sample columns...\")\n",
    "    with open(input_csv, 'r') as f:\n",
    "        header = next(csv.reader(f))\n",
    "    \n",
    "    # Find indices for sample columns and the feature_id column\n",
    "    sample_cols = [i for i, col in enumerate(header) if col not in [feature_id_col] + exclude_cols]\n",
    "    sample_ids = [header[i] for i in sample_cols]\n",
    "    feature_id_idx = header.index(feature_id_col)\n",
    "    print(f\"    Found {len(sample_cols)} samples to transpose.\")\n",
    "    \n",
    "    # First pass: collect gene IDs\n",
    "    print(f\"Reading gene IDs from column '{feature_id_col}'...\")\n",
    "    gene_ids = []\n",
    "    with open(input_csv, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            if row and len(row) > feature_id_idx:\n",
    "                gene_ids.append(row[feature_id_idx])\n",
    "    \n",
    "    print(f\"    Found {len(gene_ids)} genes!\")\n",
    "    \n",
    "    # Create output file with header\n",
    "    with open(output_csv, 'w', newline='') as out_file:\n",
    "        writer = csv.writer(out_file)\n",
    "        writer.writerow(['sample_id'] + gene_ids)\n",
    "    \n",
    "    # Process samples in batches to balance memory usage and speed\n",
    "    print(f\"Writing data to {output_csv} in batches of {batch_size} samples...\")\n",
    "    for batch_start in range(0, len(sample_cols), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(sample_cols))\n",
    "        current_batch_cols = sample_cols[batch_start:batch_end]\n",
    "        current_batch_ids = sample_ids[batch_start:batch_end]\n",
    "        \n",
    "        print(f\"    Processing batch {batch_start//batch_size + 1}/{(len(sample_cols)-1)//batch_size + 1} \"\n",
    "              f\"(samples {batch_start+1}-{batch_end})...\")\n",
    "        \n",
    "        # For each batch, read the input file once and extract all columns for this batch\n",
    "        sample_values = [[] for _ in range(len(current_batch_cols))]\n",
    "        \n",
    "        with open(input_csv, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)  # Skip header\n",
    "            for row in reader:\n",
    "                if row:\n",
    "                    # For each gene row, extract values for all samples in current batch\n",
    "                    for i, col_idx in enumerate(current_batch_cols):\n",
    "                        if col_idx < len(row):\n",
    "                            sample_values[i].append(row[col_idx])\n",
    "                        else:\n",
    "                            sample_values[i].append(\"\")\n",
    "        \n",
    "        # Now write each sample in the batch to the output file\n",
    "        with open(output_csv, 'a', newline='') as out_file:\n",
    "            writer = csv.writer(out_file)\n",
    "            for i, sample_id in enumerate(current_batch_ids):\n",
    "                writer.writerow([sample_id] + sample_values[i])\n",
    "        \n",
    "        # Force garbage collection after writing each batch\n",
    "        sample_values = None\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"🥶✅ Transposed {len(sample_ids)} samples × {len(gene_ids)} genes!\")\n",
    "\n",
    "\n",
    "def convert_gene_ids_to_symbols(dataset_chunk_iterator, mygene_client, gene_column_prefix=\"ENSG\"):\n",
    "    \"\"\"\n",
    "    Convert gene IDs to gene symbols using MyGene API in a memory-efficient way.\n",
    "    Only calls MyGene API once for all gene columns from the first chunk.\n",
    "    \n",
    "    Args:\n",
    "        dataset_chunk_iterator: Iterator yielding DataFrame chunks\n",
    "        mygene_client: MyGene client instance\n",
    "        gene_column_prefix (str): Prefix to identify gene columns\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Complete dataset with gene symbols as column names\n",
    "    \"\"\"\n",
    "    print(\"Converting gene IDs to symbols (chunked processing)...\")\n",
    "    \n",
    "    # Get first chunk to determine gene columns and create mapping\n",
    "    try:\n",
    "        first_chunk = next(dataset_chunk_iterator)\n",
    "        print(f\"Processing first chunk: {first_chunk.shape}\")\n",
    "    except StopIteration:\n",
    "        print(\"❌ Error: Dataset is empty\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Identify gene columns from first chunk\n",
    "    gene_columns = [col for col in first_chunk.columns \n",
    "                   if isinstance(col, str) and col.startswith(gene_column_prefix)]\n",
    "    \n",
    "    if not gene_columns:\n",
    "        print(\"No gene columns found - returning original data\")\n",
    "        # Concatenate all chunks and return\n",
    "        all_chunks = [first_chunk]\n",
    "        for chunk in dataset_chunk_iterator:\n",
    "            all_chunks.append(chunk)\n",
    "        return pd.concat(all_chunks, axis=0, ignore_index=False)\n",
    "    \n",
    "    print(f\"Found {len(gene_columns)} gene columns\")\n",
    "    \n",
    "    # Single MyGene API call for all gene IDs\n",
    "    gene_id_to_symbol = {}\n",
    "    symbols_found = 0\n",
    "    \n",
    "    try:\n",
    "        print(\"Making single MyGene API call...\")\n",
    "        import warnings\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            results = mygene_client.querymany(\n",
    "                gene_columns,\n",
    "                scopes='ensembl.gene',\n",
    "                fields='symbol',\n",
    "                species='human',\n",
    "                verbose=False,\n",
    "                silent=True\n",
    "            )\n",
    "        \n",
    "        # Process API results\n",
    "        for result in results:\n",
    "            gene_id = result['query']\n",
    "            if 'symbol' in result and result['symbol']:\n",
    "                gene_id_to_symbol[gene_id] = result['symbol']\n",
    "                symbols_found += 1\n",
    "            else:\n",
    "                gene_id_to_symbol[gene_id] = gene_id\n",
    "        \n",
    "        print(f\"✅ Successfully converted {symbols_found}/{len(gene_columns)} genes to symbols\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ MyGene API error: {e}. Using original gene IDs\")\n",
    "        gene_id_to_symbol = {gene_id: gene_id for gene_id in gene_columns}\n",
    "    \n",
    "    # Create final column mapping (preserve non-gene columns like 'condition')\n",
    "    final_column_mapping = {}\n",
    "    for col in first_chunk.columns:\n",
    "        if col == 'condition':\n",
    "            final_column_mapping[col] = col\n",
    "        elif col in gene_id_to_symbol:\n",
    "            final_column_mapping[col] = gene_id_to_symbol[col]\n",
    "        else:\n",
    "            final_column_mapping[col] = col\n",
    "    \n",
    "    print(\"Applying gene symbol mapping to all chunks...\")\n",
    "    \n",
    "    # Process first chunk\n",
    "    renamed_first_chunk = first_chunk.rename(columns=final_column_mapping)\n",
    "    processed_chunks = [renamed_first_chunk]\n",
    "    chunk_count = 1\n",
    "    \n",
    "    # Process remaining chunks with same mapping\n",
    "    for chunk in dataset_chunk_iterator:\n",
    "        renamed_chunk = chunk.rename(columns=final_column_mapping)\n",
    "        processed_chunks.append(renamed_chunk)\n",
    "        chunk_count += 1\n",
    "        \n",
    "        if chunk_count % 3 == 0:\n",
    "            print(f\"  ✓ Processed {chunk_count} chunks...\")\n",
    "    \n",
    "    print(f\"Concatenating {chunk_count} processed chunks...\")\n",
    "    final_dataset = pd.concat(processed_chunks, axis=0, ignore_index=False)\n",
    "    \n",
    "    print(f\"✅ Final dataset shape: {final_dataset.shape}\")\n",
    "    print(f\"   Symbols converted: {symbols_found}/{len(gene_columns)}\")\n",
    "    \n",
    "    return final_dataset\n",
    "\n",
    "\n",
    "def add_condition_labels_to_chunks(chunk_iterator, condition_label, dataset_name):\n",
    "    \"\"\"\n",
    "    Add condition labels to dataset chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunk_iterator: Iterator yielding DataFrame chunks\n",
    "        condition_label (int): Binary label (0 for healthy, 1 for unhealthy)\n",
    "        dataset_name (str): Name for logging purposes\n",
    "    \n",
    "    Returns:\n",
    "        list: List of labeled DataFrame chunks\n",
    "    \"\"\"\n",
    "    print(f\"Adding label '{condition_label}' to {dataset_name} dataset...\")\n",
    "    \n",
    "    labeled_chunks = []\n",
    "    for chunk in chunk_iterator:\n",
    "        chunk['condition'] = condition_label\n",
    "        labeled_chunks.append(chunk)\n",
    "    \n",
    "    print(f\"✅ Completed {len(labeled_chunks)} {dataset_name} chunks\")\n",
    "    return labeled_chunks\n",
    "\n",
    "\n",
    "def merge_labeled_datasets(healthy_chunks, unhealthy_chunks):\n",
    "    \"\"\"\n",
    "    Merge healthy and unhealthy dataset chunks into one DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        healthy_chunks (list): List of healthy DataFrame chunks\n",
    "        unhealthy_chunks (list): List of unhealthy DataFrame chunks\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Merged dataset\n",
    "    \"\"\"\n",
    "    print(\"Merging datasets...\")\n",
    "    \n",
    "    all_chunks = healthy_chunks + unhealthy_chunks\n",
    "    merged_dataset = pd.concat(all_chunks, axis=0, ignore_index=False)\n",
    "    \n",
    "    print(f\"✅ Merged dataset: {len(merged_dataset)} samples, {len(merged_dataset.columns)} features\")\n",
    "    print(f\"   Healthy (0): {(merged_dataset['condition'] == 0).sum()}\")\n",
    "    print(f\"   Unhealthy (1): {(merged_dataset['condition'] == 1).sum()}\")\n",
    "    \n",
    "    return merged_dataset\n",
    "\n",
    "\n",
    "def clean_duplicate_nans(chunk_iterator):\n",
    "    \"\"\"\n",
    "    Processes an iterator of DataFrame chunks to drop duplicates and NaNs.\n",
    "    Yields cleaned DataFrame chunks one at a time.\n",
    "    \"\"\"\n",
    "    print(\"Cleaning chunks by dropping NaNs and duplicates...\", flush=True)\n",
    "\n",
    "    chunks_processed = 0\n",
    "    for i, chunk_df in enumerate(chunk_iterator):\n",
    "        original_rows = len(chunk_df)\n",
    "\n",
    "        chunk_df = chunk_df.dropna()\n",
    "\n",
    "        if len(chunk_df) < original_rows:\n",
    "            print(f\"    Chunk {i+1}: Dropped {original_rows - len(chunk_df)} rows with null values.\")\n",
    "\n",
    "        if chunk_df.empty:\n",
    "            print(f\"    Chunk {i+1}: Empty after dropping NaNs, skipping...\", flush=True)\n",
    "            continue\n",
    "\n",
    "        rows_before_dedup = len(chunk_df)\n",
    "        chunk_df = chunk_df.drop_duplicates()\n",
    "\n",
    "        if rows_before_dedup > len(chunk_df):\n",
    "            print(f\"    Chunk {i+1}: Dropped {rows_before_dedup - len(chunk_df)} duplicate rows.\")\n",
    "\n",
    "        if not chunk_df.empty:\n",
    "            chunks_processed += 1\n",
    "            print(f\"    Chunk {i+1}: Yielded chunk with shape {chunk_df.shape}\")\n",
    "            yield chunk_df\n",
    "        else:\n",
    "            print(f\"    Chunk {i+1}: Empty after deduplication, skipping...\")\n",
    "\n",
    "    print(f\"Finished processing. {chunks_processed} non-empty chunks processed.\")\n",
    "\n",
    "\n",
    "def rename_index(chunk_iterator, index_name):\n",
    "    \"\"\"\n",
    "    Rename the index of DataFrame chunks.\n",
    "    \"\"\"\n",
    "    for chunk in chunk_iterator:\n",
    "        chunk.index.name = index_name\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "def filter_rows(chunk_iterator):\n",
    "    \"\"\"\n",
    "    Processes an iterator of DataFrame chunks to filter out rows where all data columns are zero.\n",
    "    Yields filtered DataFrame chunks one at a time.\n",
    "    \"\"\"\n",
    "    print(\"Filtering rows where sum of all values equals 0...\", flush=True)\n",
    "    total_rows_removed = 0\n",
    "    chunk_count = 0\n",
    "    \n",
    "    for chunk_df in chunk_iterator:\n",
    "        if chunk_df.empty:\n",
    "            continue\n",
    "\n",
    "        chunk_count += 1\n",
    "        original_rows = len(chunk_df)\n",
    "        \n",
    "        row_sums = chunk_df.sum(axis=1)\n",
    "        filtered_chunk = chunk_df[row_sums != 0]\n",
    "        \n",
    "        rows_removed = original_rows - len(filtered_chunk)\n",
    "        total_rows_removed += rows_removed\n",
    "        \n",
    "        if rows_removed > 0:\n",
    "            print(f\"  Chunk {chunk_count}: Removed {rows_removed} zero-sum rows ({len(filtered_chunk)} remaining)\")\n",
    "        else:\n",
    "            print(f\"  Chunk {chunk_count}: No zero-sum rows found ({len(filtered_chunk)} rows)\")\n",
    "        \n",
    "        if not filtered_chunk.empty:\n",
    "            yield filtered_chunk\n",
    "    \n",
    "    print(f\"Filtering complete: {total_rows_removed} total zero-sum rows removed across {chunk_count} chunks\")\n",
    "\n",
    "\n",
    "def prepare_metadata(healthy_dataset_path: str, unhealthy_dataset_path: str) -> list[str] | None:\n",
    "    output_path = \"data/merged_metadata_simple.csv\"\n",
    "\n",
    "    with open(output_path, 'w', newline='') as csvfile:\n",
    "        print(\"Creating metadata file\")\n",
    "        writer = csv.writer(csvfile, delimiter=\",\")\n",
    "        writer.writerow(['sampid', 'condition'])\n",
    "\n",
    "    def do_process(dataset_path: str, type_index: str, lock):\n",
    "        print(f\"Starting process on extracting metadata from: {dataset_path}\")\n",
    "        with open(dataset_path, mode='r') as f:\n",
    "            reader = csv.reader(f, delimiter=\",\")\n",
    "\n",
    "            # Skip header\n",
    "            next(reader)\n",
    "\n",
    "            metadata = [row[0] for row in reader]\n",
    "\n",
    "            with lock:\n",
    "                with open(output_path, mode=\"a\", newline='') as f:\n",
    "                    writer = csv.writer(f, delimiter=',')\n",
    "\n",
    "                    for row in metadata:\n",
    "                        writer.writerow([row, type_index])\n",
    "\n",
    "    lock = Lock()\n",
    "\n",
    "    p1 = Process(target=do_process, args=(healthy_dataset_path, \"healthy\", lock))\n",
    "    p2 = Process(target=do_process, args=(unhealthy_dataset_path, \"unhealthy\", lock))\n",
    "\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "\n",
    "    p1.join()\n",
    "    p2.join()\n",
    "\n",
    "\n",
    "def check_metadata(healthy_dataset_path: str, unhealthy_dataset_path: str, merged_metadata_path: str) -> None:\n",
    "    print(f\"Checking metadata...\")\n",
    "    healthy_dataset_line_count = 0\n",
    "    unhealthy_dataset_line_count = 0\n",
    "\n",
    "    with open(healthy_dataset_path, 'r') as file1:\n",
    "        reader = csv.reader(file1)\n",
    "        healthy_dataset_line_count = sum(1 for row in reader) -1\n",
    "\n",
    "        with open(unhealthy_dataset_path, 'r') as file2:\n",
    "            reader = csv.reader(file2)\n",
    "            unhealthy_dataset_line_count = sum(1 for row in reader) - 1\n",
    "\n",
    "            total = healthy_dataset_line_count + unhealthy_dataset_line_count\n",
    "\n",
    "            with open(merged_metadata_path, 'r') as file3:\n",
    "                reader = csv.reader(file3)\n",
    "                metadata_line_count = sum(1 for row in reader) - 1\n",
    "\n",
    "                if (total != metadata_line_count):\n",
    "                    raise Exception(f\"Healthy line count: {healthy_dataset_line_count} | Unhealthy line count: {unhealthy_dataset_line_count} | Total line count: {total} | Metadata line count: {metadata_line_count}\")\n",
    "                else:\n",
    "                    print(\"All good\")\n",
    "                    print(f\"Stats: Healthy line count: {healthy_dataset_line_count} | Unhealthy line count: {unhealthy_dataset_line_count} | Total line count: {total} | Metadata line count: {metadata_line_count}\")\n",
    "\n",
    "def get_healthy_whole_blood_samples(metadata_path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Get healthy whole blood samples from the metadata.\n",
    "\n",
    "    Args:\n",
    "        metadata_path (str): Path to the metadata CSV file\n",
    "        gtex_blood_ids (list[str]): List of GTEx whole blood sample IDs\n",
    "\n",
    "    Returns:\n",
    "        list[str]: List of healthy whole blood sample IDs\n",
    "    \"\"\"\n",
    "    metadata_df = pd.read_csv(metadata_path, index_col=0)\n",
    "\n",
    "    # Get all samples with 'Whole Blood' SMTSD\n",
    "    whole_blood_samples = metadata_df[metadata_df['SMTSD'] == 'Whole Blood']\n",
    "    # Filter on RNA integrity (SMRIN) to remove low quality samples\n",
    "    healthy_samples = whole_blood_samples[\"SMRIN\"] >= 7.0\n",
    "\n",
    "    # Return SAMPIDs of healthy whole blood samples\n",
    "    healthy_whole_blood_samples = healthy_samples.index.tolist()\n",
    "    return healthy_whole_blood_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_gene_columns_simple(healthy_chunk_iterator, unhealthy_chunk_iterator, \n",
    "                              gene_column_prefix=\"ENSG\"):\n",
    "    \"\"\"\n",
    "    Aligns gene columns between healthy and unhealthy datasets.\n",
    "    \n",
    "    Args:\n",
    "        healthy_chunk_iterator: Iterator for healthy dataset chunks\n",
    "        unhealthy_chunk_iterator: Iterator for unhealthy dataset chunks\n",
    "        gene_column_prefix (str): Prefix to identify gene columns (default: \"ENSG\")\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (aligned_healthy_chunks, aligned_unhealthy_chunks, alignment_info)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting gene column alignment...\")\n",
    "    \n",
    "    print(\"\\nLoading first chunks from both datasets...\")\n",
    "    try:\n",
    "        first_healthy_chunk = next(healthy_chunk_iterator)\n",
    "        print(f\"   ✓ Loaded healthy dataset first chunk: {first_healthy_chunk.shape}\")\n",
    "        \n",
    "        first_unhealthy_chunk = next(unhealthy_chunk_iterator)\n",
    "        print(f\"   ✓ Loaded unhealthy dataset first chunk: {first_unhealthy_chunk.shape}\")\n",
    "        \n",
    "    except StopIteration:\n",
    "        print(\"ERROR: One or both datasets are empty!\")\n",
    "        return [], [], {}\n",
    "    \n",
    "\n",
    "    print(\"\\nProcessing gene columns and stripping version suffixes...\")\n",
    "    def extract_gene_info(chunk, dataset_name):\n",
    "        \"\"\"Extract gene columns and create mapping from original_column to base_id\"\"\"\n",
    "\n",
    "        print(f\"Processing {dataset_name} dataset...\")\n",
    "\n",
    "        original_to_base = {}\n",
    "        base_gene_ids = set()\n",
    "        total_gene_columns = 0\n",
    "        duplicate_count = 0\n",
    "        \n",
    "        for column in chunk.columns:\n",
    "            if isinstance(column, str) and column.startswith(gene_column_prefix):\n",
    "                total_gene_columns += 1\n",
    "                base_gene_id = column.split('.')[0]\n",
    "                \n",
    "                # Handle duplicates (keep first occurrence)\n",
    "                if base_gene_id not in base_gene_ids:\n",
    "                    original_to_base[column] = base_gene_id\n",
    "                    base_gene_ids.add(base_gene_id)\n",
    "                else:\n",
    "                    duplicate_count += 1\n",
    "                    print(f\"Warning: Duplicate base gene {base_gene_id} found, skipping {column}\")\n",
    "        \n",
    "        print(f\"Total gene columns found: {total_gene_columns}\")\n",
    "        print(f\"Unique base gene IDs: {len(base_gene_ids)}\")\n",
    "        if duplicate_count > 0:\n",
    "            print(f\"Duplicates removed: {duplicate_count}\")\n",
    "\n",
    "        return original_to_base, base_gene_ids\n",
    "    \n",
    "    # Process both datasets\n",
    "    healthy_rename_mapping, healthy_base_genes = extract_gene_info(first_healthy_chunk, \"HEALTHY\")\n",
    "    unhealthy_rename_mapping, unhealthy_base_genes = extract_gene_info(first_unhealthy_chunk, \"UNHEALTHY\")\n",
    "\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"   Healthy dataset: {len(healthy_base_genes)} unique genes\")\n",
    "    print(f\"   Unhealthy dataset: {len(unhealthy_base_genes)} unique genes\")\n",
    "\n",
    "    print(\"\\nFinding common genes between datasets...\")\n",
    "    common_base_genes = healthy_base_genes & unhealthy_base_genes\n",
    "\n",
    "    if not common_base_genes:\n",
    "        print(\"ERROR: No common genes found between datasets!\")\n",
    "        return [], [], {}\n",
    "\n",
    "    print(f\"Common genes: {len(common_base_genes)}\")\n",
    "    print(f\"Genes exclusive to healthy dataset: {len(healthy_base_genes - common_base_genes)}\")\n",
    "    print(f\"Genes exclusive to unhealthy dataset: {len(unhealthy_base_genes - common_base_genes)}\")\n",
    "\n",
    "    print(\"\\nProcessing all chunks and renaming to base gene IDs...\")\n",
    "\n",
    "    def process_all_chunks(chunk_iterator, first_chunk, rename_mapping, common_genes, dataset_name):\n",
    "        \"\"\"Process all chunks: rename gene columns to base IDs and keep only common genes\"\"\"\n",
    "        print(f\"   Processing {dataset_name} dataset chunks...\")\n",
    "        \n",
    "        # Rename columns in first chunk\n",
    "        renamed_first_chunk = first_chunk.rename(columns=rename_mapping)\n",
    "        \n",
    "        # Build final column list\n",
    "        non_gene_cols = [col for col in renamed_first_chunk.columns \n",
    "                        if not (isinstance(col, str) and col.startswith(gene_column_prefix))]\n",
    "        common_gene_base_ids = sorted(common_genes)  # Sorted base gene IDs\n",
    "        final_columns = non_gene_cols + common_gene_base_ids\n",
    "        \n",
    "        print(f\"      Non-gene columns: {len(non_gene_cols)}\")\n",
    "        print(f\"      Common gene columns (base IDs): {len(common_gene_base_ids)}\")\n",
    "        print(f\"      Total columns to keep: {len(final_columns)}\")\n",
    "        \n",
    "        # Keep only desired columns\n",
    "        aligned_chunks = [renamed_first_chunk[final_columns].copy()]\n",
    "        chunk_count = 1\n",
    "        print(f\"✓ Processed chunk 1 - shape: {aligned_chunks[0].shape}\")\n",
    "\n",
    "        # Process remaining chunks\n",
    "        for chunk in chunk_iterator:\n",
    "            renamed_chunk = chunk.rename(columns=rename_mapping)\n",
    "            aligned_chunks.append(renamed_chunk[final_columns].copy())\n",
    "            chunk_count += 1\n",
    "            \n",
    "            if chunk_count % 3 == 0:\n",
    "                print(f\"✓ Processed {chunk_count} chunks so far...\")\n",
    "\n",
    "        print(f\"{dataset_name} processing complete: {chunk_count} chunks processed\")\n",
    "        return aligned_chunks\n",
    "\n",
    "    # Process both datasets\n",
    "    print(f\"\\nProcessing HEALTHY dataset...\")\n",
    "    aligned_healthy = process_all_chunks(healthy_chunk_iterator, first_healthy_chunk,\n",
    "                                    healthy_rename_mapping, common_base_genes, \"HEALTHY\")\n",
    "\n",
    "    print(f\"\\nProcessing UNHEALTHY dataset...\")\n",
    "    aligned_unhealthy = process_all_chunks(unhealthy_chunk_iterator, first_unhealthy_chunk,\n",
    "                                        unhealthy_rename_mapping, common_base_genes, \"UNHEALTHY\")\n",
    "\n",
    "    return aligned_healthy, aligned_unhealthy\n",
    "\n",
    "\n",
    "def merge_datasets(healthy_chunk_iterator, unhealthy_chunk_iterator, output_path, index=True):\n",
    "    \"\"\"\n",
    "    Merges healthy and unhealthy dataset chunks directly into a single CSV file\n",
    "    to be memory-efficient.\n",
    "    \n",
    "    Args:\n",
    "        healthy_chunk_iterator (iterator): Iterator yielding healthy DataFrame chunks.\n",
    "        unhealthy_chunk_iterator (iterator): Iterator yielding unhealthy DataFrame chunks.\n",
    "        output_path (str): Path to save the merged CSV file.\n",
    "        index (bool): Whether to write DataFrame index to CSV. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "        str: The path to the merged output file if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    print(f\"Merging datasets directly to {output_path}...\")\n",
    "    \n",
    "    if os.path.exists(output_path):\n",
    "        os.remove(output_path)\n",
    "\n",
    "    is_first_chunk_written_to_file = False\n",
    "\n",
    "    try:\n",
    "        if healthy_chunk_iterator:\n",
    "            print(\"Processing healthy dataset chunks...\")\n",
    "            for i, chunk in enumerate(healthy_chunk_iterator):\n",
    "                if not is_first_chunk_written_to_file:\n",
    "                    chunk.to_csv(output_path, mode='w', header=True, index=index)\n",
    "                    print(f\"  Written first healthy chunk (chunk {i+1}) to {output_path}\")\n",
    "                    is_first_chunk_written_to_file = True\n",
    "                else:\n",
    "                    chunk.to_csv(output_path, mode='a', header=False, index=index)\n",
    "                    print(f\"  Appended healthy chunk (chunk {i+1}) to {output_path}\")\n",
    "        else:\n",
    "            print(\"Warning: Healthy chunk iterator is None or empty.\")\n",
    "\n",
    "        if unhealthy_chunk_iterator:\n",
    "            print(\"Processing unhealthy dataset chunks...\")\n",
    "            for i, chunk in enumerate(unhealthy_chunk_iterator):\n",
    "                if not is_first_chunk_written_to_file:\n",
    "                    chunk.to_csv(output_path, mode='w', header=True, index=index)\n",
    "                    print(f\"  Written first unhealthy chunk (chunk {i+1}) as first overall chunk to {output_path}\")\n",
    "                    is_first_chunk_written_to_file = True\n",
    "                else:\n",
    "                    chunk.to_csv(output_path, mode='a', header=False, index=index)\n",
    "                    print(f\"  Appended unhealthy chunk (chunk {i+1}) to {output_path}\")\n",
    "        else:\n",
    "            print(\"Warning: Unhealthy chunk iterator is None or empty.\")\n",
    "            \n",
    "        if not is_first_chunk_written_to_file:\n",
    "            print(f\"No data chunks were processed. Output file {output_path} may not have been created or is empty.\")\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"✅ Merged dataset saved to {output_path}\")\n",
    "            return output_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during merging datasets to {output_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Initial Preprocessing Unhealthy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthy_dataset_file = 'data/rna_seq_unstranded.csv'\n",
    "unhealthy_output_file = 'data/unhealthy_data_preprocessed.pkl'\n",
    "chunk_size = 8000\n",
    "\n",
    "chunk_iterator = load_csv_in_chunks(unhealthy_dataset_file, header=0, skiprows=[1, 2], index_col=0)\n",
    "print(f\"Starting preprocessing for Unhealthy Dataset: {unhealthy_dataset_file}\")\n",
    "\n",
    "# Convert int64 columns to uint32\n",
    "\n",
    "chunk_iterator = rename_index(chunk_iterator, 'sample_id')\n",
    "chunk_iterator = clean_duplicate_nans(chunk_iterator)\n",
    "\n",
    "save_data_as_pickle(chunk_iterator, unhealthy_output_file)\n",
    "# save_data_to_csv(data_to_save=chunk_iterator, output_file_path=unhealthy_output_file, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthy_temp_file = 'data/unhealthy_data_temp.csv'\n",
    "unhealthy_output_file = 'data/unhealthy_data_preprocessed.csv'\n",
    "chunk_size = 8000\n",
    "\n",
    "transpose_csv(input_csv=unhealthy_output_file, output_csv=unhealthy_temp_file, batch_size=chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthy_chunk_iterator = load_csv_in_chunks(file_path=unhealthy_output_file, chunk_size=chunk_size, index_col=0, low_memory=False, header=0)\n",
    "unhealthy_chunk_iterator = filter_rows(unhealthy_chunk_iterator)\n",
    "\n",
    "save_data_to_csv(data_to_save=unhealthy_chunk_iterator, output_file_path=unhealthy_output_file, index=True)\n",
    "\n",
    "# Clean up\n",
    "if os.path.exists(unhealthy_temp_file):\n",
    "    os.remove(unhealthy_temp_file)\n",
    "    print(f\"Deleted temporary file: {unhealthy_temp_file}\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Initial Preprocessing Healthy Dataset\n",
    "## Convert .gct to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = 'data/GTEx_Analysis_2022-06-06_v10_RNASeQCv2.4.2_gene_reads.gct'\n",
    "output_file = 'data/healthy_data.csv'\n",
    "chunk_size = 8000\n",
    "\n",
    "gct_chunk_iterator = load_csv_in_chunks(\n",
    "    file_path=dataset_file,\n",
    "    chunk_size=chunk_size,\n",
    "    sep='\\t',\n",
    "    skiprows=2\n",
    ")\n",
    "\n",
    "if gct_chunk_iterator:\n",
    "    save_data_to_csv(\n",
    "        data_to_save=gct_chunk_iterator,\n",
    "        output_file_path=output_file,\n",
    "        delete_original_path=dataset_file,\n",
    "        index=False\n",
    "    )\n",
    "else:\n",
    "    print(f\"Failed to load {dataset_file} using load_csv_in_chunks.\")\n",
    "    \n",
    "# Clean up\n",
    "if 'gct_chunk_iterator' in locals() or 'gct_chunk_iterator' in globals():\n",
    "    del gct_chunk_iterator\n",
    "if 'dataset_file' in locals() or 'dataset_file' in globals():\n",
    "    del dataset_file\n",
    "if 'output_file' in locals() or 'output_file' in globals():\n",
    "    del output_file\n",
    "if 'read_chunk_size' in locals() or 'read_chunk_size' in globals():\n",
    "    del chunk_size\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Transpose healthy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_data_input = 'data/healthy_data.csv'\n",
    "healthy_data_output = 'data/healthy_data_transposed.csv'\n",
    "feature_id_col = 'Name'\n",
    "exclude_cols = ['Description']\n",
    "batch_size = 3000\n",
    "\n",
    "transpose_csv(\n",
    "    input_csv=healthy_data_input,\n",
    "    output_csv=healthy_data_output,\n",
    "    feature_id_col=feature_id_col,\n",
    "    exclude_cols=exclude_cols,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Load and clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_dataset_file = 'data/healthy_data_transposed.csv'\n",
    "healthy_output_file = 'data/healthy_data_preprocessed.csv'\n",
    "chunk_size = 5000\n",
    "\n",
    "chunk_iterator = load_csv_in_chunks(file_path=healthy_dataset_file, chunk_size=chunk_size, index_col=0)\n",
    "print(f\"Starting preprocessing for Healthy Dataset: {healthy_dataset_file}\")\n",
    "\n",
    "chunk_iterator = clean_duplicate_nans(chunk_iterator)\n",
    "\n",
    "save_data_to_csv(\n",
    "    data_to_save=chunk_iterator,\n",
    "    output_file_path=healthy_output_file,\n",
    "    index=True\n",
    ")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Drop genes with only zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_data_input = 'data/healthy_data_preprocessed.csv'\n",
    "healthy_dataset_temp = 'data/healthy_data_temp.csv'\n",
    "healthy_output_file = 'data/healthy_data_final.csv'\n",
    "chunk_size = 5000\n",
    "\n",
    "transpose_csv(input_csv=healthy_data_input, output_csv=healthy_dataset_temp, batch_size=chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_chunk_iterator = load_csv_in_chunks(file_path=healthy_dataset_temp, chunk_size=chunk_size, index_col=0, low_memory=False, header=0)\n",
    "healthy_chunk_iterator = filter_rows(healthy_chunk_iterator)\n",
    "\n",
    "save_data_to_csv(data_to_save=healthy_chunk_iterator, output_file_path=healthy_output_file, index=True)\n",
    "\n",
    "# Clean up temporary files\n",
    "if os.path.exists(healthy_dataset_temp):\n",
    "    os.remove(healthy_dataset_temp)\n",
    "    print(f\"Deleted temporary file: {healthy_dataset_temp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_data_input = 'data/healthy_data_preprocessed.csv'\n",
    "batch_size = 3000\n",
    "\n",
    "transpose_csv(input_csv='data/healthy_data_final.csv', output_csv=healthy_data_input, batch_size=batch_size)\n",
    "\n",
    "if os.path.exists(healthy_output_file):\n",
    "    os.remove(healthy_output_file)\n",
    "    print(f\"Deleted temporary file: {healthy_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Prepare datasets for merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_data_path = 'data/healthy_data_preprocessed.csv'\n",
    "unhealthy_data_path = 'data/unhealthy_data_preprocessed.csv'\n",
    "output_healthy_path = 'data/healthy_data_aligned.csv'\n",
    "output_unhealthy_path = 'data/unhealthy_data_aligned.csv'\n",
    "metadata_path = 'data/gtex_blood_samples_metadata.csv'\n",
    "chunk_size = 1000\n",
    "\n",
    "whole_blood_samples = get_healthy_whole_blood_samples(metadata_path)\n",
    "print(f\"Found {len(whole_blood_samples)} healthy whole blood samples in metadata\")\n",
    "\n",
    "# Load datasets\n",
    "print(\"\\nLoading datasets...\")\n",
    "print(f\"   Healthy data: {healthy_data_path}\")\n",
    "print(f\"   Unhealthy data: {unhealthy_data_path}\")\n",
    "print(f\"   Chunk size: {chunk_size}\")\n",
    "\n",
    "healthy_chunks = load_csv_in_chunks(healthy_data_path, chunk_size=chunk_size, index_col=0)\n",
    "unhealthy_chunks = load_csv_in_chunks(unhealthy_data_path, chunk_size=chunk_size, index_col=0)\n",
    "\n",
    "if healthy_chunks is None or unhealthy_chunks is None:\n",
    "    print(\"ERROR: Failed to load one or both datasets\")\n",
    "else:\n",
    "    print(\"Both datasets loaded successfully\")\n",
    "\n",
    "    # Align gene columns (strips version suffixes and keeps intersection only)\n",
    "    print(\"\\nStarting gene alignment...\")\n",
    "    aligned_healthy, aligned_unhealthy = align_gene_columns_simple(\n",
    "        healthy_chunks, unhealthy_chunks\n",
    "    )\n",
    "\n",
    "    if aligned_healthy and aligned_unhealthy:\n",
    "        print(f\"   Saving healthy dataset to: {output_healthy_path}\")\n",
    "        save_data_to_csv(aligned_healthy, output_healthy_path, index=True)\n",
    "\n",
    "        print(f\"   Saving unhealthy dataset to: {output_unhealthy_path}\")\n",
    "        save_data_to_csv(aligned_unhealthy, output_unhealthy_path, index=True)\n",
    "\n",
    "        print(\"Gene alignment completed successfully!\")\n",
    "    else:\n",
    "        print(\"ERROR: Gene alignment failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata preparation\n",
    "healthy_path = 'data/healthy_data_aligned.csv'\n",
    "unhealthy_path = 'data/unhealthy_data_aligned.csv'\n",
    "metadata_path = 'data/merged_metadata_simple.csv'\n",
    "prepare_metadata(healthy_dataset_path=healthy_path, unhealthy_dataset_path=unhealthy_path)\n",
    "\n",
    "# Metadata check\n",
    "check_metadata(healthy_dataset_path=healthy_path, unhealthy_dataset_path=unhealthy_path, merged_metadata_path=metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "healthy_data_path = 'data/healthy_data_aligned.csv'\n",
    "unhealthy_data_path = 'data/unhealthy_data_aligned.csv'\n",
    "merged_data_path = 'data/merged_dataset.csv'\n",
    "healthy_chunk_iterator = load_csv_in_chunks(healthy_data_path, chunk_size=2000, index_col=0)\n",
    "unhealthy_chunk_iterator = load_csv_in_chunks(unhealthy_data_path, chunk_size=2000, index_col=0)\n",
    "\n",
    "merged_dataset = merge_datasets(healthy_chunk_iterator, unhealthy_chunk_iterator, merged_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# Preprocessing steps after PyDESeq2\n",
    "\n",
    "- Possibly transpose, depends on results from PyDESeq2\n",
    "- Load as dataframe\n",
    "- Add labels\n",
    "- Convert gene IDs to corrosponding symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ''\n",
    "chunk_size = 5000\n",
    "\n",
    "healthy_chunk_iterator = load_csv_in_chunks(dataset, chunk_size=chunk_size, index_col=0, header=0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
