{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Imports and modular functions\n",
    "\n",
    "*Always run this if changes are made to these functions or if **python kernel crashes*** ðŸ˜ðŸ˜„ðŸ˜ƒðŸ™‚ðŸ˜ðŸ™â˜¹ï¸ðŸ˜¢ðŸ˜­ðŸ’€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Optional, Union\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import mygene\n",
    "import os\n",
    "import gc\n",
    "\n",
    "def load_csv_in_chunks(file_path, chunk_size=8000, **kwargs):\n",
    "    \"\"\"\n",
    "    Loads a CSV file in chunks to avoid memory issues.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "        chunk_size (int): The number of rows per chunk.\n",
    "        **kwargs: Additional keyword arguments to pass to pd.read_csv()\n",
    "                  (e.g., sep=',', header=0, index_col=None, usecols=None).\n",
    "\n",
    "    Returns:\n",
    "        A pandas TextFileReader object (iterator) that yields DataFrame chunks\n",
    "        if the file exists, otherwise None.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Preparing to load {file_path} in chunks of size {chunk_size}...\")\n",
    "    try:\n",
    "        chunk_iterator = pd.read_csv(file_path, chunksize=chunk_size, **kwargs)\n",
    "        return chunk_iterator\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Clean up\n",
    "    finally:\n",
    "        if 'chunk_iterator' in locals():\n",
    "            del chunk_iterator\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def load_parquet_in_chunks(file_path, chunk_size=8000):\n",
    "    \"\"\"\n",
    "    Loads a parquet file in true chunks to avoid memory issues.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the parquet file.\n",
    "        chunk_size (int): The number of rows per chunk.\n",
    "\n",
    "    Yields:\n",
    "        pandas.DataFrame: DataFrame chunks of the specified size.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Preparing to load {file_path} in chunks of size {chunk_size}...\")\n",
    "\n",
    "    try:\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        total_rows = parquet_file.metadata.num_rows\n",
    "        print(f\"Total rows in file: {total_rows}\")\n",
    "\n",
    "        for batch in parquet_file.iter_batches(batch_size=chunk_size):\n",
    "            chunk_df = batch.to_pandas()\n",
    "            yield chunk_df\n",
    "\n",
    "            del chunk_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading parquet file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Clean up\n",
    "    finally:\n",
    "        if 'parquet_file' in locals():\n",
    "            del parquet_file\n",
    "        if 'batch' in locals():\n",
    "            del batch\n",
    "        if 'chunk_df' in locals():\n",
    "            del chunk_df\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def save_data_as_parquet(chunk_iterator, output_parquet_path, preserve_index=True):\n",
    "    \"\"\"\n",
    "    Process DataFrame chunks and save as parquet using PyArrow for maximum efficiency.\n",
    "    Handles index preservation properly across chunks.\n",
    "\n",
    "    Args:\n",
    "        chunk_iterator: Iterator yielding DataFrame chunks\n",
    "        output_parquet_path (str): Path to save the parquet file\n",
    "        preserve_index (bool): Whether to preserve the DataFrame index\n",
    "    \"\"\"\n",
    "    print(f\"Processing chunks and saving to {output_parquet_path}...\")\n",
    "\n",
    "    writer = None\n",
    "    total_rows = 0\n",
    "    all_index_values = set()\n",
    "\n",
    "    try:\n",
    "        for chunk_idx, chunk in enumerate(chunk_iterator):\n",
    "            if chunk.empty:\n",
    "                continue\n",
    "\n",
    "            if preserve_index and chunk.index.name is not None:\n",
    "                chunk_index_values = set(chunk.index)\n",
    "                duplicates = all_index_values.intersection(chunk_index_values)\n",
    "                if duplicates:\n",
    "                    print(f\"Warning: Found {len(duplicates)} duplicate index values in chunk {chunk_idx}\")\n",
    "                    print(f\"First few duplicates: {list(duplicates)[:5]}\")\n",
    "                all_index_values.update(chunk_index_values)\n",
    "\n",
    "            table = pa.Table.from_pandas(chunk, preserve_index=preserve_index)\n",
    "\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(output_parquet_path, table.schema)\n",
    "                if preserve_index and chunk.index.name:\n",
    "                    print(f\"Preserving index: '{chunk.index.name}' (dtype: {chunk.index.dtype})\")\n",
    "\n",
    "            writer.write_table(table)\n",
    "            total_rows += chunk.shape[0]\n",
    "\n",
    "            del table\n",
    "\n",
    "    finally:\n",
    "        if writer:\n",
    "            writer.close()\n",
    "\n",
    "        # Clean up\n",
    "        if 'writer' in locals():\n",
    "            del writer\n",
    "        if 'chunk_iterator' in locals():\n",
    "            del chunk_iterator\n",
    "        gc.collect()\n",
    "\n",
    "    if total_rows > 0:\n",
    "        print(f\"Successfully saved {total_rows} rows to parquet\")\n",
    "        if preserve_index:\n",
    "            print(f\"Total unique index values: {len(all_index_values)}\")\n",
    "    else:\n",
    "        print(\"No data to save!\")\n",
    "\n",
    "\n",
    "def drop_dataframe_chunks(\n",
    "    chunk_generator: Iterator[pd.DataFrame],\n",
    "    drop_rows: Optional[Union[List[Union[int, str]], List[int], List[str]]] = None,\n",
    "    drop_columns: Optional[Union[List[Union[int, str]], List[int], List[str]]] = None\n",
    ") -> Iterator[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generator function that drops specified rows and/or columns from DataFrame chunks\n",
    "    in a memory-efficient way. Designed to be chained with other generators.\n",
    "\n",
    "    Args:\n",
    "        chunk_generator: An iterable that yields pandas DataFrame chunks\n",
    "        drop_rows: List of row indices/names to drop, or None to keep all rows\n",
    "        drop_columns: List of column indices/names to drop, or None to keep all columns\n",
    "\n",
    "    Yields:\n",
    "        pd.DataFrame: Chunks with specified rows/columns dropped\n",
    "    \"\"\"\n",
    "    for chunk in chunk_generator:\n",
    "        if chunk.empty:\n",
    "            yield chunk\n",
    "            continue\n",
    "\n",
    "        processed_chunk = chunk.copy()\n",
    "\n",
    "        if drop_columns is not None:\n",
    "            columns_to_drop = [col for col in drop_columns if col in processed_chunk.columns]\n",
    "            if columns_to_drop:\n",
    "                processed_chunk = processed_chunk.drop(columns=columns_to_drop)\n",
    "\n",
    "        if drop_rows is not None:\n",
    "            if drop_rows and not all(isinstance(row, int) for row in drop_rows):\n",
    "                chunk_index_str = processed_chunk.index.astype(str)\n",
    "                drop_rows_str = [str(row) for row in drop_rows]\n",
    "\n",
    "                rows_to_drop = processed_chunk.index[chunk_index_str.isin(drop_rows_str)]\n",
    "                if len(rows_to_drop) > 0:\n",
    "                    processed_chunk = processed_chunk.drop(index=rows_to_drop)\n",
    "            else:\n",
    "                valid_indices = [idx for idx in drop_rows if 0 <= idx < len(processed_chunk)]\n",
    "                if valid_indices:\n",
    "                    processed_chunk = processed_chunk.drop(processed_chunk.index[valid_indices])\n",
    "\n",
    "        if not processed_chunk.empty:\n",
    "            yield processed_chunk\n",
    "\n",
    "\n",
    "def keep_dataframe_chunks(\n",
    "    chunk_generator: Iterator[pd.DataFrame],\n",
    "    keep_rows: Optional[Union[List[Union[int, str]], List[int], List[str]]] = None,\n",
    "    keep_columns: Optional[Union[List[Union[int, str]], List[int], List[str]]] = None\n",
    ") -> Iterator[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generator function that keeps only specified rows and/or columns from DataFrame chunks\n",
    "    in a memory-efficient way. Designed to be chained with other generators.\n",
    "\n",
    "    Args:\n",
    "        chunk_generator: An iterable that yields pandas DataFrame chunks\n",
    "        keep_rows: List of row indices/names to keep, or None to keep all rows\n",
    "        keep_columns: List of column indices/names to keep, or None to keep all columns\n",
    "\n",
    "    Yields:\n",
    "        pd.DataFrame: Chunks with only specified rows/columns kept\n",
    "    \"\"\"\n",
    "    for chunk in chunk_generator:\n",
    "        if chunk.empty:\n",
    "            yield chunk\n",
    "            continue\n",
    "\n",
    "        processed_chunk = chunk.copy()\n",
    "\n",
    "        if keep_columns is not None:\n",
    "            columns_to_keep = [col for col in keep_columns if col in processed_chunk.columns]\n",
    "            if columns_to_keep:\n",
    "                processed_chunk = processed_chunk[columns_to_keep]\n",
    "            else:\n",
    "                processed_chunk = processed_chunk.iloc[:0]\n",
    "\n",
    "        if keep_rows is not None:\n",
    "            if keep_rows and not all(isinstance(row, int) for row in keep_rows):\n",
    "                chunk_index_str = processed_chunk.index.astype(str)\n",
    "                keep_rows_str = [str(row) for row in keep_rows]\n",
    "\n",
    "                rows_to_keep = processed_chunk.index[chunk_index_str.isin(keep_rows_str)]\n",
    "                if len(rows_to_keep) > 0:\n",
    "                    processed_chunk = processed_chunk.loc[rows_to_keep]\n",
    "                else:\n",
    "                    processed_chunk = processed_chunk.iloc[:0]\n",
    "            else:\n",
    "                valid_indices = [idx for idx in keep_rows if 0 <= idx < len(processed_chunk)]\n",
    "                if valid_indices:\n",
    "                    processed_chunk = processed_chunk.iloc[valid_indices]\n",
    "                else:\n",
    "                    processed_chunk = processed_chunk.iloc[:0]\n",
    "\n",
    "        if not processed_chunk.empty:\n",
    "            yield processed_chunk\n",
    "\n",
    "\n",
    "def transpose_dataframe_chunks(\n",
    "    chunk_generator,\n",
    "    skip_rows=None,\n",
    "    skip_columns=None,\n",
    "    output_batch_size=1000,\n",
    "    temp_dir=\"/tmp\",\n",
    "    dtype='uint32'\n",
    "):\n",
    "    \"\"\"\n",
    "    Generator function that collects DataFrame chunks, transposes the complete dataset,\n",
    "    and yields the transposed result in batches. Designed to be chained with other generators.\n",
    "\n",
    "    Args:\n",
    "        chunk_generator: An iterable that yields pandas DataFrame chunks\n",
    "        skip_rows: List of row indices/names to skip, or None\n",
    "        skip_columns: List of column indices/names to skip, or None\n",
    "        output_batch_size: Number of rows to yield in each output batch\n",
    "        temp_dir: Directory for temporary memory-mapped file\n",
    "        dtype: Data type for memory-mapped array (default: 'uint32')\n",
    "\n",
    "    Yields:\n",
    "        pd.DataFrame: Batches of the transposed DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Starting memory-mapped transposition...\")\n",
    "\n",
    "    chunk_list = list(chunk_generator)\n",
    "    if not chunk_list:\n",
    "        print(\"No chunks received from generator. Exiting.\")\n",
    "        return\n",
    "\n",
    "    first_chunk = chunk_list[0].copy()\n",
    "\n",
    "    if skip_columns is not None:\n",
    "        columns_to_keep = [col for col in first_chunk.columns if col not in skip_columns]\n",
    "        first_chunk = first_chunk[columns_to_keep]\n",
    "        print(f\"Skipping columns: {skip_columns}\")\n",
    "\n",
    "    if skip_rows is not None:\n",
    "        if isinstance(skip_rows[0], str):\n",
    "            rows_to_keep = [idx for idx in first_chunk.index if idx not in skip_rows]\n",
    "        else:\n",
    "            rows_to_keep = [idx for i, idx in enumerate(first_chunk.index) if i not in skip_rows]\n",
    "        first_chunk = first_chunk.loc[rows_to_keep]\n",
    "        print(f\"Skipping rows: {skip_rows}\")\n",
    "\n",
    "    original_rows = sum(len(chunk) for chunk in chunk_list)\n",
    "    original_cols = len(first_chunk.columns)\n",
    "\n",
    "    filtered_cols = first_chunk.columns.tolist()\n",
    "    n_output_rows = len(filtered_cols)\n",
    "\n",
    "    all_row_indices = []\n",
    "    for chunk in chunk_list:\n",
    "        chunk_filtered = chunk.copy()\n",
    "\n",
    "        if skip_columns is not None:\n",
    "            chunk_filtered = chunk_filtered[filtered_cols]\n",
    "        if skip_rows is not None:\n",
    "            if isinstance(skip_rows[0], str):\n",
    "                chunk_rows_to_keep = [idx for idx in chunk_filtered.index if idx not in skip_rows]\n",
    "            else:\n",
    "                chunk_rows_to_keep = [idx for i, idx in enumerate(chunk_filtered.index) if i not in skip_rows]\n",
    "            chunk_filtered = chunk_filtered.loc[chunk_rows_to_keep]\n",
    "\n",
    "        all_row_indices.extend(chunk_filtered.index.tolist())\n",
    "\n",
    "    n_output_cols = len(all_row_indices)\n",
    "\n",
    "    print(f\"Dataset dimensions:\")\n",
    "    print(f\"    Original: {original_rows} rows x {original_cols} columns\")\n",
    "    print(f\"    After filtering: {len(all_row_indices)} rows x {len(filtered_cols)} columns\")\n",
    "    print(f\"    After transpose: {n_output_rows} rows x {n_output_cols} columns\")\n",
    "\n",
    "    # Create memory-mapped file\n",
    "    try:\n",
    "        temp_file = tempfile.NamedTemporaryFile(\n",
    "            dir=temp_dir,\n",
    "            delete=False,\n",
    "            suffix='.mmap'\n",
    "        )\n",
    "        temp_filename = temp_file.name\n",
    "        temp_file.close()\n",
    "\n",
    "        # Create memory-mapped array in transposed orientation: [samples, features]\n",
    "        mmap_array = np.memmap(\n",
    "            temp_filename,\n",
    "            dtype=dtype,\n",
    "            mode='w+',\n",
    "            shape=(n_output_rows, n_output_cols)\n",
    "        )\n",
    "        print(f\"Memory map created successfully\")\n",
    "\n",
    "        print(f\"Filling memory map with data...\")\n",
    "        current_feature_idx = 0\n",
    "\n",
    "        for chunk_idx, chunk in enumerate(chunk_list):\n",
    "            chunk_filtered = chunk.copy()\n",
    "            if skip_columns is not None:\n",
    "                chunk_filtered = chunk_filtered[filtered_cols]\n",
    "            if skip_rows is not None:\n",
    "                if isinstance(skip_rows[0], str):\n",
    "                    chunk_rows_to_keep = [idx for idx in chunk_filtered.index if idx not in skip_rows]\n",
    "                else:\n",
    "                    chunk_rows_to_keep = [idx for i, idx in enumerate(chunk_filtered.index) if i not in skip_rows]\n",
    "                chunk_filtered = chunk_filtered.loc[chunk_rows_to_keep]\n",
    "\n",
    "            chunk_data = chunk_filtered.values.T.astype(dtype)\n",
    "            chunk_feature_count = chunk_filtered.shape[0]\n",
    "\n",
    "            mmap_array[:, current_feature_idx:current_feature_idx + chunk_feature_count] = chunk_data\n",
    "            current_feature_idx += chunk_feature_count\n",
    "\n",
    "            mmap_array.flush()\n",
    "            del chunk_data, chunk_filtered\n",
    "\n",
    "            print(f\"    Processed chunk {chunk_idx + 1}/{len(chunk_list)}\")\n",
    "\n",
    "        del chunk_list\n",
    "        gc.collect()\n",
    "\n",
    "        print(f\"Memory map filled successfully\")\n",
    "\n",
    "        print(f\"Yielding transposed batches (size: {output_batch_size})...\")\n",
    "        total_batches = (n_output_rows + output_batch_size - 1) // output_batch_size\n",
    "\n",
    "        for batch_idx in range(total_batches):\n",
    "            start_row = batch_idx * output_batch_size\n",
    "            end_row = min(start_row + output_batch_size, n_output_rows)\n",
    "\n",
    "            batch_data = mmap_array[start_row:end_row, :].copy()\n",
    "            batch_sample_ids = filtered_cols[start_row:end_row]\n",
    "\n",
    "            batch_df = pd.DataFrame(\n",
    "                data=batch_data,\n",
    "                index=batch_sample_ids,\n",
    "                columns=all_row_indices\n",
    "            )\n",
    "\n",
    "            batch_df = batch_df.reset_index().rename(columns={'index': 'sample_id'})\n",
    "\n",
    "            yield batch_df\n",
    "\n",
    "            del batch_data, batch_df\n",
    "            gc.collect()\n",
    "\n",
    "            print(f\"    Yielded batch {batch_idx + 1}/{total_batches}\")\n",
    "\n",
    "        print(f\"Transposition completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during memory-mapped transposition: {e}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        print(f\"Cleaning up temporary files...\")\n",
    "        try:\n",
    "            if 'mmap_array' in locals():\n",
    "                del mmap_array\n",
    "            if 'temp_filename' in locals() and os.path.exists(temp_filename):\n",
    "                os.unlink(temp_filename)\n",
    "                print(f\"Temporary file removed: {temp_filename}\")\n",
    "        except Exception as cleanup_error:\n",
    "            print(f\"Warning: Could not clean up temp file: {cleanup_error}\")\n",
    "\n",
    "\n",
    "def convert_gene_ids_to_symbols(dataset_chunk_iterator, mygene_client, gene_column_prefix=\"ENSG\"):\n",
    "    \"\"\"\n",
    "    Convert gene IDs to gene symbols using MyGene API in a memory-efficient way.\n",
    "    Only calls MyGene API once for all gene columns from the first chunk.\n",
    "\n",
    "    Args:\n",
    "        dataset_chunk_iterator: Iterator yielding DataFrame chunks\n",
    "        mygene_client: MyGene client instance\n",
    "        gene_column_prefix (str): Prefix to identify gene columns\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Complete dataset with gene symbols as column names\n",
    "    \"\"\"\n",
    "    print(\"Converting gene IDs to symbols (chunked processing)...\")\n",
    "\n",
    "    # Get first chunk to determine gene columns and create mapping\n",
    "    try:\n",
    "        first_chunk = next(dataset_chunk_iterator)\n",
    "        print(f\"Processing first chunk: {first_chunk.shape}\")\n",
    "    except StopIteration:\n",
    "        print(\"âŒ Error: Dataset is empty\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Identify gene columns from first chunk\n",
    "    gene_columns = [col for col in first_chunk.columns\n",
    "                   if isinstance(col, str) and col.startswith(gene_column_prefix)]\n",
    "\n",
    "    if not gene_columns:\n",
    "        print(\"No gene columns found - returning original data\")\n",
    "        # Concatenate all chunks and return\n",
    "        all_chunks = [first_chunk]\n",
    "        for chunk in dataset_chunk_iterator:\n",
    "            all_chunks.append(chunk)\n",
    "        return pd.concat(all_chunks, axis=0, ignore_index=False)\n",
    "\n",
    "    print(f\"Found {len(gene_columns)} gene columns\")\n",
    "\n",
    "    # Single MyGene API call for all gene IDs\n",
    "    gene_id_to_symbol = {}\n",
    "    symbols_found = 0\n",
    "\n",
    "    try:\n",
    "        print(\"Making single MyGene API call...\")\n",
    "        import warnings\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            results = mygene_client.querymany(\n",
    "                gene_columns,\n",
    "                scopes='ensembl.gene',\n",
    "                fields='symbol',\n",
    "                species='human',\n",
    "                verbose=False,\n",
    "                silent=True\n",
    "            )\n",
    "\n",
    "        # Process API results\n",
    "        for result in results:\n",
    "            gene_id = result['query']\n",
    "            if 'symbol' in result and result['symbol']:\n",
    "                gene_id_to_symbol[gene_id] = result['symbol']\n",
    "                symbols_found += 1\n",
    "            else:\n",
    "                gene_id_to_symbol[gene_id] = gene_id\n",
    "\n",
    "        print(f\"âœ… Successfully converted {symbols_found}/{len(gene_columns)} genes to symbols\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ MyGene API error: {e}. Using original gene IDs\")\n",
    "        gene_id_to_symbol = {gene_id: gene_id for gene_id in gene_columns}\n",
    "\n",
    "    # Create final column mapping (preserve non-gene columns like 'condition')\n",
    "    final_column_mapping = {}\n",
    "    for col in first_chunk.columns:\n",
    "        if col == 'condition':\n",
    "            final_column_mapping[col] = col\n",
    "        elif col in gene_id_to_symbol:\n",
    "            final_column_mapping[col] = gene_id_to_symbol[col]\n",
    "        else:\n",
    "            final_column_mapping[col] = col\n",
    "\n",
    "    print(\"Applying gene symbol mapping to all chunks...\")\n",
    "\n",
    "    # Process first chunk\n",
    "    renamed_first_chunk = first_chunk.rename(columns=final_column_mapping)\n",
    "    processed_chunks = [renamed_first_chunk]\n",
    "    chunk_count = 1\n",
    "\n",
    "    # Process remaining chunks with same mapping\n",
    "    for chunk in dataset_chunk_iterator:\n",
    "        renamed_chunk = chunk.rename(columns=final_column_mapping)\n",
    "        processed_chunks.append(renamed_chunk)\n",
    "        chunk_count += 1\n",
    "\n",
    "        if chunk_count % 3 == 0:\n",
    "            print(f\"  âœ“ Processed {chunk_count} chunks...\")\n",
    "\n",
    "    print(f\"Concatenating {chunk_count} processed chunks...\")\n",
    "    final_dataset = pd.concat(processed_chunks, axis=0, ignore_index=False)\n",
    "\n",
    "    print(f\"âœ… Final dataset shape: {final_dataset.shape}\")\n",
    "    print(f\"   Symbols converted: {symbols_found}/{len(gene_columns)}\")\n",
    "\n",
    "    return final_dataset\n",
    "\n",
    "\n",
    "def add_condition_labels_to_chunks(chunk_iterator, condition_label, dataset_name):\n",
    "    \"\"\"\n",
    "    Add condition labels to dataset chunks.\n",
    "\n",
    "    Args:\n",
    "        chunk_iterator: Iterator yielding DataFrame chunks\n",
    "        condition_label (int): Binary label (0 for healthy, 1 for unhealthy)\n",
    "        dataset_name (str): Name for logging purposes\n",
    "\n",
    "    Returns:\n",
    "        list: List of labeled DataFrame chunks\n",
    "    \"\"\"\n",
    "    print(f\"Adding label '{condition_label}' to {dataset_name} dataset...\")\n",
    "\n",
    "    labeled_chunks = []\n",
    "    for chunk in chunk_iterator:\n",
    "        chunk['condition'] = condition_label\n",
    "        labeled_chunks.append(chunk)\n",
    "\n",
    "    print(f\"âœ… Completed {len(labeled_chunks)} {dataset_name} chunks\")\n",
    "    return labeled_chunks\n",
    "\n",
    "\n",
    "def merge_labeled_datasets(healthy_chunks, unhealthy_chunks):\n",
    "    \"\"\"\n",
    "    Merge healthy and unhealthy dataset chunks into one DataFrame.\n",
    "\n",
    "    Args:\n",
    "        healthy_chunks (list): List of healthy DataFrame chunks\n",
    "        unhealthy_chunks (list): List of unhealthy DataFrame chunks\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Merged dataset\n",
    "    \"\"\"\n",
    "    print(\"Merging datasets...\")\n",
    "\n",
    "    all_chunks = healthy_chunks + unhealthy_chunks\n",
    "    merged_dataset = pd.concat(all_chunks, axis=0, ignore_index=False)\n",
    "\n",
    "    print(f\"âœ… Merged dataset: {len(merged_dataset)} samples, {len(merged_dataset.columns)} features\")\n",
    "    print(f\"   Healthy (0): {(merged_dataset['condition'] == 0).sum()}\")\n",
    "    print(f\"   Unhealthy (1): {(merged_dataset['condition'] == 1).sum()}\")\n",
    "\n",
    "    return merged_dataset\n",
    "\n",
    "\n",
    "def clean_duplicate_nans(chunk_iterator):\n",
    "    \"\"\"\n",
    "    Processes an iterator of DataFrame chunks to drop duplicates and NaNs.\n",
    "    Yields cleaned DataFrame chunks one at a time.\n",
    "    \"\"\"\n",
    "    print(\"Cleaning chunks by dropping NaNs and duplicates...\", flush=True)\n",
    "\n",
    "    chunks_processed = 0\n",
    "    for i, chunk_df in enumerate(chunk_iterator):\n",
    "        original_rows = len(chunk_df)\n",
    "\n",
    "        chunk_df = chunk_df.dropna()\n",
    "\n",
    "        if len(chunk_df) < original_rows:\n",
    "            print(f\"    Chunk {i+1}: Dropped {original_rows - len(chunk_df)} rows with null values.\")\n",
    "\n",
    "        if chunk_df.empty:\n",
    "            print(f\"    Chunk {i+1}: Empty after dropping NaNs, skipping...\", flush=True)\n",
    "            continue\n",
    "\n",
    "        rows_before_dedup = len(chunk_df)\n",
    "        chunk_df = chunk_df.drop_duplicates()\n",
    "\n",
    "        if rows_before_dedup > len(chunk_df):\n",
    "            print(f\"    Chunk {i+1}: Dropped {rows_before_dedup - len(chunk_df)} duplicate rows.\")\n",
    "\n",
    "        if not chunk_df.empty:\n",
    "            chunks_processed += 1\n",
    "            print(f\"    Chunk {i+1}: Yielded chunk with shape {chunk_df.shape}\")\n",
    "            yield chunk_df\n",
    "        else:\n",
    "            print(f\"    Chunk {i+1}: Empty after deduplication, skipping...\")\n",
    "\n",
    "    print(f\"Finished processing. {chunks_processed} non-empty chunks processed.\")\n",
    "\n",
    "\n",
    "def rename_index(chunk_iterator, index_name):\n",
    "    \"\"\"\n",
    "    Rename the index of DataFrame chunks.\n",
    "    \"\"\"\n",
    "    for chunk in chunk_iterator:\n",
    "        chunk.index.name = index_name\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "def filter_rows(chunk_iterator):\n",
    "    \"\"\"\n",
    "    Generator function that filters out rows where the sum of all numeric values equals 0.\n",
    "    Excludes the sample_id column from sum calculation.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Filtering rows where sum of all numeric values equals 0...\")\n",
    "\n",
    "    total_rows_processed = 0\n",
    "    total_rows_removed = 0\n",
    "    chunk_count = 0\n",
    "\n",
    "    for chunk_df in chunk_iterator:\n",
    "        if chunk_df.empty:\n",
    "            continue\n",
    "\n",
    "        chunk_count += 1\n",
    "        original_rows = len(chunk_df)\n",
    "\n",
    "        numeric_columns = [col for col in chunk_df.columns if col != 'sample_id']\n",
    "\n",
    "        if len(numeric_columns) == 0:\n",
    "            print(f\"    Warning: Chunk {chunk_count} has no numeric columns, keeping all rows\")\n",
    "            yield chunk_df\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            row_sums = chunk_df[numeric_columns].sum(axis=1)\n",
    "            filtered_chunk = chunk_df[row_sums != 0]\n",
    "        except Exception as e:\n",
    "            print(f\"    Error in chunk {chunk_count}: {e}\")\n",
    "            print(f\"    Column types: {chunk_df.dtypes}\")\n",
    "            raise\n",
    "\n",
    "        rows_removed = original_rows - len(filtered_chunk)\n",
    "        total_rows_processed += original_rows\n",
    "        total_rows_removed += rows_removed\n",
    "\n",
    "        if rows_removed > 0:\n",
    "            print(f\"    Chunk {chunk_count}: Removed {rows_removed}/{original_rows} rows with zero sum\")\n",
    "\n",
    "        yield filtered_chunk\n",
    "\n",
    "    print(f\"Filtering complete: {total_rows_removed}/{total_rows_processed} rows removed\")\n",
    "\n",
    "\n",
    "def check_metadata(healthy_generator, unhealthy_generator, merged_metadata_generator):\n",
    "    healthy_dataset_line_count = 0\n",
    "    unhealthy_dataset_line_count = 0\n",
    "    metadata_line_count = 0\n",
    "\n",
    "    # Count rows in healthy dataset generator\n",
    "    for chunk in healthy_generator:\n",
    "        healthy_dataset_line_count += len(chunk)\n",
    "\n",
    "    # Count rows in unhealthy dataset generator\n",
    "    for chunk in unhealthy_generator:\n",
    "        unhealthy_dataset_line_count += len(chunk)\n",
    "\n",
    "    # Count rows in metadata generator\n",
    "    for chunk in merged_metadata_generator:\n",
    "        metadata_line_count += len(chunk)\n",
    "\n",
    "    total = healthy_dataset_line_count + unhealthy_dataset_line_count\n",
    "\n",
    "    if (total != metadata_line_count):\n",
    "        raise Exception(f\"Healthy line count: {healthy_dataset_line_count} | Unhealthy line count: {unhealthy_dataset_line_count} | Total line count: {total} | Metadata line count: {metadata_line_count}\")\n",
    "    else:\n",
    "        print(\"All good\")\n",
    "        print(f\"Stats: Healthy line count: {healthy_dataset_line_count} | Unhealthy line count: {unhealthy_dataset_line_count} | Total line count: {total} | Metadata line count: {metadata_line_count}\")\n",
    "\n",
    "\n",
    "def prepare_metadata_generator(aligned_pair_iterator, output_metadata_path=\"data/merged_metadata.pq\"):\n",
    "    \"\"\"\n",
    "    Generator that creates metadata from aligned chunk pairs and saves to parquet.\n",
    "\n",
    "    Args:\n",
    "        aligned_chunk_pairs_generator: Generator yielding (healthy_chunk, unhealthy_chunk) tuples\n",
    "        output_metadata_path: Path to save metadata parquet file\n",
    "\n",
    "    Yields:\n",
    "        tuple: (healthy_chunk, unhealthy_chunk) - passes through the original chunks\n",
    "    \"\"\"\n",
    "\n",
    "    all_metadata = []\n",
    "\n",
    "    for healthy_chunk, unhealthy_chunk in aligned_pair_iterator:\n",
    "        chunk_metadata = []\n",
    "\n",
    "        if healthy_chunk is not None:\n",
    "            healthy_sample_ids = [str(sid) for sid in healthy_chunk.iloc[:, 0].tolist()]\n",
    "            healthy_metadata = [{'sample_id': sid, 'condition': 'healthy'} for sid in healthy_sample_ids]\n",
    "            chunk_metadata.extend(healthy_metadata)\n",
    "\n",
    "        if unhealthy_chunk is not None:\n",
    "            unhealthy_sample_ids = [str(sid) for sid in unhealthy_chunk.iloc[:, 0].tolist()]\n",
    "            unhealthy_metadata = [{'sample_id': sid, 'condition': 'unhealthy'} for sid in unhealthy_sample_ids]\n",
    "            chunk_metadata.extend(unhealthy_metadata)\n",
    "\n",
    "        all_metadata.extend(chunk_metadata)\n",
    "\n",
    "        # Yield the chunks unchanged (pass-through)\n",
    "        yield healthy_chunk, unhealthy_chunk\n",
    "        del healthy_chunk, unhealthy_chunk\n",
    "\n",
    "    # Save metadata at the end\n",
    "    if all_metadata:\n",
    "        metadata_df = pd.DataFrame(all_metadata)\n",
    "        metadata_df.to_parquet(output_metadata_path, index=False)\n",
    "        print(f\"Metadata saved to {output_metadata_path} with {len(metadata_df)} records\")\n",
    "    else:\n",
    "        print(\"No metadata to save\")\n",
    "\n",
    "    # Clean up\n",
    "    if 'all_metadata' in locals() or 'all_metadata' in globals():\n",
    "        del all_metadata\n",
    "    if 'metadata_df' in locals() or 'metadata_df' in globals():\n",
    "        del metadata_df\n",
    "    if 'aligned_pair_iterator' in locals() or 'aligned_pair_iterator' in globals():\n",
    "        del aligned_pair_iterator\n",
    "    if 'chunk_metadata' in locals() or 'chunk_metadata' in globals():\n",
    "        del chunk_metadata\n",
    "    if 'healthy_metadata' in locals() or 'healthy_metadata' in globals():\n",
    "        del healthy_metadata\n",
    "    if 'unhealthy_metadata' in locals() or 'unhealthy_metadata' in globals():\n",
    "        del unhealthy_metadata\n",
    "    gc.collect()\n",
    "\n",
    "def get_healthy_whole_blood_samples(metadata_path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Get healthy whole blood samples from the metadata.\n",
    "\n",
    "    Args:\n",
    "        metadata_path (str): Path to the metadata CSV file\n",
    "        gtex_blood_ids (list[str]): List of GTEx whole blood sample IDs\n",
    "\n",
    "    Returns:\n",
    "        list[str]: List of healthy whole blood sample IDs\n",
    "    \"\"\"\n",
    "    metadata_df = pd.read_csv(metadata_path, index_col=0)\n",
    "\n",
    "    # Get all samples with 'Whole Blood' SMTSD\n",
    "    whole_blood_samples = metadata_df[metadata_df['SMTSD'] == 'Whole Blood']\n",
    "    # Filter on RNA integrity (SMRIN) to remove low quality samples\n",
    "    healthy_samples = whole_blood_samples[\"SMRIN\"] >= 7.0\n",
    "\n",
    "    # Return SAMPIDs of healthy whole blood samples\n",
    "    healthy_whole_blood_samples = healthy_samples.index.tolist()\n",
    "    return healthy_whole_blood_samples\n",
    "\n",
    "def align_gene_columns_generator(healthy_chunk_generator, unhealthy_chunk_generator,\n",
    "                                gene_column_prefix=\"ENSG\"):\n",
    "    \"\"\"\n",
    "    Generator that aligns gene columns between healthy and unhealthy datasets.\n",
    "    Handles datasets of different sizes by yielding None for exhausted iterators.\n",
    "\n",
    "    Args:\n",
    "        healthy_chunk_generator: Generator yielding healthy dataset chunks\n",
    "        unhealthy_chunk_generator: Generator yielding unhealthy dataset chunks\n",
    "        gene_column_prefix (str): Prefix to identify gene columns (default: \"ENSG\")\n",
    "\n",
    "    Yields:\n",
    "        tuple: (aligned_healthy_chunk, aligned_unhealthy_chunk) for each chunk pair\n",
    "               None is yielded for exhausted datasets\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting gene column alignment...\")\n",
    "\n",
    "    healthy_iter = iter(healthy_chunk_generator)\n",
    "    unhealthy_iter = iter(unhealthy_chunk_generator)\n",
    "\n",
    "    try:\n",
    "        first_healthy_chunk = next(healthy_iter)\n",
    "        first_unhealthy_chunk = next(unhealthy_iter)\n",
    "    except StopIteration:\n",
    "        print(\"ERROR: One or both datasets are empty!\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nProcessing gene columns and stripping version suffixes...\")\n",
    "    def extract_gene_info(chunk, dataset_name):\n",
    "        \"\"\"Extract gene columns and create mapping from original_column to base_id\"\"\"\n",
    "        print(f\"Processing {dataset_name} dataset...\")\n",
    "\n",
    "        original_to_base = {}\n",
    "        base_gene_ids = set()\n",
    "        total_gene_columns = 0\n",
    "        duplicate_count = 0\n",
    "\n",
    "        for column in chunk.columns:\n",
    "            if isinstance(column, str) and column.startswith(gene_column_prefix):\n",
    "                total_gene_columns += 1\n",
    "                base_gene_id = column.split('.')[0]\n",
    "\n",
    "                # Handle duplicates (keep first occurrence)\n",
    "                if base_gene_id not in base_gene_ids:\n",
    "                    original_to_base[column] = base_gene_id\n",
    "                    base_gene_ids.add(base_gene_id)\n",
    "                else:\n",
    "                    duplicate_count += 1\n",
    "                    print(f\"Warning: Duplicate base gene {base_gene_id} found, skipping {column}\")\n",
    "\n",
    "        print(f\"Total gene columns found: {total_gene_columns}\")\n",
    "        print(f\"Unique base gene IDs: {len(base_gene_ids)}\")\n",
    "        if duplicate_count > 0:\n",
    "            print(f\"Duplicates removed: {duplicate_count}\")\n",
    "\n",
    "        return original_to_base, base_gene_ids\n",
    "\n",
    "    healthy_rename_mapping, healthy_base_genes = extract_gene_info(first_healthy_chunk, \"HEALTHY\")\n",
    "    unhealthy_rename_mapping, unhealthy_base_genes = extract_gene_info(first_unhealthy_chunk, \"UNHEALTHY\")\n",
    "\n",
    "    common_base_genes = healthy_base_genes & unhealthy_base_genes\n",
    "\n",
    "    if not common_base_genes:\n",
    "        print(\"ERROR: No common genes found between datasets!\")\n",
    "        return\n",
    "\n",
    "    print(f\"Common genes: {len(common_base_genes)}\")\n",
    "    print(f\"Genes exclusive to healthy dataset: {len(healthy_base_genes - common_base_genes)}\")\n",
    "    print(f\"Genes exclusive to unhealthy dataset: {len(unhealthy_base_genes - common_base_genes)}\")\n",
    "\n",
    "    renamed_first_healthy = first_healthy_chunk.rename(columns=healthy_rename_mapping)\n",
    "    renamed_first_unhealthy = first_unhealthy_chunk.rename(columns=unhealthy_rename_mapping)\n",
    "\n",
    "    healthy_non_gene_cols = [col for col in renamed_first_healthy.columns\n",
    "                           if not (isinstance(col, str) and col.startswith(gene_column_prefix))]\n",
    "    unhealthy_non_gene_cols = [col for col in renamed_first_unhealthy.columns\n",
    "                             if not (isinstance(col, str) and col.startswith(gene_column_prefix))]\n",
    "\n",
    "    # Validate that non-gene columns match\n",
    "    if set(healthy_non_gene_cols) != set(unhealthy_non_gene_cols):\n",
    "        print(\"Warning: Non-gene columns differ between datasets!\")\n",
    "        print(f\"Healthy only: {set(healthy_non_gene_cols) - set(unhealthy_non_gene_cols)}\")\n",
    "        print(f\"Unhealthy only: {set(unhealthy_non_gene_cols) - set(healthy_non_gene_cols)}\")\n",
    "\n",
    "    non_gene_cols = healthy_non_gene_cols\n",
    "    common_gene_base_ids = sorted(common_base_genes)\n",
    "    final_columns = non_gene_cols + common_gene_base_ids\n",
    "\n",
    "    print(\"\\nProcessing and yielding aligned chunks...\")\n",
    "    def process_chunk(chunk, rename_mapping):\n",
    "        \"\"\"Process a single chunk: rename and select columns\"\"\"\n",
    "        if chunk is None:\n",
    "            return None\n",
    "\n",
    "        renamed_chunk = chunk.rename(columns=rename_mapping)\n",
    "        aligned_chunk = renamed_chunk[final_columns].copy()\n",
    "\n",
    "        return aligned_chunk\n",
    "\n",
    "    aligned_first_healthy = process_chunk(first_healthy_chunk, healthy_rename_mapping)\n",
    "    aligned_first_unhealthy = process_chunk(first_unhealthy_chunk, unhealthy_rename_mapping)\n",
    "\n",
    "    yield aligned_first_healthy, aligned_first_unhealthy\n",
    "\n",
    "    chunk_count = 1\n",
    "    healthy_exhausted = False\n",
    "    unhealthy_exhausted = False\n",
    "\n",
    "    while not (healthy_exhausted and unhealthy_exhausted):\n",
    "        chunk_count += 1\n",
    "\n",
    "        healthy_chunk = None\n",
    "        if not healthy_exhausted:\n",
    "            try:\n",
    "                healthy_chunk = next(healthy_iter)\n",
    "            except StopIteration:\n",
    "                healthy_exhausted = True\n",
    "                print(f\"Healthy dataset exhausted after {chunk_count-1} chunks\")\n",
    "\n",
    "        unhealthy_chunk = None\n",
    "        if not unhealthy_exhausted:\n",
    "            try:\n",
    "                unhealthy_chunk = next(unhealthy_iter)\n",
    "            except StopIteration:\n",
    "                unhealthy_exhausted = True\n",
    "                print(f\"Unhealthy dataset exhausted after {chunk_count-1} chunks\")\n",
    "\n",
    "        if healthy_chunk is None and unhealthy_chunk is None:\n",
    "            break\n",
    "\n",
    "        aligned_healthy = process_chunk(healthy_chunk, healthy_rename_mapping)\n",
    "        aligned_unhealthy = process_chunk(unhealthy_chunk, unhealthy_rename_mapping)\n",
    "\n",
    "        yield aligned_healthy, aligned_unhealthy\n",
    "\n",
    "    print(f\"\\nAlignment complete!\")\n",
    "\n",
    "    # Clean up\n",
    "    if 'healthy_iter' in locals():\n",
    "        del healthy_iter\n",
    "    if 'unhealthy_iter' in locals():\n",
    "        del unhealthy_iter\n",
    "    if 'first_healthy_chunk' in locals():\n",
    "        del first_healthy_chunk\n",
    "    if 'first_unhealthy_chunk' in locals():\n",
    "        del first_unhealthy_chunk\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def merge_datasets_generator(aligned_pair_iterator):\n",
    "    \"\"\"\n",
    "    Generator that merges healthy and unhealthy dataset chunks into single merged chunks.\n",
    "\n",
    "    Args:\n",
    "        aligned_chunk_pairs_generator: Generator yielding (healthy_chunk, unhealthy_chunk) tuples\n",
    "        chunk_size (int, optional): Target size for merged chunks. If None, merges each pair directly.\n",
    "\n",
    "    Yields:\n",
    "        pandas.DataFrame: Merged chunks containing both healthy and unhealthy data\n",
    "    \"\"\"\n",
    "\n",
    "    for healthy_chunk, unhealthy_chunk in aligned_pair_iterator:\n",
    "        chunks_to_merge = []\n",
    "\n",
    "        if healthy_chunk is not None:\n",
    "            chunks_to_merge.append(healthy_chunk)\n",
    "\n",
    "        if unhealthy_chunk is not None:\n",
    "            chunks_to_merge.append(unhealthy_chunk)\n",
    "\n",
    "        if chunks_to_merge:\n",
    "            # Concatenate available chunks\n",
    "            merged_chunk = pd.concat(chunks_to_merge)\n",
    "            yield merged_chunk\n",
    "            del merged_chunk\n",
    "\n",
    "    # Clean up\n",
    "    if 'aligned_pair_iterator' in locals():\n",
    "        del aligned_pair_iterator\n",
    "    if 'healthy_chunk' in locals():\n",
    "        del healthy_chunk\n",
    "    if 'unhealthy_chunk' in locals():\n",
    "        del unhealthy_chunk\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def set_index_column(chunk_generator, column_name, drop=True):\n",
    "    \"\"\"\n",
    "    Generator function that sets a specified column as the index for each DataFrame chunk.\n",
    "\n",
    "    Args:\n",
    "        chunk_generator: An iterable that yields pandas DataFrame chunks\n",
    "        column_name: Name of the column to set as index\n",
    "        drop: Whether to drop the column after setting it as index (default: True)\n",
    "\n",
    "    Yields:\n",
    "        pd.DataFrame: Chunks with the specified column set as index\n",
    "    \"\"\"\n",
    "    for chunk in chunk_generator:\n",
    "        if chunk.empty:\n",
    "            yield chunk\n",
    "            continue\n",
    "\n",
    "        if column_name not in chunk.columns:\n",
    "            yield chunk\n",
    "            continue\n",
    "\n",
    "        chunk_with_index = chunk.set_index(column_name, drop=drop)\n",
    "\n",
    "        yield chunk_with_index\n",
    "\n",
    "\n",
    "def get_healthy_whole_blood_samples(metadata_path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Get healthy whole blood samples from the metadata.\n",
    "\n",
    "    Args:\n",
    "        metadata_path (str): Path to the metadata CSV file\n",
    "        gtex_blood_ids (list[str]): List of GTEx whole blood sample IDs\n",
    "\n",
    "    Returns:\n",
    "        list[str]: List of healthy whole blood sample IDs\n",
    "    \"\"\"\n",
    "    metadata_df = pd.read_csv(metadata_path, index_col=0)\n",
    "\n",
    "    # Get all samples with 'Whole Blood' SMTSD\n",
    "    whole_blood_samples = metadata_df[metadata_df['SMTSD'] == 'Whole Blood']\n",
    "    # Filter on RNA integrity (SMRIN) to remove low quality samples\n",
    "    healthy_samples = whole_blood_samples[\"SMRIN\"] >= 7.0\n",
    "\n",
    "    # Return SAMPIDs of healthy whole blood samples\n",
    "    healthy_whole_blood_samples = healthy_samples.index.tolist()\n",
    "    return healthy_whole_blood_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Preprocessing Unhealthy Dataset\n",
    "\n",
    "- Load `.csv` file as `pandas.DataFrame`\n",
    "- Set name of index as `sample_id` (Had no index name)\n",
    "- Remove duplicates and NaNs by dropping rows containing them\n",
    "- Transpose dataset\n",
    "- Set index to `'sample_id'` column\n",
    "- Remove genes that have a total sum of 0 recorded across all patients\n",
    "- Transpose dataset\n",
    "- Set index to `'sample_id'` column\n",
    "- Save preprocessed dataset as `.pq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthy_dataset_file = 'data/rna_seq_unstranded.csv'\n",
    "unhealthy_output_file = 'data/unhealthy_data_preprocessed.pq'\n",
    "chunk_size = 8000\n",
    "\n",
    "chunk_iterator = load_csv_in_chunks(unhealthy_dataset_file, header=0, skiprows=[1, 2], index_col=0)\n",
    "print(f\"Starting preprocessing for Unhealthy Dataset: {unhealthy_dataset_file}\")\n",
    "\n",
    "chunk_iterator = rename_index(chunk_iterator, 'sample_id')\n",
    "chunk_iterator = clean_duplicate_nans(chunk_iterator)\n",
    "chunk_iterator = transpose_dataframe_chunks(chunk_iterator, output_batch_size=chunk_size) # Also converts dtypes to uint32\n",
    "chunk_iterator = set_index_column(chunk_iterator, 'sample_id')\n",
    "chunk_iterator = filter_rows(chunk_iterator)\n",
    "chunk_iterator = transpose_dataframe_chunks(chunk_iterator, output_batch_size=chunk_size)\n",
    "chunk_iterator = set_index_column(chunk_iterator, 'sample_id')\n",
    "\n",
    "save_data_as_parquet(chunk_iterator, unhealthy_output_file)\n",
    "\n",
    "# Clean up\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Display basic unhealthy dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthy_iterator = load_parquet_in_chunks('data/unhealthy_data_preprocessed.pq', chunk_size=chunk_size)\n",
    "\n",
    "if unhealthy_iterator:\n",
    "    first_chunk = next(unhealthy_iterator)\n",
    "\n",
    "    # Basic info\n",
    "    print(f\"First chunk shape: {first_chunk.shape}\")\n",
    "    print(f\"Columns: {list(first_chunk.columns)}\")\n",
    "    print(f\"Data types:\\n{first_chunk.dtypes}\")\n",
    "    print(f\"Memory usage: {first_chunk.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(first_chunk.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Preprocess healthy dataset\n",
    "## Convert .gct to .pq (parquet)\n",
    "\n",
    "- Load `.gct` file as `pandas.DataFrame` using `load_csv_in_chunks`\n",
    "- Store `pandas.DatgaFrame` as `.pq` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = 'data/GTEx_Analysis_2022-06-06_v10_RNASeQCv2.4.2_gene_reads.gct'\n",
    "output_file = 'data/healthy_data.pq'\n",
    "chunk_size = 3000\n",
    "\n",
    "gct_chunk_iterator = load_csv_in_chunks(\n",
    "    file_path=dataset_file,\n",
    "    chunk_size=chunk_size,\n",
    "    sep='\\t',\n",
    "    skiprows=2,\n",
    "    header=0,\n",
    "    index_col=0,\n",
    ")\n",
    "\n",
    "if gct_chunk_iterator:\n",
    "    print(\"Saving GCT dataset as pickle...\")\n",
    "\n",
    "    save_data_as_parquet(\n",
    "        chunk_iterator=gct_chunk_iterator,\n",
    "        output_parquet_path=output_file\n",
    "    )\n",
    "else:\n",
    "    print(f\"Failed to load {dataset_file} using load_csv_in_chunks.\")\n",
    "\n",
    "# Clean up\n",
    "if 'gct_chunk_iterator' in locals() or 'gct_chunk_iterator' in globals():\n",
    "    del gct_chunk_iterator\n",
    "if 'dataset_file' in locals() or 'dataset_file' in globals():\n",
    "    del dataset_file\n",
    "if 'output_file' in locals() or 'output_file' in globals():\n",
    "    del output_file\n",
    "if 'read_chunk_size' in locals() or 'read_chunk_size' in globals():\n",
    "    del chunk_size\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "- Load dataset from `.pq` file\n",
    "- Rename index to `'gene_id'`\n",
    "- Remove genes that have a total sum of 0 recorded across all patients\n",
    "- Transpose dataset\n",
    "- Set index to `'sample_id'` column\n",
    "- Remove duplicates and NaNs by dropping rows containing them\n",
    "- Drop samples that don't use whole blood samples\n",
    "- Save preprocessed dataset as `.pq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_data_input = 'data/healthy_data.pq'\n",
    "healthy_data_output = 'data/healthy_data_preprocessed.pq'\n",
    "feature_id_col = 'Name'\n",
    "exclude_cols = ['Description']\n",
    "chunk_size = 3000\n",
    "\n",
    "# metadata_path = 'data/gtex_blood_samples_metadata.csv'\n",
    "# whole_blood_samples = get_healthy_whole_blood_samples(metadata_path)\n",
    "# print(f\"Found {len(whole_blood_samples)} healthy whole blood samples in metadata\")\n",
    "\n",
    "whole_blood_samples = ['GTEX-1117F-0005-SM-HL9SH', 'GTEX-1122O-0526-SM-5N9DM', 'GTEX-1122O-0826-SM-5GICV', 'GTEX-117YX-0526-SM-5EGJH', 'GTEX-11DYG-0011-R10b-SM-DNZZO']\n",
    "\n",
    "chunk_iterator = load_parquet_in_chunks(\n",
    "    file_path=healthy_data_input,\n",
    "    chunk_size=chunk_size\n",
    ")\n",
    "\n",
    "chunk_iterator = rename_index(chunk_iterator, 'gene_id')\n",
    "chunk_iterator = drop_dataframe_chunks(chunk_generator=chunk_iterator, drop_columns=['Description'])\n",
    "chunk_iterator = filter_rows(chunk_iterator)\n",
    "chunk_iterator = transpose_dataframe_chunks(chunk_generator=chunk_iterator, output_batch_size=chunk_size)\n",
    "chunk_iterator = set_index_column(chunk_iterator, 'sample_id', drop=True)\n",
    "chunk_iterator = clean_duplicate_nans(chunk_iterator)\n",
    "chunk_iterator = keep_dataframe_chunks(chunk_iterator, keep_rows=whole_blood_samples)\n",
    "\n",
    "save_data_as_parquet(\n",
    "    chunk_iterator=chunk_iterator,\n",
    "    output_parquet_path=healthy_data_output\n",
    ")\n",
    "\n",
    "# Clean up\n",
    "if 'chunk_iterator' in locals() or 'chunk_iterator' in globals():\n",
    "    del chunk_iterator\n",
    "if 'healthy_data_input' in locals() or 'healthy_data_input' in globals():\n",
    "    del healthy_data_input\n",
    "if 'healthy_data_output' in locals() or 'healthy_data_output' in globals():\n",
    "    del healthy_data_output\n",
    "if 'feature_id_col' in locals() or 'feature_id_col' in globals():\n",
    "    del feature_id_col\n",
    "if 'exclude_cols' in locals() or 'exclude_cols' in globals():\n",
    "    del exclude_cols\n",
    "if 'chunk_size' in locals() or 'chunk_size' in globals():\n",
    "    del chunk_size\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Display basic healthy dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthy_iterator = load_parquet_in_chunks('data/healthy_data_preprocessed.pq', chunk_size=5000)\n",
    "\n",
    "if unhealthy_iterator:\n",
    "    first_chunk = next(unhealthy_iterator)\n",
    "\n",
    "    # Basic info\n",
    "    print(f\"First chunk shape: {first_chunk.shape}\")\n",
    "    print(f\"Columns: {list(first_chunk.columns)}\")\n",
    "    print(f\"Data types:\\n{first_chunk.dtypes}\")\n",
    "    print(f\"Memory usage: {first_chunk.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(first_chunk.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Prepare and merge datasets\n",
    "\n",
    "- Load datasets from `.pq` files\n",
    "- Align datasets by removing gene versioning suffix from Gene IDs, removing duplicate genes, keeping the common (intersection) genes, sorting the common genes in same order\n",
    "- Generate custom metadata (sample_id, condition) based on the datasets\n",
    "- Merge datasets into single dataset file\n",
    "- Save merged dataset as `.pq` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_data_path = 'data/healthy_data_preprocessed.pq'\n",
    "unhealthy_data_path = 'data/unhealthy_data_preprocessed.pq'\n",
    "chunk_size = 3000\n",
    "\n",
    "\n",
    "healthy_iterator = load_parquet_in_chunks(healthy_data_path, chunk_size=chunk_size)\n",
    "unhealthy_iterator = load_parquet_in_chunks(unhealthy_data_path, chunk_size=chunk_size)\n",
    "\n",
    "aligned_pair_iterator = align_gene_columns_generator(healthy_iterator, unhealthy_iterator)\n",
    "aligned_pair_iterator = prepare_metadata_generator(aligned_pair_iterator, output_metadata_path=\"data/merged_metadata.pq\")\n",
    "merged_chunk_iterator = merge_datasets_generator(aligned_pair_iterator)\n",
    "\n",
    "save_data_as_parquet(chunk_iterator=merged_chunk_iterator, output_parquet_path='data/merged_dataset.pq')\n",
    "\n",
    "# Clean up\n",
    "if 'healthy_iterator' in locals() or 'healthy_iterator' in globals():\n",
    "    del healthy_iterator\n",
    "if 'unhealthy_iterator' in locals() or 'unhealthy_iterator' in globals():\n",
    "    del unhealthy_iterator\n",
    "if 'aligned_pair_iterator' in locals() or 'aligned_pair_iterator' in globals():\n",
    "    del aligned_pair_iterator\n",
    "if 'merged_chunk_iterator' in locals() or 'merged_chunk_iterator' in globals():\n",
    "    del merged_chunk_iterator\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Display basic merged dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_data_path = 'data/healthy_data_preprocessed.csv'\n",
    "unhealthy_data_path = 'data/unhealthy_data_preprocessed.csv'\n",
    "output_healthy_path = 'data/healthy_data_aligned.csv'\n",
    "output_unhealthy_path = 'data/unhealthy_data_aligned.csv'\n",
    "chunk_size = 2000\n",
    "\n",
    "merged_iterator = load_parquet_in_chunks('data/merged_dataset.pq', chunk_size=5000)\n",
    "\n",
    "if merged_iterator:\n",
    "    first_chunk = next(merged_iterator)\n",
    "\n",
    "    # Basic info\n",
    "    print(f\"First chunk shape: {first_chunk.shape}\")\n",
    "    print(f\"Columns: {list(first_chunk.columns)}\")\n",
    "    print(f\"Data types:\\n{first_chunk.dtypes}\")\n",
    "    print(f\"Memory usage: {first_chunk.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(first_chunk.head())\n",
    "\n",
    "# Clean up\n",
    "if 'merged_iterator' in locals() or 'merged_iterator' in globals():\n",
    "    del merged_iterator\n",
    "if 'first_chunk' in locals() or 'first_chunk' in globals():\n",
    "    del first_chunk\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Verify metadata\n",
    "\n",
    "Double check if generated metadata is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_iterator = load_parquet_in_chunks('data/healthy_data_preprocessed.pq', chunk_size=5000)\n",
    "unhealthy_iterator = load_parquet_in_chunks('data/unhealthy_data_preprocessed.pq', chunk_size=5000)\n",
    "metadata_iterator = load_parquet_in_chunks('data/merged_metadata.pq', chunk_size=5000)\n",
    "\n",
    "check_metadata(healthy_generator=healthy_iterator, unhealthy_generator=unhealthy_iterator, merged_metadata_generator=metadata_iterator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
