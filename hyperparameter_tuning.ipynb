{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna Hyperparameter Optimization for RNA-seq Binary Classification\n",
    "\n",
    "## Cell 1: Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "import pickle\n",
    "import joblib\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_validate\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Cell 2: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your VST-transformed data\n",
    "# Assume vst_data is a DataFrame with samples as rows, genes as columns\n",
    "# and labels is a Series with binary classification labels (healthy/unhealthy)\n",
    "\n",
    "# Replace with your actual data loading\n",
    "vst_data = pd.read_csv('data/final_vst_normalised_data.csv', index_col=0)  # samples x genes\n",
    "labels = pd.read_csv('data/simple_metadata_for_pydeseq2.csv', index_col=0)['condition']  # condition column\n",
    "\n",
    "# Convert string labels to binary (healthy=0, unhealthy=1)\n",
    "label_mapping = {'healthy': 0, 'unhealthy': 1}\n",
    "labels = labels.map(label_mapping)\n",
    "\n",
    "print(f\"Data shape: {vst_data.shape}\")\n",
    "print(f\"Labels distribution after encoding: {labels.value_counts()}\")\n",
    "print(f\"Features (genes): {vst_data.shape[1]}\")\n",
    "print(f\"Samples: {vst_data.shape[0]}\")\n",
    "\n",
    "# Basic data validation\n",
    "assert vst_data.shape[0] == len(labels), \"Mismatch between samples and labels\"\n",
    "assert set(labels.unique()) == {0, 1}, \"Labels should be binary (0, 1) after encoding\"\n",
    "assert not labels.isnull().any(), \"Labels contain missing values\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop condition column if it exists in vst_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'condition' in vst_data.columns:\n",
    "    print(\"WARNING: Found a 'condition' column in the VST data. This is unexpected and will be dropped before optimization.\")\n",
    "    vst_data.drop(columns=['condition'], inplace=True)\n",
    "    print(f\"Shape of VST data after dropping 'condition' column: {vst_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 3: Define Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna optimization.\n",
    "    This function will be called for each trial to evaluate hyperparameter combinations.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Feature Selection Hyperparameters\n",
    "    variance_threshold = trial.suggest_float('variance_threshold', 0.0, 1.0)\n",
    "    n_features = trial.suggest_int('n_features', 500, min(8000, vst_data.shape[1]))\n",
    "\n",
    "    # 1.5. Feature selection method\n",
    "    feature_selector = trial.suggest_categorical('feature_selector', ['f_classif', 'mutual_info_classif'])\n",
    "\n",
    "    # 2. Model Selection\n",
    "    classifier_name = trial.suggest_categorical('classifier',\n",
    "                                              ['xgboost', 'lightgbm', 'random_forest'])\n",
    "\n",
    "    # 3. Preprocessing\n",
    "    use_scaling = trial.suggest_categorical('use_scaling', [True, False])\n",
    "\n",
    "    # 4. Class imbalance handling\n",
    "    class_weight = trial.suggest_categorical('class_weight', ['balanced', None])\n",
    "\n",
    "    # 5. Model-specific hyperparameters\n",
    "    if classifier_name == 'xgboost':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 100, 500)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "        subsample = trial.suggest_float('subsample', 0.6, 1.0)\n",
    "        colsample_bytree = trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
    "        reg_alpha = trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True)\n",
    "        reg_lambda = trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True)\n",
    "\n",
    "        # Handle class imbalance for XGBoost\n",
    "        if class_weight == 'balanced':\n",
    "            scale_pos_weight = len(labels[labels == 0]) / len(labels[labels == 1])\n",
    "        else:\n",
    "            scale_pos_weight = 1\n",
    "\n",
    "        classifier = XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "\n",
    "    elif classifier_name == 'lightgbm':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 100, 500)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "        subsample = trial.suggest_float('subsample', 0.6, 1.0)\n",
    "        colsample_bytree = trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
    "        reg_alpha = trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True)\n",
    "        reg_lambda = trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True)\n",
    "        num_leaves = trial.suggest_int('num_leaves', 10, 100)\n",
    "\n",
    "        # Handle class imbalance for LightGBM\n",
    "        if class_weight == 'balanced':\n",
    "            class_weight_dict = 'balanced'\n",
    "        else:\n",
    "            class_weight_dict = None\n",
    "\n",
    "        classifier = LGBMClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            num_leaves=num_leaves,\n",
    "            class_weight=class_weight_dict,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "            verbosity=-1\n",
    "        )\n",
    "\n",
    "    elif classifier_name == 'random_forest':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 100, 500)\n",
    "        max_depth = trial.suggest_int('max_depth', 5, 20)\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "        max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "\n",
    "        classifier = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            class_weight=class_weight,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    # 6. Build pipeline\n",
    "    pipeline_steps = []\n",
    "\n",
    "    # Add variance threshold filter\n",
    "    pipeline_steps.append(('variance_filter', VarianceThreshold(threshold=variance_threshold)))\n",
    "\n",
    "    # Add feature selection\n",
    "    if feature_selector == 'f_classif':\n",
    "        selector = SelectKBest(f_classif, k=n_features)\n",
    "    else:\n",
    "        selector = SelectKBest(mutual_info_classif, k=n_features)\n",
    "    pipeline_steps.append(('feature_selection', selector))\n",
    "\n",
    "    # Add scaling if selected\n",
    "    if use_scaling:\n",
    "        pipeline_steps.append(('scaler', StandardScaler()))\n",
    "\n",
    "    # Add classifier\n",
    "    pipeline_steps.append(('classifier', classifier))\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(pipeline_steps)\n",
    "\n",
    "    # 7. Cross-validation with multiple metrics (reduced for memory)\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    try:\n",
    "        # Get comprehensive metrics\n",
    "        scoring = ['roc_auc', 'accuracy', 'precision', 'recall', 'f1']\n",
    "        cv_results = cross_validate(pipeline, vst_data, labels,\n",
    "                                   cv=cv, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "        # Store additional metrics as user attributes\n",
    "        trial.set_user_attr('accuracy', cv_results['test_accuracy'].mean())\n",
    "        trial.set_user_attr('precision', cv_results['test_precision'].mean())\n",
    "        trial.set_user_attr('recall', cv_results['test_recall'].mean())\n",
    "        trial.set_user_attr('f1', cv_results['test_f1'].mean())\n",
    "\n",
    "        # Return mean AUC score (primary metric)\n",
    "        return cv_results['test_roc_auc'].mean()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Return a poor score if the configuration fails\n",
    "        print(f\"Trial failed with error: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "print(\"Objective function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 4: Configure and Run Optuna Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Optuna study\n",
    "study_name = 'rna_seq_binary_classification'\n",
    "storage_name = f'sqlite:///{study_name}.db'\n",
    "\n",
    "# Create study\n",
    "study = optuna.create_study(\n",
    "    study_name=study_name,\n",
    "    storage=storage_name,\n",
    "    direction='maximize',  # We want to maximize AUC\n",
    "    sampler=TPESampler(seed=RANDOM_STATE),\n",
    "    pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10),\n",
    "    load_if_exists=True  # Continue from existing study if it exists\n",
    ")\n",
    "\n",
    "print(f\"Study created: {study_name}\")\n",
    "print(f\"Storage: {storage_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 5: Execute Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization\n",
    "N_TRIALS = 30  # Adjust based on your computational resources and time\n",
    "\n",
    "print(f\"Starting optimization with {N_TRIALS} trials...\")\n",
    "print(\"This may take some time depending on your dataset size and number of trials.\")\n",
    "\n",
    "# Optimize\n",
    "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "print(\"Optimization completed!\")\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "print(f\"Best trial number: {study.best_trial.number}\")\n",
    "print(f\"Best AUC score: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 6: Analyze Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best parameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Get best trial details\n",
    "best_trial = study.best_trial\n",
    "print(f\"\\nBest trial:\")\n",
    "print(f\"  Value (AUC): {best_trial.value:.4f}\")\n",
    "print(f\"  Number: {best_trial.number}\")\n",
    "\n",
    "# Show optimization history\n",
    "trials_df = study.trials_dataframe()\n",
    "print(f\"\\nTrials summary:\")\n",
    "print(f\"  Total trials: {len(trials_df)}\")\n",
    "print(f\"  Completed trials: {len(trials_df[trials_df['state'] == 'COMPLETE'])}\")\n",
    "print(f\"  Failed trials: {len(trials_df[trials_df['state'] == 'FAIL'])}\")\n",
    "\n",
    "# Basic statistics\n",
    "if len(trials_df[trials_df['state'] == 'COMPLETE']) > 0:\n",
    "    completed_values = trials_df[trials_df['state'] == 'COMPLETE']['value']\n",
    "    print(f\"  Mean AUC: {completed_values.mean():.4f}\")\n",
    "    print(f\"  Std AUC: {completed_values.std():.4f}\")\n",
    "    print(f\"  Min AUC: {completed_values.min():.4f}\")\n",
    "    print(f\"  Max AUC: {completed_values.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 7: Save Optimization Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the complete study for later analysis\n",
    "study_file = f'{study_name}_complete_study.pkl'\n",
    "with open(study_file, 'wb') as f:\n",
    "    pickle.dump(study, f)\n",
    "\n",
    "# Save best parameters separately for easy loading\n",
    "best_params_file = f'{study_name}_best_params.pkl'\n",
    "with open(best_params_file, 'wb') as f:\n",
    "    pickle.dump(best_params, f)\n",
    "\n",
    "# Save as JSON for human readability\n",
    "import json\n",
    "best_params_json = f'{study_name}_best_params.json'\n",
    "with open(best_params_json, 'w') as f:\n",
    "    json.dump(best_params, f, indent=2)\n",
    "\n",
    "# Save trials dataframe\n",
    "trials_csv = f'{study_name}_trials.csv'\n",
    "trials_df.to_csv(trials_csv, index=False)\n",
    "\n",
    "print(\"Files saved:\")\n",
    "print(f\"  Complete study: {study_file}\")\n",
    "print(f\"  Best parameters (pickle): {best_params_file}\")\n",
    "print(f\"  Best parameters (JSON): {best_params_json}\")\n",
    "print(f\"  All trials: {trials_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 8: Create Optimized Pipeline for Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the best pipeline for training in another notebook\n",
    "def create_optimized_pipeline(best_params):\n",
    "    \"\"\"\n",
    "    Create the optimized pipeline based on best parameters.\n",
    "    This function can be imported and used in the training notebook.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract parameters\n",
    "    variance_threshold = best_params['variance_threshold']\n",
    "    n_features = best_params['n_features']\n",
    "    feature_selector = best_params['feature_selector']\n",
    "    classifier_name = best_params['classifier']\n",
    "    use_scaling = best_params['use_scaling']\n",
    "    class_weight = best_params['class_weight']\n",
    "\n",
    "    # Build pipeline steps\n",
    "    pipeline_steps = []\n",
    "\n",
    "    # Add variance threshold filter\n",
    "    pipeline_steps.append(('variance_filter', VarianceThreshold(threshold=variance_threshold)))\n",
    "\n",
    "    # Add feature selection\n",
    "    if feature_selector == 'f_classif':\n",
    "        selector = SelectKBest(f_classif, k=n_features)\n",
    "    else:\n",
    "        selector = SelectKBest(mutual_info_classif, k=n_features)\n",
    "    pipeline_steps.append(('feature_selection', selector))\n",
    "\n",
    "    # Add scaling if selected\n",
    "    if use_scaling:\n",
    "        pipeline_steps.append(('scaler', StandardScaler()))\n",
    "\n",
    "    # Create classifier based on best parameters\n",
    "    if classifier_name == 'xgboost':\n",
    "        # Handle class imbalance for XGBoost\n",
    "        if class_weight == 'balanced':\n",
    "            scale_pos_weight = len(labels[labels == 0]) / len(labels[labels == 1])\n",
    "        else:\n",
    "            scale_pos_weight = 1\n",
    "\n",
    "        classifier = XGBClassifier(\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            max_depth=best_params['max_depth'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            subsample=best_params['subsample'],\n",
    "            colsample_bytree=best_params['colsample_bytree'],\n",
    "            reg_alpha=best_params['reg_alpha'],\n",
    "            reg_lambda=best_params['reg_lambda'],\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "    elif classifier_name == 'lightgbm':\n",
    "        # Handle class imbalance for LightGBM\n",
    "        if class_weight == 'balanced':\n",
    "            class_weight_dict = 'balanced'\n",
    "        else:\n",
    "            class_weight_dict = None\n",
    "\n",
    "        classifier = LGBMClassifier(\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            max_depth=best_params['max_depth'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            subsample=best_params['subsample'],\n",
    "            colsample_bytree=best_params['colsample_bytree'],\n",
    "            reg_alpha=best_params['reg_alpha'],\n",
    "            reg_lambda=best_params['reg_lambda'],\n",
    "            num_leaves=best_params['num_leaves'],\n",
    "            class_weight=class_weight_dict,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "            verbosity=-1\n",
    "        )\n",
    "    elif classifier_name == 'random_forest':\n",
    "        classifier = RandomForestClassifier(\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            max_depth=best_params['max_depth'],\n",
    "            min_samples_split=best_params['min_samples_split'],\n",
    "            min_samples_leaf=best_params['min_samples_leaf'],\n",
    "            max_features=best_params['max_features'],\n",
    "            class_weight=class_weight,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    # Add classifier to pipeline\n",
    "    pipeline_steps.append(('classifier', classifier))\n",
    "\n",
    "    # Create and return pipeline\n",
    "    return Pipeline(pipeline_steps)\n",
    "\n",
    "# Create and save the optimized pipeline\n",
    "optimized_pipeline = create_optimized_pipeline(best_params)\n",
    "\n",
    "# Save the pipeline\n",
    "pipeline_file = f'{study_name}_optimized_pipeline.pkl'\n",
    "joblib.dump(optimized_pipeline, pipeline_file)\n",
    "\n",
    "print(f\"Optimized pipeline saved: {pipeline_file}\")\n",
    "print(f\"Pipeline steps: {[step[0] for step in optimized_pipeline.steps]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 9: Validation and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick validation of the optimized pipeline\n",
    "print(\"Validating optimized pipeline...\")\n",
    "\n",
    "# Perform one final cross-validation with best parameters\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "final_scores = cross_val_score(optimized_pipeline, vst_data, labels,\n",
    "                              cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "print(f\"Final validation scores (AUC): {final_scores}\")\n",
    "print(f\"Mean AUC: {final_scores.mean():.4f} ± {final_scores.std():.4f}\")\n",
    "\n",
    "# Create summary dictionary\n",
    "optimization_summary = {\n",
    "    'study_name': study_name,\n",
    "    'n_trials': len(study.trials),\n",
    "    'best_score': study.best_value,\n",
    "    'best_params': best_params,\n",
    "    'final_validation_mean': final_scores.mean(),\n",
    "    'final_validation_std': final_scores.std(),\n",
    "    'data_shape': vst_data.shape,\n",
    "    'random_state': RANDOM_STATE\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "summary_file = f'{study_name}_summary.pkl'\n",
    "with open(summary_file, 'wb') as f:\n",
    "    pickle.dump(optimization_summary, f)\n",
    "\n",
    "print(f\"\\nOptimization Summary:\")\n",
    "print(f\"  Study: {optimization_summary['study_name']}\")\n",
    "print(f\"  Trials completed: {optimization_summary['n_trials']}\")\n",
    "print(f\"  Best AUC: {optimization_summary['best_score']:.4f}\")\n",
    "print(f\"  Final validation: {optimization_summary['final_validation_mean']:.4f} ± {optimization_summary['final_validation_std']:.4f}\")\n",
    "print(f\"  Data shape: {optimization_summary['data_shape']}\")\n",
    "\n",
    "print(f\"\\nSummary saved: {summary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 10: Files for Next Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files created for the training notebook\n",
    "created_files = [\n",
    "    f'{study_name}_best_params.pkl',\n",
    "    f'{study_name}_best_params.json',\n",
    "    f'{study_name}_optimized_pipeline.pkl',\n",
    "    f'{study_name}_summary.pkl',\n",
    "    f'{study_name}_complete_study.pkl',\n",
    "    f'{study_name}_trials.csv'\n",
    "]\n",
    "\n",
    "print(\"Files created for training notebook:\")\n",
    "for file in created_files:\n",
    "    print(f\"  ✓ {file}\")\n",
    "\n",
    "print(f\"\\nTo use in training notebook:\")\n",
    "print(f\"1. Load best parameters: pickle.load(open('{best_params_file}', 'rb'))\")\n",
    "print(f\"2. Load optimized pipeline: joblib.load('{pipeline_file}')\")\n",
    "print(f\"3. Load summary: pickle.load(open('{summary_file}', 'rb'))\")\n",
    "\n",
    "print(f\"\\nReady for training phase!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to a new cell to diagnose:\n",
    "print(\"Data diagnostics:\")\n",
    "print(f\"Healthy samples: {(labels == 0).sum()}\")\n",
    "print(f\"Unhealthy samples: {(labels == 1).sum()}\")\n",
    "print(f\"Unique sample sources in data\")\n",
    "\n",
    "# Check for perfect separation\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(vst_data)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pca_data[labels==0, 0], pca_data[labels==0, 1], alpha=0.7, label='Healthy')\n",
    "plt.scatter(pca_data[labels==1, 0], pca_data[labels==1, 1], alpha=0.7, label='Unhealthy')\n",
    "plt.legend()\n",
    "plt.title(\"PCA: Healthy vs Unhealthy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a simple train/test split to see if perfect scores persist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    vst_data, labels, test_size=0.3, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Simple Random Forest test\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
    "test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"Manual test AUC: {test_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Notes for Training Notebook\n",
    "\n",
    "When you move to the training notebook, you can load the optimized configuration with:\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Load best parameters\n",
    "best_params = pickle.load(open('rna_seq_binary_classification_best_params.pkl', 'rb'))\n",
    "\n",
    "# Load pre-configured optimized pipeline\n",
    "optimized_pipeline = joblib.load('rna_seq_binary_classification_optimized_pipeline.pkl')\n",
    "\n",
    "# Load optimization summary\n",
    "summary = pickle.load(open('rna_seq_binary_classification_summary.pkl', 'rb'))\n",
    "\n",
    "# Now you can fit the pipeline and apply SHAP analysis\n",
    "optimized_pipeline.fit(X_train, y_train)\n",
    "predictions = optimized_pipeline.predict(X_test)\n",
    "```\n",
    "\n",
    "The pipeline is ready to use and contains all the optimized preprocessing steps and model configuration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
