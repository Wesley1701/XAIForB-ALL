{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### (ATTENTION: To use the standard pandas not the GPU version, uncomment the pandas import and comment the cudf comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf as pd\n",
    "# import pandas as pd\n",
    "from cudf import DataFrame\n",
    "import os\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "path_to_data = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Preprocessing healthy: dataset | Drop operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_healthy_csv_path = os.path.join(path_to_data, \"raw_healthy_data.csv\")\n",
    "\n",
    "raw_healthy_df = pd.read_csv(raw_healthy_csv_path, index_col=0, header=0,  sep=',')\n",
    "\n",
    "raw_healthy_df.drop('Description', axis=1, inplace=True)\n",
    "\n",
    "if raw_healthy_df.dtypes.nunique() > 1:\n",
    "    raise ValueError(\"DataFrame contains multiple data types, which is not supported.\")\n",
    "\n",
    "raw_healthy_df = raw_healthy_df.T\n",
    "\n",
    "raw_healthy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Preprocessing unhealthy: dataset | Drop operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_unhealthy_csv_path = os.path.join(path_to_data, \"raw_unhealthy_data.csv\")\n",
    "\n",
    "raw_unhealthy_df = pd.read_csv(raw_unhealthy_csv_path, index_col=0, header=0, sep=',')\n",
    "\n",
    "raw_unhealthy_df.drop('gene_name', inplace=True)\n",
    "raw_unhealthy_df.drop('gene_type', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Preprocessing healthy: gencode processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_column_names = raw_healthy_df.columns.str.split('.').str[0]\n",
    "\n",
    "columns_to_keep_mask = ~temp_column_names.duplicated(keep='first')\n",
    "\n",
    "print(\"Total columns before removal:\", len(raw_healthy_df.columns))\n",
    "\n",
    "raw_healthy_df_cleaned = raw_healthy_df.loc[:, columns_to_keep_mask]\n",
    "raw_healthy_df_cleaned.columns = temp_column_names[columns_to_keep_mask]\n",
    "\n",
    "raw_healthy_df = raw_healthy_df_cleaned\n",
    "\n",
    "del raw_healthy_df_cleaned  # Free memory\n",
    "\n",
    "print(\"Total columns after removal:\", len(raw_healthy_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Preprocessing unhealthy: gencode processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_column_names = raw_unhealthy_df.columns.str.split('.').str[0]\n",
    "\n",
    "columns_to_keep_mask = ~temp_column_names.duplicated(keep='first')\n",
    "\n",
    "print(\"Total columns before removal:\", len(raw_unhealthy_df.columns))\n",
    "\n",
    "raw_unhealthy_df_cleaned = raw_unhealthy_df.loc[:, columns_to_keep_mask]\n",
    "raw_unhealthy_df_cleaned.columns = temp_column_names[columns_to_keep_mask]\n",
    "\n",
    "raw_unhealthy_df = raw_unhealthy_df_cleaned\n",
    "\n",
    "del raw_unhealthy_df_cleaned  # Free memory\n",
    "\n",
    "print(\"Total columns after removal:\", len(raw_unhealthy_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Preprocessing healthy: Convert dtypes to int32 and drop NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_healthy_df = raw_healthy_df.astype(np.int32)\n",
    "raw_healthy_df.dropna(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Preprocessing unhealthy: Convert dtypes to int32 and drop NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_unhealthy_df = raw_unhealthy_df.astype(np.int32)\n",
    "raw_unhealthy_df.dropna(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Preprocessing: selecting only common genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_genes = raw_healthy_df.columns.intersection(raw_unhealthy_df.columns)\n",
    "\n",
    "unhealthy_df_rows_length = len(raw_unhealthy_df)\n",
    "\n",
    "raw_healthy_df_filtered = raw_healthy_df[matching_genes]\n",
    "\n",
    "raw_unhealthy_df_filtered = raw_unhealthy_df[matching_genes]\n",
    "\n",
    "raw_healthy_df_filtered = raw_healthy_df_filtered.drop(raw_healthy_df_filtered.index[-(len(raw_healthy_df_filtered) - unhealthy_df_rows_length):])\n",
    "\n",
    "if len(raw_healthy_df_filtered) != unhealthy_df_rows_length:\n",
    "    raise ValueError(\"The number of rows in the healthy DataFrame does not match the unhealthy DataFrame after slicing.\")\n",
    "\n",
    "print(f\"Healthy DataFrame rows after slicing: {len(raw_healthy_df_filtered)}\")\n",
    "print(f\"Unhealthy DataFrame rows: {unhealthy_df_rows_length}\")\n",
    "\n",
    "print(\"Healthy DataFrame columns:\", len(raw_healthy_df_filtered.columns))\n",
    "print(\"Unhealthy DataFrame columns:\", len(raw_unhealthy_df_filtered.columns))\n",
    "\n",
    "raw_healthy_df = raw_healthy_df_filtered\n",
    "\n",
    "raw_unhealthy_df = raw_unhealthy_df_filtered\n",
    "\n",
    "del raw_healthy_df_filtered\n",
    "del raw_unhealthy_df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Preprocessing: add condition column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_unhealthy_df[\"condition\"] = 0\n",
    "raw_healthy_df[\"condition\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Preprocessing: change index name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_unhealthy_df_filtered.index.name = \"sample_id\"\n",
    "# raw_healthy_df_filtered.index.name = \"sample_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Debug checks (can be commented out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_healthy_df_filtered.info()\n",
    "# raw_healthy_df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_healthy_df_filtered.info()\n",
    "# raw_unhealthy_df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Final checks before merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "are_column_names_same_regardless_order = set(raw_healthy_df.columns) == set(raw_unhealthy_df.columns)\n",
    "\n",
    "if not are_column_names_same_regardless_order:\n",
    "    raise ValueError(\"Column names in healthy and unhealthy DataFrames do not match.\")\n",
    "\n",
    "if raw_healthy_df.duplicated().any():\n",
    "    raise ValueError(\"Healthy DataFrame contains duplicate rows.\")\n",
    "\n",
    "if raw_unhealthy_df.duplicated().any():\n",
    "    raise ValueError(\"Unhealthy DataFrame contains duplicate rows.\")\n",
    "\n",
    "if raw_healthy_df.columns.duplicated().any():\n",
    "    raise ValueError(\"Healthy DataFrame contains duplicate columns.\")\n",
    "\n",
    "if raw_unhealthy_df.columns.duplicated().any():\n",
    "    raise ValueError(\"Unhealthy DataFrame contains duplicate columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Do merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df: DataFrame = pd.concat([raw_healthy_df, raw_unhealthy_df], axis=0)\n",
    "\n",
    "\n",
    "merged_df.to_parquet(f\"{path_to_data}merged_data.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "\n",
    "print(f\"Data processing completed in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "# Statistics and Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf as pd\n",
    "import cuml as sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### PCA Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(f\"{path_to_data}merged_data.pq\")\n",
    "\n",
    "x = df.drop(columns=[\"condition\"])\n",
    "\n",
    "y = df[\"condition\"]\n",
    "\n",
    "gene_columns = x.columns\n",
    "mean_healthy = x[y == 1][gene_columns].mean()\n",
    "mean_unhealthy = x[y == 0][gene_columns].mean()\n",
    "mean_diff = (mean_healthy - mean_unhealthy).abs()\n",
    "\n",
    "print(\"\\nHead of Mean Differences (for top 5 genes):\")\n",
    "print(mean_diff.head())\n",
    "\n",
    "k_genes = 10000\n",
    "\n",
    "top_k_genes = mean_diff.nlargest(k_genes).index.to_pandas()\n",
    "\n",
    "x_selected = x[top_k_genes]\n",
    "\n",
    "print(f\"\\nOriginal number of genes: {x.shape[1]}\")\n",
    "\n",
    "print(f\"Number of genes after aggressive selection (top {k_genes} by mean difference): {x_selected.shape[1]}\")\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "x_scaled = scaler.fit_transform(x_selected)\n",
    "\n",
    "print(\"Shape of x_scaled:\", x_scaled.shape)\n",
    "\n",
    "pca = sklearn.decomposition.PCA(n_components=2)\n",
    "pca_df = pca.fit_transform(x_scaled)\n",
    "\n",
    "print(\"Shape of principal components:\", pca_df.shape)\n",
    "\n",
    "pca_df.columns = [\"PC1\", \"PC2\"]\n",
    "pca_df.index = x_selected.index\n",
    "pca_df[\"condition\"] = y\n",
    "pca_df = pca_df.to_pandas()\n",
    "\n",
    "print(f\"Shape of PCA DataFrame: {pca_df.shape}\")\n",
    "print(\"\\nExplained Variance Ratio:\")\n",
    "print(f\"PC1: {pca.explained_variance_ratio_[0]:.4f}\")\n",
    "print(f\"PC2: {pca.explained_variance_ratio_[1]:.4f}\")\n",
    "print(f\"Total Explained Variance (PC1 + PC2): {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "print(\"Generating PCA plot...\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(data=pca_df, x=\"PC1\", y=\"PC2\", hue=\"condition\", palette='viridis', alpha=0.7, s=50)\n",
    "\n",
    "plt.title('PCA of Gene Expression Data (Healthy vs. Unhealthy Samples)')\n",
    "plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]*100:.2f}% Variance Explained)')\n",
    "plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]*100:.2f}% Variance Explained)')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(title='Condition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Investigation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf as pd\n",
    "import cuml as sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading merged data from Parquet file...\")\n",
    "\n",
    "df = pd.read_parquet(f\"{path_to_data}merged_data.pq\")\n",
    "\n",
    "x = df.drop(columns=[\"condition\"])\n",
    "\n",
    "gene_columns = x.columns\n",
    "\n",
    "y = df[\"condition\"]\n",
    "\n",
    "print(\"Data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Variance thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Applying variance thresholding...\")\n",
    "\n",
    "print(f\"Original number of genes: {len(gene_columns)}\")\n",
    "\n",
    "x_np = x.to_numpy()\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.1)\n",
    "selector.fit(x_np)\n",
    "\n",
    "selector_gene_mask = selector.get_support()\n",
    "\n",
    "gene_columns_temp = gene_columns[selector_gene_mask]\n",
    "x_filtered_variance = x[gene_columns_temp]\n",
    "\n",
    "print(f\"Number of genes after variance thresholding: {len(gene_columns_temp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### Expression filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Low expression filtering...\")\n",
    "\n",
    "print(f\"Number of genes before low expression filtering: {len(x_filtered_variance.columns)}\")\n",
    "\n",
    "gene_means = x_filtered_variance.mean()\n",
    "\n",
    "low_expression_threshold = 0.1\n",
    "\n",
    "gene_columns = gene_means[gene_means > low_expression_threshold].index.to_pandas()\n",
    "\n",
    "x_filtered_low_expression = x_filtered_variance[gene_columns]\n",
    "\n",
    "print(f\"Number of genes after low expression filtering: {len(gene_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Splitting data into training and testing sets...\")\n",
    "\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    x_filtered_low_expression, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "### Agressive feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Aggressive gene selection based on mean differences...\")\n",
    "\n",
    "print(\"Number of genes before selection:\", x_train.shape[1])\n",
    "\n",
    "mean_healthy = x_train[y_train == 1][gene_columns].mean()\n",
    "\n",
    "mean_unhealthy = x_train[y_train == 0][gene_columns].mean()\n",
    "\n",
    "mean_diff = (mean_healthy - mean_unhealthy).abs()\n",
    "\n",
    "k_genes = 2000\n",
    "\n",
    "print(f\"Selecting top {k_genes} genes based on mean differences...\")\n",
    "\n",
    "top_k_genes = mean_diff.nlargest(k_genes).index.to_pandas()\n",
    "\n",
    "x_train_selected = x_train[top_k_genes]\n",
    "x_test_selected = x_test[top_k_genes]\n",
    "\n",
    "print(f\"Number of genes after aggressive selection: {x_train_selected.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "### Scaling gene expression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scaling selected gene expression data...\")\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_selected)\n",
    "x_test_scaled = scaler.transform(x_test_selected)\n",
    "\n",
    "print(\"Shape of x_train_scaled:\", x_train_scaled.shape)\n",
    "print(\"Shape of x_test_scaled:\", x_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "## Logistical regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "### Logistic regression (to prevent perfect seperators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training logistical regression model...\")\n",
    "\n",
    "logreg_model = sklearn.linear_model.LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=0.0001, \n",
    "    solver='qn',\n",
    "    max_iter=1000, \n",
    ")\n",
    "\n",
    "logreg_model.fit(x_train_scaled, y_train)\n",
    "\n",
    "print(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### Logistic regression prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logreg = logreg_model.predict(x_test_scaled)\n",
    "\n",
    "y_test_np = y_test.to_numpy()\n",
    "y_pred_logreg_np = y_pred_logreg.to_numpy()\n",
    "\n",
    "accuracy = accuracy_score(y_test_np, y_pred_logreg_np)\n",
    "precision = precision_score(y_test_np, y_pred_logreg_np, average='binary')\n",
    "recall = recall_score(y_test_np, y_pred_logreg_np, average='binary')\n",
    "f1 = f1_score(y_test_np, y_pred_logreg_np, average='binary')\n",
    "\n",
    "print(f\"Logistic Regression Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
