{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Classify B-ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "- After every kernel restart rerun \"Core\"\n",
    "- It's best to restart after you run a training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Core (Always run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "import shap\n",
    "import optuna\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, auc, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "path_to_data = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b_all = cudf.read_parquet(f\"{path_to_data}B_ALL.pq\") # Sample names is column\n",
    "df_all = cudf.read_parquet(f\"{path_to_data}ALL.pq\") # Sample names is column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_all_length = len(df_b_all.columns.drop(['gene_name', 'gene_type']))  # Exclude non-numeric columns\n",
    "all_length = len(df_all.columns.drop(['gene_name', 'gene_type']))  # Exclude non-numeric columns\n",
    "\n",
    "total_length = b_all_length + all_length\n",
    "\n",
    "df_b_all_filtered = df_b_all[df_b_all['gene_type'] == 'protein_coding']  # Filter for protein-coding genes\n",
    "df_all_filtered = df_all[df_all['gene_type'] == 'protein_coding']  # Filter for protein-coding genes\n",
    "\n",
    "# df_b_all_filtered = df_b_all  # Filter for protein-coding genes\n",
    "# df_all_filtered = df_all  # Filter for protein-coding genes\n",
    "\n",
    "df_b_all_filtered = df_b_all_filtered.drop(['gene_name', 'gene_type'], axis=1)  # Drop non-numeric columns\n",
    "df_all_filtered = df_all_filtered.drop(['gene_name', 'gene_type'], axis=1)  # Drop non-numeric columns\n",
    "\n",
    "df_b_all_filtered = df_b_all_filtered.fillna(0).select_dtypes(include='number').T\n",
    "df_all_filtered = df_all_filtered.fillna(0).select_dtypes(include='number').T\n",
    "\n",
    "print(\"Filtered B-ALL length:\", len(df_b_all_filtered))\n",
    "print(\"Filtered B-ALL Healthy length:\", len(df_all_filtered))\n",
    "\n",
    "combined_df = cudf.concat([df_b_all_filtered, df_all_filtered], axis=0)\n",
    "\n",
    "combined_df['condition'] = [1] * len(df_all_filtered) + [0] * len(df_b_all_filtered)\n",
    "\n",
    "if (len(df_b_all_filtered) + len(df_all_filtered)) != combined_df.shape[0]:\n",
    "    print(f\"Expected number of rows: {len(df_b_all_filtered) + len(df_all_filtered)}, Actual number of rows: {combined_df.shape[0]}\")\n",
    "    raise ValueError(\"The number of rows in the combined DataFrame does not match the sum of B-ALL and B-ALL Healthy lengths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.to_pandas()  # Convert to pandas DataFrame for further processing\n",
    "\n",
    "print(f\"Amount of samples in the merged DataFrame: {combined_df.shape[0]}\")\n",
    "print(f\"Amount of features in the merged DataFrame: {combined_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = combined_df['condition']  # Use the 'condition' column as the target variable\n",
    "\n",
    "combined_df.drop(columns=['condition'], inplace=True)  # Drop the 'condition' column for normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## LR (Single Test Split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Label shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = y.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Log2 Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_normalized = np.log2(combined_df + 1)  # Log2 transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_normalized.reset_index(drop=True, inplace=True)\n",
    "y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    merged_df_normalized, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "print(f\"Training set shape: {x_train.shape}, Test set shape: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Variance Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selector_vt = VarianceThreshold(threshold=0.0)\n",
    "\n",
    "# x_train = selector_vt.fit_transform(x_train)\n",
    "# x_test = selector_vt.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42, sampling_strategy='auto', k_neighbors=5)\n",
    "\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"After SMOTE, training set shape: {x_train.shape}, Test set shape: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_results = {\"xgboost\": [], \"random_forest\": [], \"logistic_regression\": []}\n",
    "\n",
    "# Callback to record each trial's model type and its score.\n",
    "def record_trial_callback(study, trial):\n",
    "    classifier = trial.params.get(\"classifier\")\n",
    "    trial_results[classifier].append((trial.number, trial.value))\n",
    "\n",
    "def optimize_classifier(x, y, n_trials=20):\n",
    "    def objective(trial):\n",
    "        k = trial.suggest_int(\"k\", 5, min(50, x.shape[1] // 2))  # k is capped at half of features.\n",
    "        classifier_choice = trial.suggest_categorical(\"classifier\", [\"xgboost\", \"random_forest\", \"logistic_regression\"])\n",
    "    \n",
    "        if classifier_choice == \"xgboost\":\n",
    "            params = {\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "                # Enable GPU support:\n",
    "                \"tree_method\": \"hist\",\n",
    "                \"device\": \"cuda\",\n",
    "                \"predictor\": \"gpu_predictor\",\n",
    "                \"objective\": \"binary:logistic\",\n",
    "                \"eval_metric\": \"auc\",\n",
    "            }\n",
    "            model = xgb.XGBClassifier(**params, use_label_encoder=False, verbosity=0)\n",
    "        elif classifier_choice == \"random_forest\":\n",
    "            params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators_rf\", 50, 300),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth_rf\", 3, 20),\n",
    "                \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "            }\n",
    "            model = RandomForestClassifier(**params, random_state=42, class_weight=\"balanced\")\n",
    "        elif classifier_choice == \"logistic_regression\":\n",
    "            c_value = trial.suggest_float(\"C\", 1e-4, 1e2, log=True)\n",
    "            model = LogisticRegression(C=c_value, solver=\"liblinear\",\n",
    "                                       random_state=42, class_weight=\"balanced\", max_iter=1000)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported classifier selected.\")\n",
    "        \n",
    "        pipeline = Pipeline([\n",
    "            (\"select_kbest\", SelectKBest(score_func=f_classif, k=k)),\n",
    "            (\"classifier\", model)\n",
    "        ])\n",
    "    \n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        score = cross_val_score(pipeline, x, y, scoring=\"roc_auc\", cv=cv).mean()\n",
    "    \n",
    "        if np.isnan(score):\n",
    "            print(\"NaN score encountered, returning a low score.\")\n",
    "            return 0.0\n",
    "        return score\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials, callbacks=[record_trial_callback])\n",
    "    \n",
    "    print(\"Best parameters:\", study.best_params)\n",
    "    print(\"Best ROC-AUC:\", study.best_value)\n",
    "    return study\n",
    "\n",
    "# Run the optimization.\n",
    "study_result = optimize_classifier(x_train, y_train, n_trials=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Apply Optuna result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best ROC-AUC value per classifier from the recorded trials.\n",
    "best_scores = {}\n",
    "for clf in trial_results:\n",
    "    if trial_results[clf]:\n",
    "        best_trial = max(trial_results[clf], key=lambda t: t[1])\n",
    "        best_scores[clf] = best_trial[1]\n",
    "    else:\n",
    "        best_scores[clf] = 0\n",
    "\n",
    "# Plot the best ROC-AUC for each model.\n",
    "models = list(best_scores.keys())\n",
    "scores = [best_scores[m] for m in models]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(models, scores, color=['blue', 'green', 'orange'])\n",
    "plt.ylabel(\"Best ROC-AUC\")\n",
    "plt.xlabel(\"Classifier\")\n",
    "plt.title(\"Best ROC-AUC per Classifier from Optuna Trials\")\n",
    "plt.ylim(0, 1)\n",
    "for bar, score in zip(bars, scores):\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Create the \"plots\" directory if it doesn't exist.\n",
    "if not os.path.exists(\"plots\"):\n",
    "    os.makedirs(\"plots\")\n",
    "\n",
    "plot_path = os.path.join(\"plots\", \"best_classifier_comparison.png\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plot saved to {plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of x_train after feature selection: {x_train.shape}\"\n",
    "      f\", Shape of y_test: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if x_train.shape[0] != y_train.shape[0] or x_test.shape[0] != y_test.shape[0]:\n",
    "    raise ValueError(\"Mismatch: number of samples in X_train/X_test and labels in y_train/y_test\")\n",
    "\n",
    "if sum(y_train == 1) == 0 or sum(y_train == 0) == 0:\n",
    "    raise ValueError(\"Training set must contain both classes (B-ALL and non-B-ALL).\")\n",
    "\n",
    "if isinstance(y_train, pd.DataFrame) or isinstance(y_test, pd.DataFrame):\n",
    "    raise ValueError(\"y_train and y_test must be Series, not DataFrames.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Select K Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study_result.best_params\n",
    "\n",
    "select_k_best = SelectKBest(score_func=f_classif, k=best_params[\"k\"])\n",
    "x_train = select_k_best.fit_transform(x_train, y_train)\n",
    "x_test = select_k_best.transform(x_test)\n",
    "\n",
    "print(f\"Shape of x_train after SelectKBest: {x_train.shape}, Shape of y_test: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(\n",
    "    n_estimators=best_params[\"n_estimators_rf\"], \n",
    "    max_depth=best_params[\"max_depth_rf\"], \n",
    "    criterion=best_params[\"criterion\"],\n",
    "    random_state=42, \n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "y_proba = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"ROC-AUC on test set: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=1000,\n",
    "    tree_method='hist',\n",
    "    device='cuda',\n",
    "    predictor='gpu_predictor',\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='auc',\n",
    "    use_label_encoder=False,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_pred_xgb = model.predict(x_test)\n",
    "\n",
    "y_proba_xgb = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "roc_auc_xgb = roc_auc_score(y_test, y_proba_xgb)\n",
    "print(f\"ROC-AUC on test set (XGBoost): {roc_auc_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Statistics and Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "Precision recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, marker='.', label=f'ROC AUC = {roc_auc:.3f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='red', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer   = shap.Explainer(model, x_train)      # TreeExplainer under the hood\n",
    "shap_values = explainer.shap_values(x_test)       # list or 3-D array\n",
    "\n",
    "# ---- ordinary SHAP summary for the positive class ----\n",
    "shap.summary_plot(\n",
    "    shap_values[1],      # <-- use only class 1 (shape: n_samples × n_features)\n",
    "    x_test,\n",
    "    plot_type=\"violin\",\n",
    "    max_display=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, x_test, plot_type=\"bar\", max_display=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "# B_ALL Subtype Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "## Core (Always Run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cuml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "import shap\n",
    "import umap\n",
    "\n",
    "from sklearn.cluster import KMeans, HDBSCAN\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, silhouette_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, make_scorer\n",
    "from sklearn.utils import shuffle, resample\n",
    "from xgboost import XGBClassifier, DMatrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import plotnine as pn\n",
    "import patchworklib as pw\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.stats import ttest_ind, ks_2samp, mannwhitneyu\n",
    "import mygene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "path_to_data = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b_all = cudf.read_parquet(f\"{path_to_data}B_ALL.pq\")  # Sample names is column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b_all_filtered = df_b_all[df_b_all['gene_type'] == 'protein_coding']  # Filter for protein-coding genes\n",
    "\n",
    "df_b_all_dropped = df_b_all_filtered.drop(['gene_name', 'gene_type'], axis=1)  # Drop non-numeric columns\n",
    "\n",
    "df_b_all_transposed = df_b_all_dropped.fillna(0).select_dtypes(include='number').T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "GPU to CPU Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b_all_transposed = df_b_all_transposed.to_pandas()  # Convert to pandas DataFrame for further processing\n",
    "\n",
    "print(f\"Shape of df_b_all_transposed: {df_b_all_transposed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b_all_transposed = np.log2(df_b_all_transposed + 1)  # Log2 transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled = StandardScaler().fit_transform(df_b_all_transposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50, random_state=42)\n",
    "x_pca = pca.fit_transform(x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "KMean operations\n",
    "\n",
    "We start by doing KMeans operation N amount of times to detect the optimal amount of clusters (done later in the plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_list = []\n",
    "optimal_labels_kmeans = None\n",
    "\n",
    "for i in range(1, 6):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "\n",
    "    labels_kmeans = kmeans.fit_predict(x_pca)\n",
    "\n",
    "    if i == 3:\n",
    "        optimal_labels_kmeans = labels_kmeans\n",
    "\n",
    "    df_clusters = pd.DataFrame(x_pca, columns=[f'PC{i+1}' for i in range(x_pca.shape[1])])\n",
    "    df_clusters['cluster'] = labels_kmeans.astype(str)\n",
    "\n",
    "    clusters_list.append(df_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "KMeans plots\n",
    "\n",
    "Silhouette score between 0.51 - 0.70 is prefered\n",
    "\n",
    "(SOURCE: https://www.sciencedirect.com/science/article/pii/0377042787901257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = []\n",
    "\n",
    "silhouette_scores = []\n",
    "\n",
    "for i, df in enumerate(clusters_list):\n",
    "    # Compute silhouette score\n",
    "    if len(df['cluster'].unique()) > 1:  # Silhouette requires at least 2 clusters\n",
    "        score = silhouette_score(df[['PC1', 'PC2']], df['cluster'])\n",
    "    else:\n",
    "        score = float('nan')  # or 0.0 or \"N/A\"\n",
    "\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "for i, df in enumerate(clusters_list):\n",
    "    hull_dfs = []\n",
    "\n",
    "    for cluster in df['cluster'].unique():\n",
    "        points = df[df['cluster'] == cluster][['PC1', 'PC2']].values\n",
    "\n",
    "        # Must be >=3 points\n",
    "        if points.shape[0] >= 3:\n",
    "            try:\n",
    "                hull = ConvexHull(points)\n",
    "                hull_pts = points[hull.vertices]\n",
    "                hull_df = pd.DataFrame(hull_pts, columns=['PC1', 'PC2'])\n",
    "                hull_df['cluster'] = cluster\n",
    "                hull_dfs.append(hull_df)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    centroids = df.groupby('cluster')[['PC1', 'PC2']].mean().reset_index()\n",
    "    centroids['label'] = centroids['cluster'].astype(str)\n",
    "\n",
    "    df_hulls = pd.concat(hull_dfs, ignore_index=True)\n",
    "\n",
    "    plot = (\n",
    "        pn.ggplot(df, pn.aes('PC1', 'PC2'))\n",
    "        + pn.geom_point(pn.aes(color='cluster', shape='cluster'), size=1.5, alpha=0.7)\n",
    "        + pn.geom_polygon(df_hulls, pn.aes('PC1', 'PC2', fill='cluster', group='cluster'), alpha=0.15, show_legend=False, color='black', linetype='dashed')\n",
    "        + pn.geom_text(data=centroids, mapping=pn.aes('PC1', 'PC2', label='label'), size=8, color='black')\n",
    "        + pn.scale_fill_brewer(type='qual', palette='Set2')\n",
    "        + pn.scale_color_brewer(type='qual', palette='Set2')\n",
    "        + pn.theme_bw()\n",
    "        + pn.ggtitle(f\"Clusters = {i + 1}, Silhouette Score = {silhouette_scores[i]:.3f}\")\n",
    "    )\n",
    "\n",
    "    plots.append(pw.load_ggplot(plot, figsize=(4, 4)))\n",
    "\n",
    "g04 = ((plots[4] | plots[3]) / (plots[2] | plots[1] | plots[0]))\n",
    "g04.savefig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "Differential Expression Analysis - Setup\n",
    "\n",
    "Also adjust P-values (Benjamini-Hochberg FDR correction)\n",
    "\n",
    "SOURCE: https://physiology.med.cornell.edu/people/banfelder/qbio/resources_2008/1.5_GenespringMTC.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.float_format', lambda x: f'{x:.3e}')\n",
    "\n",
    "expression_df = df_b_all_transposed.copy()\n",
    "expression_df['cluster'] = optimal_labels_kmeans\n",
    "\n",
    "results = []\n",
    "\n",
    "for cluster_id in expression_df['cluster'].unique():\n",
    "    in_cluster = expression_df[expression_df['cluster'] == cluster_id].drop(columns=['cluster'])\n",
    "    out_cluster = expression_df[expression_df['cluster'] != cluster_id].drop(columns=['cluster'])\n",
    "\n",
    "    valid_genes = (\n",
    "        (in_cluster.var(skipna=True) > 0) |\n",
    "        (out_cluster.var(skipna=True) > 0)\n",
    "    )\n",
    "\n",
    "    in_cluster = in_cluster.loc[:, valid_genes]\n",
    "    out_cluster = out_cluster.loc[:, valid_genes]\n",
    "\n",
    "    t_stats, p_vals = ttest_ind(in_cluster, out_cluster, axis=0, equal_var=False, nan_policy='omit')\n",
    "\n",
    "    mean_diff = in_cluster.mean() - out_cluster.mean()\n",
    "\n",
    "    _, adj_pvals, _, _ = multipletests(p_vals, method='fdr_bh')\n",
    "\n",
    "    adj_pvals = np.clip(adj_pvals, 1e-300, 1.0)\n",
    "\n",
    "    res = pd.DataFrame({\n",
    "        'gene': in_cluster.columns,\n",
    "        'cluster': cluster_id,\n",
    "        'mean_diff': mean_diff.values.astype(np.float64),\n",
    "        'p_value': p_vals.astype(np.float64),\n",
    "        'adj_p_value': adj_pvals.astype(np.float64)\n",
    "    }).sort_values(by='adj_p_value')\n",
    "\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "Differential Expression Analysis - Plots\n",
    "\n",
    "These volcano plots show the mean difference and significance of a gene, we don't know what gene that is yet (will be done on the next step).\n",
    "\n",
    "1. Far left or far right (large mean difference) -> Indicates strong expression change in that cluster\n",
    "2. High on the y-axis (high −log₁₀ p-value) -> Indicates statistical significance\n",
    "\n",
    "Cluster 0 and 2 are considered promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in results:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.scatterplot(x=df['mean_diff'], y=-np.log10(df['adj_p_value']), alpha=0.6)\n",
    "    plt.axhline(-np.log10(0.05), color='red', linestyle='--')\n",
    "    plt.title(f'Cluster {df[\"cluster\"].iloc[0]} Volcano Plot')\n",
    "    plt.xlabel('Mean Difference (Effect Size)')\n",
    "    plt.ylabel('Significance (-log10 adjusted p-value)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "Top genes extraction and visualizing via box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all clusters into a single dataframe\n",
    "dea_results = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Top N genes per cluster\n",
    "top_genes_per_cluster = (\n",
    "    dea_results\n",
    "    .sort_values('adj_p_value')  # ensure it's sorted\n",
    "    .groupby('cluster')\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "# If needed, flatten into a list of unique top genes\n",
    "top_gene_list = top_genes_per_cluster['gene'].unique().tolist()\n",
    "\n",
    "# # Subset to top genes only\n",
    "expression_subset = expression_df[top_gene_list].copy()\n",
    "\n",
    "# Add cluster labels for sorting\n",
    "expression_subset['cluster'] = optimal_labels_kmeans\n",
    "\n",
    "# Sort by cluster for clean group blocks\n",
    "expression_sorted = expression_subset.sort_values('cluster')\n",
    "\n",
    "# Remove the cluster column for the heatmap\n",
    "heatmap_data = expression_sorted.drop('cluster', axis=1)\n",
    "heatmap_data_normalized = heatmap_data.apply(lambda x: (x - x.mean()) / x.std(), axis=0)\n",
    "\n",
    "# APPROACH 1: Aggregate by cluster (most informative for large datasets)\n",
    "# Calculate mean expression per cluster for each gene\n",
    "cluster_means = expression_sorted.groupby('cluster')[top_gene_list].mean()\n",
    "\n",
    "# Z-score normalize the cluster means\n",
    "cluster_means_normalized = cluster_means.apply(lambda x: (x - x.mean()) / x.std(ddof=0), axis=0)\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "sns.heatmap(cluster_means_normalized.T,  # Genes as rows, clusters as columns\n",
    "            cmap='RdBu_r',\n",
    "            center=0,\n",
    "            annot=False,\n",
    "            cbar_kws={'label': 'Z-score (cluster mean)'},\n",
    "            xticklabels=[f'Cluster {i}' for i in cluster_means.index],\n",
    "            yticklabels=True)\n",
    "plt.title('Mean Gene Expression by Cluster')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Genes')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_genes_per_cluster = (\n",
    "    dea_results\n",
    "    .sort_values('adj_p_value')  # ensure it's sorted\n",
    "    .groupby('cluster')\n",
    "    .head(40)\n",
    ")\n",
    "\n",
    "# If needed, flatten into a list of unique top genes\n",
    "top_gene_list = top_genes_per_cluster['gene'].unique().tolist()\n",
    "\n",
    "mg = mygene.MyGeneInfo()\n",
    "\n",
    "# Remove version numbers from Ensembl IDs if present (e.g., ENSG00000276672.1 → ENSG00000276672)\n",
    "top_genes = [gene.split('.')[0] for gene in top_gene_list]\n",
    "\n",
    "# Query annotations\n",
    "results = mg.querymany(top_genes, scopes='ensembl.gene', fields=['symbol', 'name', 'summary', 'entrezgene'], species='human')\n",
    "\n",
    "annotation_df = pd.DataFrame(results)\n",
    "annotation_df = annotation_df[['query', 'symbol', 'name', 'summary']]\n",
    "annotation_df.dropna(inplace=True)\n",
    "\n",
    "print(annotation_df[\"symbol\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 30 unique genes across all clusters\n",
    "\n",
    "top_genes_per_cluster = (\n",
    "    dea_results\n",
    "    .sort_values('adj_p_value')  # ensure it's sorted\n",
    "    .groupby('cluster')\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "# If needed, flatten into a list of unique top genes\n",
    "top_gene_list = top_genes_per_cluster['gene'].unique().tolist()\n",
    "\n",
    "X = expression_df[top_gene_list]\n",
    "y = optimal_labels_kmeans\n",
    "\n",
    "# np.random.shuffle(y)  # Shuffle labels for demonstration\n",
    "\n",
    "print(f\"Using {len(top_gene_list)} unique genes as features\")\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_bin = label_binarize(y_test, classes=sorted(set(y)))\n",
    "y_proba = clf.predict_proba(X_test)\n",
    "\n",
    "auc_score = roc_auc_score(y_test_bin, y_proba, average=\"macro\", multi_class=\"ovr\")\n",
    "print(f\"Multiclass AUC (OvR, macro-average): {auc_score:.3f}\")\n",
    "\n",
    "n_classes = y_test_bin.shape[1]\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=f'Cluster {i} (AUC = {roc_auc[i]:.2f})')\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Multiclass ROC Curve\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "Confustion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create display\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Raw counts\n",
    "disp1 = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                              display_labels=[f'Cluster {i}' for i in sorted(set(y))])\n",
    "disp1.plot(ax=ax1, cmap='Blues', values_format='d')\n",
    "ax1.set_title('Confusion Matrix (Raw Counts)')\n",
    "\n",
    "# Normalized (percentages)\n",
    "cm_normalized = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm_normalized,\n",
    "                              display_labels=[f'Cluster {i}' for i in sorted(set(y))])\n",
    "disp2.plot(ax=ax2, cmap='Blues', values_format='.2f')\n",
    "ax2.set_title('Confusion Matrix (Normalized)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "# Breast Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "## Core (Always Run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cuml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "import shap\n",
    "import umap\n",
    "\n",
    "from sklearn.cluster import KMeans, HDBSCAN\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, silhouette_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, make_scorer\n",
    "from sklearn.utils import shuffle, resample\n",
    "from xgboost import XGBClassifier, DMatrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import plotnine as pn\n",
    "import patchworklib as pw\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.stats import ttest_ind, ks_2samp, mannwhitneyu\n",
    "import mygene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "path_to_data = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breast_cancer = cudf.read_parquet(f\"{path_to_data}breast_cancer.pq\")  # Sample names is column\n",
    "df_breast_cancer_healthy = cudf.read_parquet(f\"{path_to_data}breast_cancer_healthy.pq\")  # Sample names is column"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
