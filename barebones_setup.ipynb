{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Classify B-ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "- After every kernel restart rerun \"Core\"\n",
    "- It's best to restart after you run a training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Core (Always run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cuml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "import shap\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.utils import shuffle, resample\n",
    "from xgboost import XGBClassifier, DMatrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.stats import ttest_ind, ks_2samp, mannwhitneyu\n",
    "import plotnine as pn\n",
    "import patchworklib as pw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "path_to_data = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b_all = cudf.read_parquet(f\"{path_to_data}B_ALL.pq\") # Sample names is column\n",
    "df_b_all_healthy = cudf.read_parquet(f\"{path_to_data}B_ALL_HEALTHY.pq\") # Sample names is column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_all_length = len(df_b_all.columns.drop(['gene_name', 'gene_type']))  # Exclude non-numeric columns\n",
    "b_all_healthy_length = len(df_b_all_healthy.columns.drop(['gene_name', 'gene_type']))  # Exclude non-numeric columns\n",
    "\n",
    "total_length = b_all_length + b_all_healthy_length\n",
    "\n",
    "print(\"B-ALL length:\", len(df_b_all))\n",
    "print(\"B-ALL Healthy length:\", len(df_b_all_healthy))\n",
    "\n",
    "df_b_all_filtered = df_b_all[df_b_all['gene_type'] == 'protein_coding']  # Filter for protein-coding genes\n",
    "df_b_all_healthy_filtered = df_b_all_healthy[df_b_all_healthy['gene_type'] == 'protein_coding']  # Filter for protein-coding genes\n",
    "\n",
    "print(\"Filtered B-ALL length:\", len(df_b_all_filtered))\n",
    "print(\"Filtered B-ALL Healthy length:\", len(df_b_all_healthy_filtered))\n",
    "\n",
    "df_b_all_filtered = df_b_all_filtered.drop(['gene_name', 'gene_type'], axis=1)  # Drop non-numeric columns\n",
    "df_b_all_healthy_filtered = df_b_all_healthy_filtered.drop(['gene_name', 'gene_type'], axis=1)  # Drop non-numeric columns\n",
    "\n",
    "df_b_all_filtered = df_b_all_filtered.fillna(0).select_dtypes(include='number').T\n",
    "df_b_all_healthy_filtered = df_b_all_healthy_filtered.fillna(0).select_dtypes(include='number').T\n",
    "\n",
    "combined_df = cudf.concat([df_b_all_filtered, df_b_all_healthy_filtered], axis=0)\n",
    "\n",
    "combined_df['condition'] = [1] * len(df_b_all_healthy_filtered) + [0] * len(df_b_all_filtered)\n",
    "\n",
    "if (len(df_b_all_filtered) + len(df_b_all_healthy_filtered)) != combined_df.shape[0]:\n",
    "    print(f\"Expected number of rows: {len(df_b_all_filtered) + len(df_b_all_healthy_filtered)}, Actual number of rows: {combined_df.shape[0]}\")\n",
    "    raise ValueError(\"The number of rows in the combined DataFrame does not match the sum of B-ALL and B-ALL Healthy lengths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.to_pandas()  # Convert to pandas DataFrame for further processing\n",
    "\n",
    "print(f\"Amount of features in the merged DataFrame: {combined_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = combined_df['condition']  # Use the 'condition' column as the target variable\n",
    "\n",
    "combined_df.drop(columns=['condition'], inplace=True)  # Drop the 'condition' column for normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## LR (Single Test Split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Label shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = y.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Log2 Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_normalized = np.log2(combined_df + 1)  # Log2 transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_normalized.reset_index(drop=True, inplace=True)\n",
    "y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    merged_df_normalized, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Undersampler and oversampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_neighbor_value = 5  # Number of neighbors for SMOTE\n",
    "k_samples = 600\n",
    "\n",
    "# rus = RandomUnderSampler(sampling_strategy={0: k_samples}, random_state=42)\n",
    "# x_train, y_train = rus.fit_resample(x_train, y_train)\n",
    "\n",
    "# smote = SMOTE(sampling_strategy={1: k_samples}, k_neighbors=k_neighbor_value, random_state=42)\n",
    "# x_train, y_train = smote.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Variance Threshold (Remove constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_vt = VarianceThreshold(threshold=0.0)\n",
    "\n",
    "x_train = selector_vt.fit_transform(x_train)\n",
    "x_test = selector_vt.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selector = PCA(n_components=0.95, random_state=42)\n",
    "# x_train = selector.fit_transform(x_train)\n",
    "# x_test = selector.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(score_func=f_classif, k=400)\n",
    "x_train = selector.fit_transform(x_train, y_train)\n",
    "x_test = selector.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of x_train after feature selection: {x_train.shape}\"\n",
    "      f\", Shape of y_test: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "if x_train.shape[0] != y_train.shape[0] or x_test.shape[0] != y_test.shape[0]:\n",
    "    raise ValueError(\"Mismatch: number of samples in X_train/X_test and labels in y_train/y_test\")\n",
    "\n",
    "if sum(y_train == 1) == 0 or sum(y_train == 0) == 0:\n",
    "    raise ValueError(\"Training set must contain both classes (B-ALL and non-B-ALL).\")\n",
    "\n",
    "if isinstance(y_train, pd.DataFrame) or isinstance(y_test, pd.DataFrame):\n",
    "    raise ValueError(\"y_train and y_test must be Series, not DataFrames.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = LogisticRegression(\n",
    "#     penalty='l2',\n",
    "#     C=0.1,\n",
    "#     solver='liblinear',\n",
    "#     random_state=42,\n",
    "#     max_iter=1000,\n",
    "#     class_weight='balanced'\n",
    "# )\n",
    "\n",
    "# lr.fit(x_train, y_train)\n",
    "\n",
    "# y_proba = lr.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# auc_score = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# print(f\"AUC: {auc_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos_weight = sum(y_train == 0) / sum(y_train == 1)\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    use_label_encoder=False,\n",
    "    scale_pos_weight=scale_pos_weight,  # Keep this for class imbalance\n",
    "    tree_method=\"hist\",\n",
    "    device=\"cuda\",\n",
    "    \n",
    "    # Core parameters\n",
    "    n_estimators=1000,        # More trees, let early stopping decide\n",
    "    learning_rate=0.01,      # Moderate learning rate\n",
    "    max_depth=3,             # Allow some complexity for gene interactions\n",
    "    \n",
    "    # Regularization (important for high-dimensional genomics)\n",
    "    reg_lambda=5.0,          # L2 regularization\n",
    "    reg_alpha=2.0,           # L1 regularization (feature selection)\n",
    "    \n",
    "    # Sampling (reduces overfitting)\n",
    "    subsample=0.8,           # Sample 80% of rows\n",
    "    colsample_bytree=0.7,    # Sample 80% of features per tree\n",
    "    colsample_bylevel=0.7,   # Additional feature sampling\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping_rounds=50,  # Stop if no improvement\n",
    "    \n",
    "    # Reproducibility\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb.fit(x_train, y_train, eval_set=[(x_test, y_test)], verbose=True)\n",
    "\n",
    "y_proba = xgb.predict_proba(x_test)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(f\"AUC: {auc_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Statistics and Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Check seperator genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train.shape)\n",
    "\n",
    "# gene_aucs = []\n",
    "\n",
    "# for i in range(x_train.shape[1]):\n",
    "#     gene_values = x_train[:, i]\n",
    "#     try:\n",
    "#         auc_value = roc_auc_score(y_train, gene_values)\n",
    "#         gene_aucs.append((i, auc_value))\n",
    "#     except ValueError:\n",
    "#         continue\n",
    "\n",
    "# gene_aucs = np.array(gene_aucs, dtype=[('index', int), ('auc', float)])\n",
    "\n",
    "# gene_aucs_df = pd.DataFrame({\n",
    "#     \"gene\": gene_aucs['index'],\n",
    "#     \"auc\": gene_aucs['auc']\n",
    "# })\n",
    "# gene_aucs_df[\"auc_diff\"] = abs(gene_aucs_df[\"auc\"] - 0.5)  # How far from random\n",
    "# gene_aucs_df = gene_aucs_df.sort_values(\"auc_diff\", ascending=False)\n",
    "\n",
    "# top_genes = gene_aucs_df.head(5)['gene'].astype(int).tolist()\n",
    "\n",
    "# for i in top_genes[:5]:  # visualize top 5 for example\n",
    "#     # Create a DataFrame manually\n",
    "#     df_plot = pd.DataFrame({\n",
    "#         'Expression': x_train[:, i],\n",
    "#         'Class': y_train\n",
    "#     })\n",
    "#     sns.boxplot(data=df_plot, x='Class', y='Expression')\n",
    "#     plt.title(f'Expression of by Class')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Plot Prediction Score Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Wrap predictions into a DataFrame for easier slicing\n",
    "# df_scores = pd.DataFrame({\n",
    "#     \"B_ALL_score\": y_proba,\n",
    "#     \"Label\": y_test  # ensure matching index\n",
    "# })\n",
    "\n",
    "# # Plot for known B-ALL (positives)\n",
    "# plt.figure(figsize=(7, 4))\n",
    "# sns.histplot(df_scores[df_scores[\"Label\"] == 1][\"B_ALL_score\"], color=\"blue\", bins=25, kde=True)\n",
    "# plt.title(\"Predicted B-ALL Score — Known B-ALL Samples\")\n",
    "# plt.xlabel(\"Predicted Probability of B-ALL\")\n",
    "# plt.ylabel(\"Sample Count\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Plot for unlabeled/mixed\n",
    "# plt.figure(figsize=(7, 4))\n",
    "# sns.histplot(df_scores[df_scores[\"Label\"] == 0][\"B_ALL_score\"], color=\"red\", bins=25, kde=True)\n",
    "# plt.title(\"Predicted B-ALL Score — Unlabeled Samples\")\n",
    "# plt.xlabel(\"Predicted Probability of B-ALL\")\n",
    "# plt.ylabel(\"Sample Count\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "Precision recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(recall, precision, marker='.', label=f'PR AUC = {pr_auc:.3f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_gene_ids = np.array(combined_df.columns)[selector.get_support()]\n",
    "\n",
    "# x_test_named = pd.DataFrame(x_test, columns=selected_gene_ids)\n",
    "\n",
    "# explainer = shap.LinearExplainer(lr, x_train, feature_perturbation=\"interventional\")\n",
    "# shap_values = explainer.shap_values(x_test)\n",
    "\n",
    "# shap.summary_plot(shap_values, x_test_named, \n",
    "#                   plot_type=\"violin\", \n",
    "#                   max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(xgb)\n",
    "\n",
    "shap_values = explainer.shap_values(x_test)\n",
    "\n",
    "shap.summary_plot(shap_values, x_test, plot_type=\"bar\", max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gene_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## LR (Stratified KFold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "#### Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = combined_df['condition']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "auc_scores = []\n",
    "\n",
    "x = combined_df\n",
    "\n",
    "k_samples_initial = 300\n",
    "k_samples_per_fold = 100  # Number of samples to keep in the minority class\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(x, y)):\n",
    "    print(f\"Processing fold {fold + 1}...\")\n",
    "\n",
    "    x_train_fold, x_val_fold = x.iloc[train_index], x.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    x_train_fold = np.log2(x_train_fold + 1)  # Log2 transformation\n",
    "    x_val_fold = np.log2(x_val_fold + 1)      # Log2 transformation\n",
    "\n",
    "    k_samples = k_samples_initial + (k_samples_per_fold * fold)\n",
    "\n",
    "    rus = RandomUnderSampler(sampling_strategy={0: int(k_samples // 2)}, random_state=42)\n",
    "    x_train_fold, y_train_fold = rus.fit_resample(x_train_fold, y_train_fold)\n",
    "\n",
    "    smote = SMOTE(sampling_strategy={1: int(k_samples // 2)}, k_neighbors=5, random_state=42)\n",
    "    x_train_fold, y_train_fold = smote.fit_resample(x_train_fold, y_train_fold)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x_train_fold = scaler.fit_transform(x_train_fold)\n",
    "    x_val_fold = scaler.transform(x_val_fold)\n",
    "\n",
    "    selector = SelectKBest(score_func=f_classif, k=350)\n",
    "    x_train_fold = selector.fit_transform(x_train_fold, y_train_fold)\n",
    "    x_val_fold = selector.transform(x_val_fold)\n",
    "\n",
    "    lr = LogisticRegression(\n",
    "        penalty='l2',\n",
    "        C=0.1,\n",
    "        solver='liblinear',\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "\n",
    "    lr.fit(x_train, y_train)\n",
    "\n",
    "    y_proba = lr.predict_proba(x_test)[:, 1]\n",
    "\n",
    "    auc_fold = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    print(f\"Fold {fold + 1}, AUC: {auc_fold:.3f}\")\n",
    "\n",
    "print(f\"Mean AUC across all folds: {np.mean(auc_scores):.3f} ± {np.std(auc_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Statistics and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(auc_scores, kde=True, bins=5)\n",
    "plt.title(\"Distribution of AUC Scores Across Folds\")\n",
    "plt.xlabel(\"AUC\")\n",
    "plt.ylabel(\"Number of Folds\")\n",
    "plt.axvline(np.mean(auc_scores), color=\"red\", linestyle=\"--\", label=\"Mean AUC\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "# B_ALL Subtype Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "## Core (Always Run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cuml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "import shap\n",
    "import umap\n",
    "\n",
    "from sklearn.cluster import KMeans, HDBSCAN\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, silhouette_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, make_scorer\n",
    "from sklearn.utils import shuffle, resample\n",
    "from xgboost import XGBClassifier, DMatrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import plotnine as pn\n",
    "import patchworklib as pw\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.stats import ttest_ind, ks_2samp, mannwhitneyu\n",
    "import mygene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "path_to_data = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b_all = cudf.read_parquet(f\"{path_to_data}B_ALL.pq\")  # Sample names is column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b_all_filtered = df_b_all[df_b_all['gene_type'] == 'protein_coding']  # Filter for protein-coding genes\n",
    "\n",
    "df_b_all_dropped = df_b_all_filtered.drop(['gene_name', 'gene_type'], axis=1)  # Drop non-numeric columns\n",
    "\n",
    "df_b_all_transposed = df_b_all_dropped.fillna(0).select_dtypes(include='number').T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "GPU to CPU Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b_all_transposed = df_b_all_transposed.to_pandas()  # Convert to pandas DataFrame for further processing\n",
    "\n",
    "print(f\"Shape of df_b_all_transposed: {df_b_all_transposed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b_all_transposed = np.log2(df_b_all_transposed + 1)  # Log2 transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled = StandardScaler().fit_transform(df_b_all_transposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50, random_state=42)\n",
    "x_pca = pca.fit_transform(x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "KMean operations\n",
    "\n",
    "We start by doing KMeans operation N amount of times to detect the optimal amount of clusters (done later in the plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_list = []\n",
    "optimal_labels_kmeans = None\n",
    "\n",
    "for i in range(1, 6):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "\n",
    "    labels_kmeans = kmeans.fit_predict(x_pca)\n",
    "\n",
    "    if i == 3:\n",
    "        optimal_labels_kmeans = labels_kmeans\n",
    "\n",
    "    df_clusters = pd.DataFrame(x_pca, columns=[f'PC{i+1}' for i in range(x_pca.shape[1])])\n",
    "    df_clusters['cluster'] = labels_kmeans.astype(str)\n",
    "\n",
    "    clusters_list.append(df_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "KMeans plots\n",
    "\n",
    "Silhouette score between 0.51 - 0.70 is prefered\n",
    "\n",
    "(SOURCE: https://www.sciencedirect.com/science/article/pii/0377042787901257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = []\n",
    "\n",
    "silhouette_scores = []\n",
    "\n",
    "for i, df in enumerate(clusters_list):\n",
    "    # Compute silhouette score\n",
    "    if len(df['cluster'].unique()) > 1:  # Silhouette requires at least 2 clusters\n",
    "        score = silhouette_score(df[['PC1', 'PC2']], df['cluster'])\n",
    "    else:\n",
    "        score = float('nan')  # or 0.0 or \"N/A\"\n",
    "\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "for i, df in enumerate(clusters_list):\n",
    "    hull_dfs = []\n",
    "\n",
    "    for cluster in df['cluster'].unique():\n",
    "        points = df[df['cluster'] == cluster][['PC1', 'PC2']].values\n",
    "\n",
    "        # Must be >=3 points\n",
    "        if points.shape[0] >= 3:\n",
    "            try:\n",
    "                hull = ConvexHull(points)\n",
    "                hull_pts = points[hull.vertices]\n",
    "                hull_df = pd.DataFrame(hull_pts, columns=['PC1', 'PC2'])\n",
    "                hull_df['cluster'] = cluster\n",
    "                hull_dfs.append(hull_df)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    centroids = df.groupby('cluster')[['PC1', 'PC2']].mean().reset_index()\n",
    "    centroids['label'] = centroids['cluster'].astype(str)\n",
    "\n",
    "    df_hulls = pd.concat(hull_dfs, ignore_index=True)\n",
    "\n",
    "    plot = (\n",
    "        pn.ggplot(df, pn.aes('PC1', 'PC2'))\n",
    "        + pn.geom_point(pn.aes(color='cluster', shape='cluster'), size=1.5, alpha=0.7)\n",
    "        + pn.geom_polygon(df_hulls, pn.aes('PC1', 'PC2', fill='cluster', group='cluster'), alpha=0.15, show_legend=False, color='black', linetype='dashed')\n",
    "        + pn.geom_text(data=centroids, mapping=pn.aes('PC1', 'PC2', label='label'), size=8, color='black')\n",
    "        + pn.scale_fill_brewer(type='qual', palette='Set2')\n",
    "        + pn.scale_color_brewer(type='qual', palette='Set2')\n",
    "        + pn.theme_bw()\n",
    "        + pn.ggtitle(f\"Clusters = {i + 1}, Silhouette Score = {silhouette_scores[i]:.3f}\")\n",
    "    )\n",
    "\n",
    "    plots.append(pw.load_ggplot(plot, figsize=(4, 4)))\n",
    "\n",
    "g04 = ((plots[4] | plots[3]) / (plots[2] | plots[1] | plots[0]))\n",
    "g04.savefig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "Differential Expression Analysis - Setup\n",
    "\n",
    "Also adjust P-values (Benjamini-Hochberg FDR correction)\n",
    "\n",
    "SOURCE: https://physiology.med.cornell.edu/people/banfelder/qbio/resources_2008/1.5_GenespringMTC.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.float_format', lambda x: f'{x:.3e}')\n",
    "\n",
    "expression_df = df_b_all_transposed.copy()\n",
    "expression_df['cluster'] = optimal_labels_kmeans\n",
    "\n",
    "results = []\n",
    "\n",
    "for cluster_id in expression_df['cluster'].unique():\n",
    "    in_cluster = expression_df[expression_df['cluster'] == cluster_id].drop(columns=['cluster'])\n",
    "    out_cluster = expression_df[expression_df['cluster'] != cluster_id].drop(columns=['cluster'])\n",
    "\n",
    "    valid_genes = (\n",
    "        (in_cluster.var(skipna=True) > 0) |\n",
    "        (out_cluster.var(skipna=True) > 0)\n",
    "    )\n",
    "\n",
    "    in_cluster = in_cluster.loc[:, valid_genes]\n",
    "    out_cluster = out_cluster.loc[:, valid_genes]\n",
    "\n",
    "    t_stats, p_vals = ttest_ind(in_cluster, out_cluster, axis=0, equal_var=False, nan_policy='omit')\n",
    "\n",
    "    mean_diff = in_cluster.mean() - out_cluster.mean()\n",
    "\n",
    "    _, adj_pvals, _, _ = multipletests(p_vals, method='fdr_bh')\n",
    "\n",
    "    adj_pvals = np.clip(adj_pvals, 1e-300, 1.0)\n",
    "\n",
    "    res = pd.DataFrame({\n",
    "        'gene': in_cluster.columns,\n",
    "        'cluster': cluster_id,\n",
    "        'mean_diff': mean_diff.values.astype(np.float64),\n",
    "        'p_value': p_vals.astype(np.float64),\n",
    "        'adj_p_value': adj_pvals.astype(np.float64)\n",
    "    }).sort_values(by='adj_p_value')\n",
    "\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "Differential Expression Analysis - Plots\n",
    "\n",
    "These volcano plots show the mean difference and significance of a gene, we don't know what gene that is yet (will be done on the next step).\n",
    "\n",
    "1. Far left or far right (large mean difference) -> Indicates strong expression change in that cluster\n",
    "2. High on the y-axis (high −log₁₀ p-value) -> Indicates statistical significance\n",
    "\n",
    "Cluster 0 and 2 are considered promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in results:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.scatterplot(x=df['mean_diff'], y=-np.log10(df['adj_p_value']), alpha=0.6)\n",
    "    plt.axhline(-np.log10(0.05), color='red', linestyle='--')\n",
    "    plt.title(f'Cluster {df[\"cluster\"].iloc[0]} Volcano Plot')\n",
    "    plt.xlabel('Mean Difference (Effect Size)')\n",
    "    plt.ylabel('Significance (-log10 adjusted p-value)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "Top genes extraction and visualizing via box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all clusters into a single dataframe\n",
    "dea_results = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Top N genes per cluster\n",
    "top_genes_per_cluster = (\n",
    "    dea_results\n",
    "    .sort_values('adj_p_value')  # ensure it's sorted\n",
    "    .groupby('cluster')\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "# If needed, flatten into a list of unique top genes\n",
    "top_gene_list = top_genes_per_cluster['gene'].unique().tolist()\n",
    "\n",
    "# # Subset to top genes only\n",
    "expression_subset = expression_df[top_gene_list].copy()\n",
    "\n",
    "expression_subset.head()\n",
    "\n",
    "# Add cluster labels for sorting\n",
    "expression_subset['cluster'] = optimal_labels_kmeans\n",
    "\n",
    "# Sort by cluster for clean group blocks\n",
    "expression_sorted = expression_subset.sort_values('cluster')\n",
    "\n",
    "# Remove the cluster column for the heatmap\n",
    "heatmap_data = expression_sorted.drop('cluster', axis=1)\n",
    "heatmap_data_normalized = heatmap_data.apply(lambda x: (x - x.mean()) / x.std(), axis=0)\n",
    "\n",
    "# APPROACH 1: Aggregate by cluster (most informative for large datasets)\n",
    "# Calculate mean expression per cluster for each gene\n",
    "cluster_means = expression_sorted.groupby('cluster')[top_gene_list].mean()\n",
    "\n",
    "# Z-score normalize the cluster means\n",
    "cluster_means_normalized = cluster_means.apply(lambda x: (x - x.mean()) / x.std(ddof=0), axis=0)\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "sns.heatmap(cluster_means_normalized.T,  # Genes as rows, clusters as columns\n",
    "            cmap='RdBu_r',\n",
    "            center=0,\n",
    "            annot=False,\n",
    "            cbar_kws={'label': 'Z-score (cluster mean)'},\n",
    "            xticklabels=[f'Cluster {i}' for i in cluster_means.index],\n",
    "            yticklabels=True)\n",
    "plt.title('Mean Gene Expression by Cluster')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Genes')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "mg = mygene.MyGeneInfo()\n",
    "\n",
    "# Replace with your list of top genes (Ensembl or gene symbols)\n",
    "top_genes = top_gene_list[:20]  # Or all\n",
    "\n",
    "# Remove version numbers from Ensembl IDs if present (e.g., ENSG00000276672.1 → ENSG00000276672)\n",
    "top_genes = [gene.split('.')[0] for gene in top_genes]\n",
    "\n",
    "# Query annotations\n",
    "results = mg.querymany(top_genes, scopes='ensembl.gene', fields=['symbol', 'name', 'summary', 'entrezgene'], species='human')\n",
    "\n",
    "annotation_df = pd.DataFrame(results)\n",
    "annotation_df = annotation_df[['query', 'symbol', 'name', 'summary']]\n",
    "annotation_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 30 unique genes across all clusters\n",
    "\n",
    "top_genes_per_cluster = (\n",
    "    dea_results\n",
    "    .sort_values('adj_p_value')  # ensure it's sorted\n",
    "    .groupby('cluster')\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "# If needed, flatten into a list of unique top genes\n",
    "top_gene_list = top_genes_per_cluster['gene'].unique().tolist()\n",
    "\n",
    "X = expression_df[top_gene_list]\n",
    "y = optimal_labels_kmeans\n",
    "\n",
    "# np.random.shuffle(y)  # Shuffle labels for demonstration\n",
    "\n",
    "print(f\"Using {len(top_gene_list)} unique genes as features\")\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_bin = label_binarize(y_test, classes=sorted(set(y)))\n",
    "y_proba = clf.predict_proba(X_test)\n",
    "\n",
    "auc_score = roc_auc_score(y_test_bin, y_proba, average=\"macro\", multi_class=\"ovr\")\n",
    "print(f\"Multiclass AUC (OvR, macro-average): {auc_score:.3f}\")\n",
    "\n",
    "n_classes = y_test_bin.shape[1]\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=f'Cluster {i} (AUC = {roc_auc[i]:.2f})')\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Multiclass ROC Curve\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "Confustion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create display\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Raw counts\n",
    "disp1 = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                              display_labels=[f'Cluster {i}' for i in sorted(set(y))])\n",
    "disp1.plot(ax=ax1, cmap='Blues', values_format='d')\n",
    "ax1.set_title('Confusion Matrix (Raw Counts)')\n",
    "\n",
    "# Normalized (percentages)\n",
    "cm_normalized = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm_normalized,\n",
    "                              display_labels=[f'Cluster {i}' for i in sorted(set(y))])\n",
    "disp2.plot(ax=ax2, cmap='Blues', values_format='.2f')\n",
    "ax2.set_title('Confusion Matrix (Normalized)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
