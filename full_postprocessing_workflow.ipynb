{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Post processing workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Imports and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Lock, Process, set_start_method, cpu_count, Value\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from pydeseq2.dds import DeseqDataSet\n",
    "from pydeseq2.ds import DeseqStats\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "# set_start_method('spawn', force=True)\n",
    "\n",
    "error_flag = Value('i', 0)\n",
    "\n",
    "merged_dataset_path = \"merged_dataset.pq\"\n",
    "\n",
    "merged_metadata_path = \"merged_metadata.pq\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Multiprocessing part (Legacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_of_processes = 30\n",
    "\n",
    "max_amount_of_processes = 5\n",
    "\n",
    "# Necessary to run this code when running the worker\n",
    "temp_df_header = pd.read_csv(merged_dataset_path, nrows=0, header=0, sep=\",\")\n",
    "master_full_column_names = temp_df_header.columns.tolist()\n",
    "\n",
    "index_col_name = master_full_column_names[0]\n",
    "gene_count_column_names = master_full_column_names[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(start: int, end: int, increment: int, processed_files_output_path = \"data/dataset_parts\") -> None:\n",
    "    try:\n",
    "        if error_flag.value == 1:\n",
    "            print(f\"Error flag is set, skipping processing for columns {start} to {end + start}.\")\n",
    "            return\n",
    "        \n",
    "        column_names = [index_col_name] + gene_count_column_names\n",
    "        \n",
    "        # range_list = list(range(start, end))\n",
    "\n",
    "        print(f\"Processing columns {start} to {end}...\")\n",
    "\n",
    "        merged_dataset_df = pd.read_csv(merged_dataset_path, usecols=column_names[start:end], index_col=0, header=0, sep=\",\")\n",
    "\n",
    "        print(f\"Convert count data to integer type...\")\n",
    "        merged_dataset_df = merged_dataset_df.astype(np.uint32)\n",
    "\n",
    "        print(f\"Processing metadata...\")\n",
    "\n",
    "        if not os.path.exists(processed_files_output_path):\n",
    "            os.makedirs(processed_files_output_path)\n",
    "\n",
    "        output_path = f\"{processed_files_output_path}/processed_{increment}.pkl\"\n",
    "\n",
    "        merged_dataset_df.to_pickle(output_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing rows {start} to {end}: {e}\")\n",
    "\n",
    "        error_flag.value = 1\n",
    "    finally:\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def workers_manager() -> None:\n",
    "    print(\"Starting workers manager...\")\n",
    "\n",
    "    error_flag.value = 0\n",
    "\n",
    "    if not os.path.exists(merged_dataset_path):\n",
    "        raise FileNotFoundError(f\"The merged dataset file '{merged_dataset_path}' does not exist.\")\n",
    "    \n",
    "    if not os.path.exists(merged_metadata_path):\n",
    "        raise FileNotFoundError(f\"The merged metadata file '{merged_metadata_path}' does not exist.\")\n",
    "    \n",
    "    # ceil division to ensure all columns are processed\n",
    "    chunk_size = -(-len(master_full_column_names) // amount_of_processes)\n",
    "\n",
    "    if chunk_size == 0:\n",
    "        raise ValueError(\"Chunk size is zero. Please increase the number of processes or reduce the dataset size.\")\n",
    "\n",
    "    print(f\"Chunk size: {chunk_size}\")\n",
    "\n",
    "    tasks_to_process = []\n",
    "\n",
    "    running_processes = []\n",
    "\n",
    "    total_columns_allocated = 0\n",
    "\n",
    "    for i in range(amount_of_processes):\n",
    "        p = Process(target=worker, args=(total_columns_allocated, total_columns_allocated + chunk_size, i))\n",
    "\n",
    "        total_columns_allocated += chunk_size\n",
    "        tasks_to_process.append(p)\n",
    "\n",
    "    if (total_columns_allocated < len(master_full_column_names)):\n",
    "        raise ValueError(f\"Total columns allocated ({total_columns_allocated}) is less than total lines ({len(master_full_column_names)}). Please adjust the chunk size or amount of processes.\")\n",
    "    else:\n",
    "        print(f\"Total columns allocated ({total_columns_allocated}) is sufficient for total lines ({len(master_full_column_names)}). Proceeding with processing.\")\n",
    "\n",
    "\n",
    "    for i, process in enumerate(tasks_to_process):\n",
    "        # Clean up finished processes (p.is_alive() returns False if the process has finished)\n",
    "        running_processes = [p for p in running_processes if p.is_alive()]\n",
    "\n",
    "        while len(running_processes) >= max_amount_of_processes:\n",
    "            # print(f\"Waiting for processes to finish. Currently running: {len(running_processes)}\")\n",
    "\n",
    "            for p in running_processes:\n",
    "                if not p.is_alive():\n",
    "                    print(f\"Process {p.pid} has finished. Joining...\")\n",
    "                    p.join()\n",
    "                    running_processes.remove(p)\n",
    "                    break\n",
    "                else:\n",
    "                    # Sleep to avoid busy waiting\n",
    "                    time.sleep(0.1)\n",
    "            \n",
    "            # clear_output(wait=True)\n",
    "\n",
    "        process.start()\n",
    "        running_processes.append(process)\n",
    "\n",
    "        print(f\"Started process {i + 1}/{len(tasks_to_process)}. Total running processes: {len(running_processes)}\")\n",
    "\n",
    "    for process in tasks_to_process:\n",
    "        process.join()\n",
    "        print(f\"Process {process.pid} has finished.\")\n",
    "\n",
    "    print(\"All processes have completed.\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Normalization completed successfully.\")\n",
    "    print(\"Exiting...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "workers_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Full post processing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_post_process_no_chunks(\n",
    "        output_path: str = \"pydeseq_output\",\n",
    "        cpu_amount: int = 4) -> None:\n",
    "    try:\n",
    "        merged_df = pd.read_parquet(merged_dataset_path)\n",
    "        merged_metadata = pd.read_parquet(merged_metadata_path)\n",
    "\n",
    "        common_samples = merged_df.index.intersection(merged_metadata.index)\n",
    "\n",
    "        filtered_merged_df = merged_df.loc[common_samples]\n",
    "        filtered_merged_metadata = merged_metadata.loc[common_samples]\n",
    "\n",
    "        print(f\"Final merged dataframe shape: {filtered_merged_df.shape}\")\n",
    "        print(f\"Final merged metadata shape: {filtered_merged_metadata.shape}\")\n",
    "\n",
    "        if filtered_merged_df.empty or filtered_merged_metadata.empty:\n",
    "            print(\"Merged dataframe or metadata is empty. Exiting.\")\n",
    "            return\n",
    "        \n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "\n",
    "        result = gc.collect()\n",
    "\n",
    "        print(f\"Garbage collector cleaned up {result} unreachable objects after merging dataframes.\")\n",
    "\n",
    "        try:\n",
    "            dds = DeseqDataSet(\n",
    "                counts=merged_df,\n",
    "                metadata=merged_metadata,\n",
    "                design=\"~condition\",\n",
    "                n_cpus=cpu_amount\n",
    "            )\n",
    "\n",
    "            dds.deseq2()\n",
    "\n",
    "            dds.vst()\n",
    "\n",
    "            vst_transformed_counts_df = pd.DataFrame(dds.layers['vst_counts'], index=dds.obs.index, columns=dds.var.index)\n",
    "\n",
    "            vst_transformed_counts_df.to_parquet(f\"{output_path}/vst_transformed_counts.parquet\")\n",
    "\n",
    "            print(\"VST transformed counts saved successfully.\")\n",
    "\n",
    "            print(f\"Garbage collector cleaned up {result} unreachable objects after processing chunk {i + 1}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during DeseqDataSet processing: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during full post-processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_post_process_no_chunks(cpu_amount=cpu_count() - 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Merge functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pydeseq_results(output_path: str = \"data/pydeseq_output\") -> None:\n",
    "    try:\n",
    "        result_files = [f for f in os.listdir(output_path) if f.startswith('deseq_result_chunks_') and f.endswith('.pkl')]\n",
    "\n",
    "        if not result_files:\n",
    "            print(\"No result files found to merge.\")\n",
    "            return\n",
    "        \n",
    "        merged_results = []\n",
    "\n",
    "        for file in result_files:\n",
    "            file_path = os.path.join(output_path, file)\n",
    "            print(f\"Reading result file: {file_path}\")\n",
    "            df = pd.read_pickle(file_path)\n",
    "            merged_results.append(df)\n",
    "\n",
    "        if merged_results:\n",
    "            final_merged_df = pd.concat(merged_results, axis=0)\n",
    "            final_output_file = os.path.join(output_path, \"final_merged_deseq_results.pkl\")\n",
    "            final_merged_df.to_pickle(final_output_file)\n",
    "            print(f\"Final merged results saved to {final_output_file}\")\n",
    "        else:\n",
    "            print(\"No data to merge from result files.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while merging pydeseq results: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_pydeseq_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataset_parts(output_path: str = \"data/dataset_parts\", output_file: str = \"data/merged_dataset.pkl\") -> None:\n",
    "    try:\n",
    "        part_files = [f for f in os.listdir(output_path) if f.startswith('processed_') and f.endswith('.pkl')]\n",
    "        part_files.sort(key=lambda x: int(re.search(r'processed_(\\d+)', x).group(1)))\n",
    "        \n",
    "        if not part_files:\n",
    "            print(\"No dataset parts found to merge.\")\n",
    "            return\n",
    "        \n",
    "        merged_parts = []\n",
    "\n",
    "        for file in part_files:\n",
    "            file_path = os.path.join(output_path, file)\n",
    "            print(f\"Reading dataset part file: {file_path}\")\n",
    "\n",
    "            df = pd.read_pickle(file_path)\n",
    "            merged_parts.append(df)\n",
    "\n",
    "        if merged_parts:\n",
    "            final_merged_df = pd.concat(merged_parts, axis=1)\n",
    "            final_merged_df.to_pickle(output_file)\n",
    "            print(f\"Final merged dataset saved to {output_file}\")\n",
    "        else:\n",
    "            print(\"No data to merge from dataset parts.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while merging dataset parts: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dataset_parts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
