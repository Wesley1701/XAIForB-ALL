{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Optional, Union\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import mygene\n",
    "import os\n",
    "import gc\n",
    "from IPython.display import display\n",
    "\n",
    "def load_csv_in_chunks(file_path, chunk_size=8000, **kwargs):\n",
    "    \"\"\"\n",
    "    Loads a CSV file in chunks to avoid memory issues.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "        chunk_size (int): The number of rows per chunk.\n",
    "        **kwargs: Additional keyword arguments to pass to pd.read_csv()\n",
    "                  (e.g., sep=',', header=0, index_col=None, usecols=None).\n",
    "\n",
    "    Returns:\n",
    "        A pandas TextFileReader object (iterator) that yields DataFrame chunks\n",
    "        if the file exists, otherwise None.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Preparing to load {file_path} in chunks of size {chunk_size}...\")\n",
    "    try:\n",
    "        chunk_iterator = pd.read_csv(file_path, chunksize=chunk_size, **kwargs)\n",
    "        return chunk_iterator\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Clean up\n",
    "    finally:\n",
    "        if 'chunk_iterator' in locals():\n",
    "            del chunk_iterator\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def load_parquet_in_chunks(file_path, chunk_size=8000):\n",
    "    \"\"\"\n",
    "    Loads a parquet file in true chunks to avoid memory issues.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the parquet file.\n",
    "        chunk_size (int): The number of rows per chunk.\n",
    "\n",
    "    Yields:\n",
    "        pandas.DataFrame: DataFrame chunks of the specified size.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Preparing to load {file_path} in chunks of size {chunk_size}...\")\n",
    "\n",
    "    try:\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        total_rows = parquet_file.metadata.num_rows\n",
    "        print(f\"Total rows in file: {total_rows}\")\n",
    "\n",
    "        for batch in parquet_file.iter_batches(batch_size=chunk_size):\n",
    "            chunk_df = batch.to_pandas()\n",
    "            yield chunk_df\n",
    "\n",
    "            del chunk_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading parquet file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Clean up\n",
    "    finally:\n",
    "        if 'parquet_file' in locals():\n",
    "            del parquet_file\n",
    "        if 'batch' in locals():\n",
    "            del batch\n",
    "        if 'chunk_df' in locals():\n",
    "            del chunk_df\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def save_data_as_parquet(chunk_iterator, output_parquet_path, preserve_index=True):\n",
    "    \"\"\"\n",
    "    Process DataFrame chunks and save as parquet using PyArrow for maximum efficiency.\n",
    "    Handles index preservation properly across chunks.\n",
    "\n",
    "    Args:\n",
    "        chunk_iterator: Iterator yielding DataFrame chunks\n",
    "        output_parquet_path (str): Path to save the parquet file\n",
    "        preserve_index (bool): Whether to preserve the DataFrame index\n",
    "    \"\"\"\n",
    "\n",
    "    writer = None\n",
    "    total_rows = 0\n",
    "    all_index_values = set()\n",
    "\n",
    "    try:\n",
    "        for chunk_idx, chunk in enumerate(chunk_iterator):\n",
    "            if chunk.empty:\n",
    "                continue\n",
    "\n",
    "            if preserve_index and chunk.index.name is not None:\n",
    "                chunk_index_values = set(chunk.index)\n",
    "                duplicates = all_index_values.intersection(chunk_index_values)\n",
    "                if duplicates:\n",
    "                    print(f\"Warning: Found {len(duplicates)} duplicate index values in chunk {chunk_idx}\")\n",
    "                    print(f\"First few duplicates: {list(duplicates)[:5]}\")\n",
    "                all_index_values.update(chunk_index_values)\n",
    "\n",
    "            table = pa.Table.from_pandas(chunk, preserve_index=preserve_index)\n",
    "\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(output_parquet_path, table.schema)\n",
    "                if preserve_index and chunk.index.name:\n",
    "                    print(f\"Preserving index: '{chunk.index.name}' (dtype: {chunk.index.dtype})\")\n",
    "\n",
    "            writer.write_table(table)\n",
    "            total_rows += chunk.shape[0]\n",
    "\n",
    "            del table\n",
    "\n",
    "    finally:\n",
    "        if writer:\n",
    "            writer.close()\n",
    "\n",
    "        # Clean up\n",
    "        if 'writer' in locals():\n",
    "            del writer\n",
    "        if 'chunk_iterator' in locals():\n",
    "            del chunk_iterator\n",
    "        gc.collect()\n",
    "\n",
    "    if total_rows > 0:\n",
    "        print(f\"Successfully saved {total_rows} rows to parquet\")\n",
    "        if preserve_index:\n",
    "            print(f\"Total unique index values: {len(all_index_values)}\")\n",
    "    else:\n",
    "        print(\"No data to save!\")\n",
    "\n",
    "\n",
    "def drop_dataframe_chunks(\n",
    "    chunk_generator: Iterator[pd.DataFrame],\n",
    "    drop_rows: Optional[Union[List[Union[int, str]], List[int], List[str]]] = None,\n",
    "    drop_columns: Optional[Union[List[Union[int, str]], List[int], List[str]]] = None\n",
    ") -> Iterator[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generator function that drops specified rows and/or columns from DataFrame chunks\n",
    "    in a memory-efficient way. Designed to be chained with other generators.\n",
    "\n",
    "    Args:\n",
    "        chunk_generator: An iterable that yields pandas DataFrame chunks\n",
    "        drop_rows: List of row indices/names to drop, or None to keep all rows\n",
    "        drop_columns: List of column indices/names to drop, or None to keep all columns\n",
    "\n",
    "    Yields:\n",
    "        pd.DataFrame: Chunks with specified rows/columns dropped\n",
    "    \"\"\"\n",
    "    for chunk in chunk_generator:\n",
    "        if chunk.empty:\n",
    "            yield chunk\n",
    "            continue\n",
    "\n",
    "        processed_chunk = chunk.copy()\n",
    "\n",
    "        if drop_columns is not None:\n",
    "            columns_to_drop = [col for col in drop_columns if col in processed_chunk.columns]\n",
    "            if columns_to_drop:\n",
    "                processed_chunk = processed_chunk.drop(columns=columns_to_drop)\n",
    "\n",
    "        if drop_rows is not None:\n",
    "            if drop_rows and not all(isinstance(row, int) for row in drop_rows):\n",
    "                chunk_index_str = processed_chunk.index.astype(str)\n",
    "                drop_rows_str = [str(row) for row in drop_rows]\n",
    "\n",
    "                rows_to_drop = processed_chunk.index[chunk_index_str.isin(drop_rows_str)]\n",
    "                if len(rows_to_drop) > 0:\n",
    "                    processed_chunk = processed_chunk.drop(index=rows_to_drop)\n",
    "            else:\n",
    "                valid_indices = [idx for idx in drop_rows if 0 <= idx < len(processed_chunk)]\n",
    "                if valid_indices:\n",
    "                    processed_chunk = processed_chunk.drop(processed_chunk.index[valid_indices])\n",
    "\n",
    "        if not processed_chunk.empty:\n",
    "            yield processed_chunk\n",
    "\n",
    "\n",
    "def keep_dataframe_chunks(\n",
    "    chunk_generator: Iterator[pd.DataFrame],\n",
    "    keep_rows: Optional[Union[List[Union[int, str]], List[int], List[str]]] = None,\n",
    "    keep_columns: Optional[Union[List[Union[int, str]], List[int], List[str]]] = None\n",
    ") -> Iterator[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generator function that keeps only specified rows and/or columns from DataFrame chunks\n",
    "    in a memory-efficient way. Designed to be chained with other generators.\n",
    "\n",
    "    Args:\n",
    "        chunk_generator: An iterable that yields pandas DataFrame chunks\n",
    "        keep_rows: List of row indices/names to keep, or None to keep all rows\n",
    "        keep_columns: List of column indices/names to keep, or None to keep all columns\n",
    "\n",
    "    Yields:\n",
    "        pd.DataFrame: Chunks with only specified rows/columns kept\n",
    "    \"\"\"\n",
    "    for chunk in chunk_generator:\n",
    "        if chunk.empty:\n",
    "            yield chunk\n",
    "            continue\n",
    "\n",
    "        processed_chunk = chunk.copy()\n",
    "\n",
    "        if keep_columns is not None:\n",
    "            columns_to_keep = [col for col in keep_columns if col in processed_chunk.columns]\n",
    "            if columns_to_keep:\n",
    "                processed_chunk = processed_chunk[columns_to_keep]\n",
    "            else:\n",
    "                processed_chunk = processed_chunk.iloc[:0]\n",
    "\n",
    "        if keep_rows is not None:\n",
    "            if keep_rows and not all(isinstance(row, int) for row in keep_rows):\n",
    "                chunk_index_str = processed_chunk.index.astype(str)\n",
    "                keep_rows_str = [str(row) for row in keep_rows]\n",
    "\n",
    "                rows_to_keep = processed_chunk.index[chunk_index_str.isin(keep_rows_str)]\n",
    "                if len(rows_to_keep) > 0:\n",
    "                    processed_chunk = processed_chunk.loc[rows_to_keep]\n",
    "                else:\n",
    "                    processed_chunk = processed_chunk.iloc[:0]\n",
    "            else:\n",
    "                valid_indices = [idx for idx in keep_rows if 0 <= idx < len(processed_chunk)]\n",
    "                if valid_indices:\n",
    "                    processed_chunk = processed_chunk.iloc[valid_indices]\n",
    "                else:\n",
    "                    processed_chunk = processed_chunk.iloc[:0]\n",
    "\n",
    "        if not processed_chunk.empty:\n",
    "            yield processed_chunk\n",
    "\n",
    "\n",
    "def transpose_dataframe_chunks(\n",
    "    chunk_generator,\n",
    "    skip_rows=None,\n",
    "    skip_columns=None,\n",
    "    output_batch_size=1000,\n",
    "    temp_dir=\"/tmp\",\n",
    "    dtype='uint32'\n",
    "):\n",
    "    \"\"\"\n",
    "    Generator function that collects DataFrame chunks, transposes the complete dataset,\n",
    "    and yields the transposed result in batches. Designed to be chained with other generators.\n",
    "\n",
    "    Args:\n",
    "        chunk_generator: An iterable that yields pandas DataFrame chunks\n",
    "        skip_rows: List of row indices/names to skip, or None\n",
    "        skip_columns: List of column indices/names to skip, or None\n",
    "        output_batch_size: Number of rows to yield in each output batch\n",
    "        temp_dir: Directory for temporary memory-mapped file\n",
    "        dtype: Data type for memory-mapped array (default: 'uint32')\n",
    "\n",
    "    Yields:\n",
    "        pd.DataFrame: Batches of the transposed DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    chunk_list = list(chunk_generator)\n",
    "    if not chunk_list:\n",
    "        print(\"No chunks received from generator. Exiting.\")\n",
    "        return\n",
    "\n",
    "    first_chunk = chunk_list[0].copy()\n",
    "\n",
    "    if skip_columns is not None:\n",
    "        columns_to_keep = [col for col in first_chunk.columns if col not in skip_columns]\n",
    "        first_chunk = first_chunk[columns_to_keep]\n",
    "        print(f\"Skipping columns: {skip_columns}\")\n",
    "\n",
    "    if skip_rows is not None:\n",
    "        if isinstance(skip_rows[0], str):\n",
    "            rows_to_keep = [idx for idx in first_chunk.index if idx not in skip_rows]\n",
    "        else:\n",
    "            rows_to_keep = [idx for i, idx in enumerate(first_chunk.index) if i not in skip_rows]\n",
    "        first_chunk = first_chunk.loc[rows_to_keep]\n",
    "        print(f\"Skipping rows: {skip_rows}\")\n",
    "\n",
    "    original_rows = sum(len(chunk) for chunk in chunk_list)\n",
    "    original_cols = len(first_chunk.columns)\n",
    "\n",
    "    filtered_cols = first_chunk.columns.tolist()\n",
    "    n_output_rows = len(filtered_cols)\n",
    "\n",
    "    all_row_indices = []\n",
    "    for chunk in chunk_list:\n",
    "        chunk_filtered = chunk.copy()\n",
    "\n",
    "        if skip_columns is not None:\n",
    "            chunk_filtered = chunk_filtered[filtered_cols]\n",
    "        if skip_rows is not None:\n",
    "            if isinstance(skip_rows[0], str):\n",
    "                chunk_rows_to_keep = [idx for idx in chunk_filtered.index if idx not in skip_rows]\n",
    "            else:\n",
    "                chunk_rows_to_keep = [idx for i, idx in enumerate(chunk_filtered.index) if i not in skip_rows]\n",
    "            chunk_filtered = chunk_filtered.loc[chunk_rows_to_keep]\n",
    "\n",
    "        all_row_indices.extend(chunk_filtered.index.tolist())\n",
    "\n",
    "    n_output_cols = len(all_row_indices)\n",
    "\n",
    "    print(f\"Dataset dimensions:\")\n",
    "    print(f\"    Original: {original_rows} rows x {original_cols} columns\")\n",
    "    print(f\"    After filtering: {len(all_row_indices)} rows x {len(filtered_cols)} columns\")\n",
    "    print(f\"    After transpose: {n_output_rows} rows x {n_output_cols} columns\")\n",
    "\n",
    "    # Create memory-mapped file\n",
    "    try:\n",
    "        temp_file = tempfile.NamedTemporaryFile(\n",
    "            dir=temp_dir,\n",
    "            delete=False,\n",
    "            suffix='.mmap'\n",
    "        )\n",
    "        temp_filename = temp_file.name\n",
    "        temp_file.close()\n",
    "\n",
    "        # Create memory-mapped array in transposed orientation: [samples, features]\n",
    "        mmap_array = np.memmap(\n",
    "            temp_filename,\n",
    "            dtype=dtype,\n",
    "            mode='w+',\n",
    "            shape=(n_output_rows, n_output_cols)\n",
    "        )\n",
    "        print(f\"Memory map created successfully\")\n",
    "\n",
    "        print(f\"Filling memory map with data...\")\n",
    "        current_feature_idx = 0\n",
    "\n",
    "        for chunk_idx, chunk in enumerate(chunk_list):\n",
    "            chunk_filtered = chunk.copy()\n",
    "            if skip_columns is not None:\n",
    "                chunk_filtered = chunk_filtered[filtered_cols]\n",
    "            if skip_rows is not None:\n",
    "                if isinstance(skip_rows[0], str):\n",
    "                    chunk_rows_to_keep = [idx for idx in chunk_filtered.index if idx not in skip_rows]\n",
    "                else:\n",
    "                    chunk_rows_to_keep = [idx for i, idx in enumerate(chunk_filtered.index) if i not in skip_rows]\n",
    "                chunk_filtered = chunk_filtered.loc[chunk_rows_to_keep]\n",
    "\n",
    "            chunk_data = chunk_filtered.values.T.astype(dtype)\n",
    "            chunk_feature_count = chunk_filtered.shape[0]\n",
    "\n",
    "            mmap_array[:, current_feature_idx:current_feature_idx + chunk_feature_count] = chunk_data\n",
    "            current_feature_idx += chunk_feature_count\n",
    "\n",
    "            mmap_array.flush()\n",
    "            del chunk_data, chunk_filtered\n",
    "\n",
    "            print(f\"    Processed chunk {chunk_idx + 1}/{len(chunk_list)}\")\n",
    "\n",
    "        del chunk_list\n",
    "        gc.collect()\n",
    "\n",
    "        print(f\"Memory map filled successfully\")\n",
    "\n",
    "        print(f\"Yielding transposed batches (size: {output_batch_size})...\")\n",
    "        total_batches = (n_output_rows + output_batch_size - 1) // output_batch_size\n",
    "\n",
    "        for batch_idx in range(total_batches):\n",
    "            start_row = batch_idx * output_batch_size\n",
    "            end_row = min(start_row + output_batch_size, n_output_rows)\n",
    "\n",
    "            batch_data = mmap_array[start_row:end_row, :].copy()\n",
    "            batch_sample_ids = filtered_cols[start_row:end_row]\n",
    "\n",
    "            batch_df = pd.DataFrame(\n",
    "                data=batch_data,\n",
    "                index=batch_sample_ids,\n",
    "                columns=all_row_indices\n",
    "            )\n",
    "\n",
    "            batch_df = batch_df.reset_index().rename(columns={'index': 'sample_id'})\n",
    "\n",
    "            yield batch_df\n",
    "\n",
    "            del batch_data, batch_df\n",
    "            gc.collect()\n",
    "\n",
    "            print(f\"    Yielded batch {batch_idx + 1}/{total_batches}\")\n",
    "\n",
    "        print(f\"Transposition completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during memory-mapped transposition: {e}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        print(f\"Cleaning up temporary files...\")\n",
    "        try:\n",
    "            if 'mmap_array' in locals():\n",
    "                del mmap_array\n",
    "            if 'temp_filename' in locals() and os.path.exists(temp_filename):\n",
    "                os.unlink(temp_filename)\n",
    "                print(f\"Temporary file removed: {temp_filename}\")\n",
    "        except Exception as cleanup_error:\n",
    "            print(f\"Warning: Could not clean up temp file: {cleanup_error}\")\n",
    "\n",
    "\n",
    "def convert_gene_ids_to_symbols(dataset_chunk_iterator, mygene_client, gene_column_prefix=\"ENSG\"):\n",
    "    \"\"\"\n",
    "    Convert gene IDs to gene symbols using MyGene API in a memory-efficient way.\n",
    "    Only calls MyGene API once for all gene columns from the first chunk.\n",
    "\n",
    "    Args:\n",
    "        dataset_chunk_iterator: Iterator yielding DataFrame chunks\n",
    "        mygene_client: MyGene client instance\n",
    "        gene_column_prefix (str): Prefix to identify gene columns\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Complete dataset with gene symbols as column names\n",
    "    \"\"\"\n",
    "    print(\"Converting gene IDs to symbols (chunked processing)...\")\n",
    "\n",
    "    # Get first chunk to determine gene columns and create mapping\n",
    "    try:\n",
    "        first_chunk = next(dataset_chunk_iterator)\n",
    "        print(f\"Processing first chunk: {first_chunk.shape}\")\n",
    "    except StopIteration:\n",
    "        print(\"❌ Error: Dataset is empty\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Identify gene columns from first chunk\n",
    "    gene_columns = [col for col in first_chunk.columns\n",
    "                   if isinstance(col, str) and col.startswith(gene_column_prefix)]\n",
    "\n",
    "    if not gene_columns:\n",
    "        print(\"No gene columns found - returning original data\")\n",
    "        # Concatenate all chunks and return\n",
    "        all_chunks = [first_chunk]\n",
    "        for chunk in dataset_chunk_iterator:\n",
    "            all_chunks.append(chunk)\n",
    "        return pd.concat(all_chunks, axis=0, ignore_index=False)\n",
    "\n",
    "    print(f\"Found {len(gene_columns)} gene columns\")\n",
    "\n",
    "    # Single MyGene API call for all gene IDs\n",
    "    gene_id_to_symbol = {}\n",
    "    symbols_found = 0\n",
    "\n",
    "    try:\n",
    "        print(\"Making single MyGene API call...\")\n",
    "        import warnings\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            results = mygene_client.querymany(\n",
    "                gene_columns,\n",
    "                scopes='ensembl.gene',\n",
    "                fields='symbol',\n",
    "                species='human',\n",
    "                verbose=False,\n",
    "                silent=True\n",
    "            )\n",
    "\n",
    "        # Process API results\n",
    "        for result in results:\n",
    "            gene_id = result['query']\n",
    "            if 'symbol' in result and result['symbol']:\n",
    "                gene_id_to_symbol[gene_id] = result['symbol']\n",
    "                symbols_found += 1\n",
    "            else:\n",
    "                gene_id_to_symbol[gene_id] = gene_id\n",
    "\n",
    "        print(f\"✅ Successfully converted {symbols_found}/{len(gene_columns)} genes to symbols\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ MyGene API error: {e}. Using original gene IDs\")\n",
    "        gene_id_to_symbol = {gene_id: gene_id for gene_id in gene_columns}\n",
    "\n",
    "    # Create final column mapping (preserve non-gene columns like 'condition')\n",
    "    final_column_mapping = {}\n",
    "    for col in first_chunk.columns:\n",
    "        if col == 'condition':\n",
    "            final_column_mapping[col] = col\n",
    "        elif col in gene_id_to_symbol:\n",
    "            final_column_mapping[col] = gene_id_to_symbol[col]\n",
    "        else:\n",
    "            final_column_mapping[col] = col\n",
    "\n",
    "    print(\"Applying gene symbol mapping to all chunks...\")\n",
    "\n",
    "    # Process first chunk\n",
    "    renamed_first_chunk = first_chunk.rename(columns=final_column_mapping)\n",
    "    processed_chunks = [renamed_first_chunk]\n",
    "    chunk_count = 1\n",
    "\n",
    "    # Process remaining chunks with same mapping\n",
    "    for chunk in dataset_chunk_iterator:\n",
    "        renamed_chunk = chunk.rename(columns=final_column_mapping)\n",
    "        processed_chunks.append(renamed_chunk)\n",
    "        chunk_count += 1\n",
    "\n",
    "        if chunk_count % 3 == 0:\n",
    "            print(f\"  ✓ Processed {chunk_count} chunks...\")\n",
    "\n",
    "    print(f\"Concatenating {chunk_count} processed chunks...\")\n",
    "    final_dataset = pd.concat(processed_chunks, axis=0, ignore_index=False)\n",
    "\n",
    "    print(f\"✅ Final dataset shape: {final_dataset.shape}\")\n",
    "    print(f\"   Symbols converted: {symbols_found}/{len(gene_columns)}\")\n",
    "\n",
    "    return final_dataset\n",
    "\n",
    "\n",
    "def add_condition_labels_to_chunks(chunk_iterator, condition_label, dataset_name):\n",
    "    \"\"\"\n",
    "    Add condition labels to dataset chunks.\n",
    "\n",
    "    Args:\n",
    "        chunk_iterator: Iterator yielding DataFrame chunks\n",
    "        condition_label (int): Binary label (0 for healthy, 1 for unhealthy)\n",
    "        dataset_name (str): Name for logging purposes\n",
    "\n",
    "    Returns:\n",
    "        list: List of labeled DataFrame chunks\n",
    "    \"\"\"\n",
    "    print(f\"Adding label '{condition_label}' to {dataset_name} dataset...\")\n",
    "\n",
    "    labeled_chunks = []\n",
    "    for chunk in chunk_iterator:\n",
    "        chunk['condition'] = condition_label\n",
    "        labeled_chunks.append(chunk)\n",
    "\n",
    "    print(f\"✅ Completed {len(labeled_chunks)} {dataset_name} chunks\")\n",
    "    return labeled_chunks\n",
    "\n",
    "\n",
    "def merge_labeled_datasets(healthy_chunks, unhealthy_chunks):\n",
    "    \"\"\"\n",
    "    Merge healthy and unhealthy dataset chunks into one DataFrame.\n",
    "\n",
    "    Args:\n",
    "        healthy_chunks (list): List of healthy DataFrame chunks\n",
    "        unhealthy_chunks (list): List of unhealthy DataFrame chunks\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Merged dataset\n",
    "    \"\"\"\n",
    "    print(\"Merging datasets...\")\n",
    "\n",
    "    all_chunks = healthy_chunks + unhealthy_chunks\n",
    "    merged_dataset = pd.concat(all_chunks, axis=0, ignore_index=False)\n",
    "\n",
    "    print(f\"✅ Merged dataset: {len(merged_dataset)} samples, {len(merged_dataset.columns)} features\")\n",
    "    print(f\"   Healthy (0): {(merged_dataset['condition'] == 0).sum()}\")\n",
    "    print(f\"   Unhealthy (1): {(merged_dataset['condition'] == 1).sum()}\")\n",
    "\n",
    "    return merged_dataset\n",
    "\n",
    "\n",
    "def clean_duplicate_nans(chunk_iterator):\n",
    "    \"\"\"\n",
    "    Processes an iterator of DataFrame chunks to drop duplicates and NaNs.\n",
    "    Yields cleaned DataFrame chunks one at a time.\n",
    "    \"\"\"\n",
    "\n",
    "    chunks_processed = 0\n",
    "    for i, chunk_df in enumerate(chunk_iterator):\n",
    "        original_rows = len(chunk_df)\n",
    "\n",
    "        chunk_df = chunk_df.dropna()\n",
    "\n",
    "        if len(chunk_df) < original_rows:\n",
    "            print(f\"    Chunk {i+1}: Dropped {original_rows - len(chunk_df)} rows with null values.\")\n",
    "\n",
    "        if chunk_df.empty:\n",
    "            print(f\"    Chunk {i+1}: Empty after dropping NaNs, skipping...\", flush=True)\n",
    "            continue\n",
    "\n",
    "        rows_before_dedup = len(chunk_df)\n",
    "        chunk_df = chunk_df.drop_duplicates()\n",
    "\n",
    "        if rows_before_dedup > len(chunk_df):\n",
    "            print(f\"    Chunk {i+1}: Dropped {rows_before_dedup - len(chunk_df)} duplicate rows.\")\n",
    "\n",
    "        if not chunk_df.empty:\n",
    "            chunks_processed += 1\n",
    "            print(f\"    Chunk {i+1}: Yielded chunk with shape {chunk_df.shape}\")\n",
    "            yield chunk_df\n",
    "        else:\n",
    "            print(f\"    Chunk {i+1}: Empty after deduplication, skipping...\")\n",
    "\n",
    "    print(f\"Finished processing. {chunks_processed} non-empty chunks processed.\")\n",
    "\n",
    "\n",
    "def rename_index(chunk_iterator, index_name):\n",
    "    \"\"\"\n",
    "    Rename the index of DataFrame chunks.\n",
    "    \"\"\"\n",
    "    for chunk in chunk_iterator:\n",
    "        chunk.index.name = index_name\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "def filter_rows(chunk_iterator):\n",
    "    \"\"\"\n",
    "    Generator function that filters out rows where the sum of all numeric values equals 0.\n",
    "    Excludes the sample_id column from sum calculation.\n",
    "    \"\"\"\n",
    "\n",
    "    total_rows_processed = 0\n",
    "    total_rows_removed = 0\n",
    "    chunk_count = 0\n",
    "\n",
    "    for chunk_df in chunk_iterator:\n",
    "        if chunk_df.empty:\n",
    "            continue\n",
    "\n",
    "        chunk_count += 1\n",
    "        original_rows = len(chunk_df)\n",
    "\n",
    "        numeric_columns = [col for col in chunk_df.columns if col != 'sample_id']\n",
    "\n",
    "        if len(numeric_columns) == 0:\n",
    "            print(f\"    Warning: Chunk {chunk_count} has no numeric columns, keeping all rows\")\n",
    "            yield chunk_df\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            row_sums = chunk_df[numeric_columns].sum(axis=1)\n",
    "            filtered_chunk = chunk_df[row_sums != 0]\n",
    "        except Exception as e:\n",
    "            print(f\"    Error in chunk {chunk_count}: {e}\")\n",
    "            print(f\"    Column types: {chunk_df.dtypes}\")\n",
    "            raise\n",
    "\n",
    "        rows_removed = original_rows - len(filtered_chunk)\n",
    "        total_rows_processed += original_rows\n",
    "        total_rows_removed += rows_removed\n",
    "\n",
    "        if rows_removed > 0:\n",
    "            print(f\"    Chunk {chunk_count}: Removed {rows_removed}/{original_rows} rows with zero sum\")\n",
    "\n",
    "        yield filtered_chunk\n",
    "\n",
    "    print(f\"Filtering complete: {total_rows_removed}/{total_rows_processed} rows removed\")\n",
    "\n",
    "\n",
    "def get_healthy_whole_blood_samples(metadata_path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Get healthy whole blood samples from the metadata.\n",
    "\n",
    "    Args:\n",
    "        metadata_path (str): Path to the metadata CSV file\n",
    "        gtex_blood_ids (list[str]): List of GTEx whole blood sample IDs\n",
    "\n",
    "    Returns:\n",
    "        list[str]: List of healthy whole blood sample IDs\n",
    "    \"\"\"\n",
    "    metadata_df = pd.read_csv(metadata_path, index_col=0)\n",
    "\n",
    "    # Get all samples with 'Whole Blood' SMTSD\n",
    "    whole_blood_samples = metadata_df[metadata_df['SMTSD'] == 'Whole Blood']\n",
    "    # Filter on RNA integrity (SMRIN) to remove low quality samples\n",
    "    healthy_samples = whole_blood_samples[\"SMRIN\"] >= 7.0\n",
    "\n",
    "    # Return SAMPIDs of healthy whole blood samples\n",
    "    healthy_whole_blood_samples = healthy_samples.index.tolist()\n",
    "    return healthy_whole_blood_samples\n",
    "\n",
    "def align_gene_columns_generator(healthy_chunk_generator, unhealthy_chunk_generator,\n",
    "                                gene_column_prefix=\"ENSG\"):\n",
    "    \"\"\"\n",
    "    Generator that aligns gene columns between healthy and unhealthy datasets.\n",
    "    Handles datasets of different sizes by yielding None for exhausted iterators.\n",
    "\n",
    "    Args:\n",
    "        healthy_chunk_generator: Generator yielding healthy dataset chunks\n",
    "        unhealthy_chunk_generator: Generator yielding unhealthy dataset chunks\n",
    "        gene_column_prefix (str): Prefix to identify gene columns (default: \"ENSG\")\n",
    "\n",
    "    Yields:\n",
    "        tuple: (aligned_healthy_chunk, aligned_unhealthy_chunk) for each chunk pair\n",
    "               None is yielded for exhausted datasets\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting gene column alignment...\")\n",
    "\n",
    "    healthy_iter = iter(healthy_chunk_generator)\n",
    "    unhealthy_iter = iter(unhealthy_chunk_generator)\n",
    "\n",
    "    try:\n",
    "        first_healthy_chunk = next(healthy_iter)\n",
    "        first_unhealthy_chunk = next(unhealthy_iter)\n",
    "    except StopIteration:\n",
    "        print(\"ERROR: One or both datasets are empty!\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nProcessing gene columns and stripping version suffixes...\")\n",
    "    def extract_gene_info(chunk, dataset_name):\n",
    "        \"\"\"Extract gene columns and create mapping from original_column to base_id\"\"\"\n",
    "        print(f\"Processing {dataset_name} dataset...\")\n",
    "\n",
    "        original_to_base = {}\n",
    "        base_gene_ids = set()\n",
    "        total_gene_columns = 0\n",
    "        duplicate_count = 0\n",
    "\n",
    "        for column in chunk.columns:\n",
    "            if isinstance(column, str) and column.startswith(gene_column_prefix):\n",
    "                total_gene_columns += 1\n",
    "                base_gene_id = column.split('.')[0]\n",
    "\n",
    "                # Handle duplicates (keep first occurrence)\n",
    "                if base_gene_id not in base_gene_ids:\n",
    "                    original_to_base[column] = base_gene_id\n",
    "                    base_gene_ids.add(base_gene_id)\n",
    "                else:\n",
    "                    duplicate_count += 1\n",
    "                    print(f\"Warning: Duplicate base gene {base_gene_id} found, skipping {column}\")\n",
    "\n",
    "        print(f\"Total gene columns found: {total_gene_columns}\")\n",
    "        print(f\"Unique base gene IDs: {len(base_gene_ids)}\")\n",
    "        if duplicate_count > 0:\n",
    "            print(f\"Duplicates removed: {duplicate_count}\")\n",
    "\n",
    "        return original_to_base, base_gene_ids\n",
    "\n",
    "    healthy_rename_mapping, healthy_base_genes = extract_gene_info(first_healthy_chunk, \"HEALTHY\")\n",
    "    unhealthy_rename_mapping, unhealthy_base_genes = extract_gene_info(first_unhealthy_chunk, \"UNHEALTHY\")\n",
    "\n",
    "    common_base_genes = healthy_base_genes & unhealthy_base_genes\n",
    "\n",
    "    if not common_base_genes:\n",
    "        print(\"ERROR: No common genes found between datasets!\")\n",
    "        return\n",
    "\n",
    "    print(f\"Common genes: {len(common_base_genes)}\")\n",
    "    print(f\"Genes exclusive to healthy dataset: {len(healthy_base_genes - common_base_genes)}\")\n",
    "    print(f\"Genes exclusive to unhealthy dataset: {len(unhealthy_base_genes - common_base_genes)}\")\n",
    "\n",
    "    renamed_first_healthy = first_healthy_chunk.rename(columns=healthy_rename_mapping)\n",
    "    renamed_first_unhealthy = first_unhealthy_chunk.rename(columns=unhealthy_rename_mapping)\n",
    "\n",
    "    healthy_non_gene_cols = [col for col in renamed_first_healthy.columns\n",
    "                           if not (isinstance(col, str) and col.startswith(gene_column_prefix))]\n",
    "    unhealthy_non_gene_cols = [col for col in renamed_first_unhealthy.columns\n",
    "                             if not (isinstance(col, str) and col.startswith(gene_column_prefix))]\n",
    "\n",
    "    # Validate that non-gene columns match\n",
    "    if set(healthy_non_gene_cols) != set(unhealthy_non_gene_cols):\n",
    "        print(\"Warning: Non-gene columns differ between datasets!\")\n",
    "        print(f\"Healthy only: {set(healthy_non_gene_cols) - set(unhealthy_non_gene_cols)}\")\n",
    "        print(f\"Unhealthy only: {set(unhealthy_non_gene_cols) - set(healthy_non_gene_cols)}\")\n",
    "\n",
    "    non_gene_cols = healthy_non_gene_cols\n",
    "    common_gene_base_ids = sorted(common_base_genes)\n",
    "    final_columns = non_gene_cols + common_gene_base_ids\n",
    "\n",
    "    print(\"\\nProcessing and yielding aligned chunks...\")\n",
    "    def process_chunk(chunk, rename_mapping):\n",
    "        \"\"\"Process a single chunk: rename and select columns\"\"\"\n",
    "        if chunk is None:\n",
    "            return None\n",
    "\n",
    "        renamed_chunk = chunk.rename(columns=rename_mapping)\n",
    "        aligned_chunk = renamed_chunk[final_columns].copy()\n",
    "\n",
    "        return aligned_chunk\n",
    "\n",
    "    aligned_first_healthy = process_chunk(first_healthy_chunk, healthy_rename_mapping)\n",
    "    aligned_first_unhealthy = process_chunk(first_unhealthy_chunk, unhealthy_rename_mapping)\n",
    "\n",
    "    yield aligned_first_healthy, aligned_first_unhealthy\n",
    "\n",
    "    chunk_count = 1\n",
    "    healthy_exhausted = False\n",
    "    unhealthy_exhausted = False\n",
    "\n",
    "    while not (healthy_exhausted and unhealthy_exhausted):\n",
    "        chunk_count += 1\n",
    "\n",
    "        healthy_chunk = None\n",
    "        if not healthy_exhausted:\n",
    "            try:\n",
    "                healthy_chunk = next(healthy_iter)\n",
    "            except StopIteration:\n",
    "                healthy_exhausted = True\n",
    "                print(f\"Healthy dataset exhausted after {chunk_count-1} chunks\")\n",
    "\n",
    "        unhealthy_chunk = None\n",
    "        if not unhealthy_exhausted:\n",
    "            try:\n",
    "                unhealthy_chunk = next(unhealthy_iter)\n",
    "            except StopIteration:\n",
    "                unhealthy_exhausted = True\n",
    "                print(f\"Unhealthy dataset exhausted after {chunk_count-1} chunks\")\n",
    "\n",
    "        if healthy_chunk is None and unhealthy_chunk is None:\n",
    "            break\n",
    "\n",
    "        aligned_healthy = process_chunk(healthy_chunk, healthy_rename_mapping)\n",
    "        aligned_unhealthy = process_chunk(unhealthy_chunk, unhealthy_rename_mapping)\n",
    "\n",
    "        yield aligned_healthy, aligned_unhealthy\n",
    "\n",
    "    print(f\"\\nAlignment complete!\")\n",
    "\n",
    "    # Clean up\n",
    "    if 'healthy_iter' in locals():\n",
    "        del healthy_iter\n",
    "    if 'unhealthy_iter' in locals():\n",
    "        del unhealthy_iter\n",
    "    if 'first_healthy_chunk' in locals():\n",
    "        del first_healthy_chunk\n",
    "    if 'first_unhealthy_chunk' in locals():\n",
    "        del first_unhealthy_chunk\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def merge_datasets_generator(aligned_pair_iterator):\n",
    "    \"\"\"\n",
    "    Generator that merges healthy and unhealthy dataset chunks into single merged chunks.\n",
    "\n",
    "    Args:\n",
    "        aligned_chunk_pairs_generator: Generator yielding (healthy_chunk, unhealthy_chunk) tuples\n",
    "        chunk_size (int, optional): Target size for merged chunks. If None, merges each pair directly.\n",
    "\n",
    "    Yields:\n",
    "        pandas.DataFrame: Merged chunks containing both healthy and unhealthy data\n",
    "    \"\"\"\n",
    "\n",
    "    for healthy_chunk, unhealthy_chunk in aligned_pair_iterator:\n",
    "        chunks_to_merge = []\n",
    "\n",
    "        if healthy_chunk is not None:\n",
    "            chunks_to_merge.append(healthy_chunk)\n",
    "\n",
    "        if unhealthy_chunk is not None:\n",
    "            chunks_to_merge.append(unhealthy_chunk)\n",
    "\n",
    "        if chunks_to_merge:\n",
    "            # Concatenate available chunks\n",
    "            merged_chunk = pd.concat(chunks_to_merge)\n",
    "            yield merged_chunk\n",
    "            del merged_chunk\n",
    "\n",
    "    # Clean up\n",
    "    if 'aligned_pair_iterator' in locals():\n",
    "        del aligned_pair_iterator\n",
    "    if 'healthy_chunk' in locals():\n",
    "        del healthy_chunk\n",
    "    if 'unhealthy_chunk' in locals():\n",
    "        del unhealthy_chunk\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def set_index_column(chunk_generator, column_name, drop=True):\n",
    "    \"\"\"\n",
    "    Generator function that sets a specified column as the index for each DataFrame chunk.\n",
    "\n",
    "    Args:\n",
    "        chunk_generator: An iterable that yields pandas DataFrame chunks\n",
    "        column_name: Name of the column to set as index\n",
    "        drop: Whether to drop the column after setting it as index (default: True)\n",
    "\n",
    "    Yields:\n",
    "        pd.DataFrame: Chunks with the specified column set as index\n",
    "    \"\"\"\n",
    "    for chunk in chunk_generator:\n",
    "        if chunk.empty:\n",
    "            yield chunk\n",
    "            continue\n",
    "\n",
    "        if column_name not in chunk.columns:\n",
    "            yield chunk\n",
    "            continue\n",
    "\n",
    "        chunk_with_index = chunk.set_index(column_name, drop=drop)\n",
    "\n",
    "        yield chunk_with_index\n",
    "\n",
    "\n",
    "def prepare_metadata_generator(aligned_pair_iterator, output_metadata_path=\"data/merged_metadata.pq\"):\n",
    "    \"\"\"\n",
    "    Generator that creates metadata from aligned chunk pairs and saves to parquet.\n",
    "\n",
    "    Args:\n",
    "        aligned_chunk_pairs_generator: Generator yielding (healthy_chunk, unhealthy_chunk) tuples\n",
    "        output_metadata_path: Path to save metadata parquet file\n",
    "\n",
    "    Yields:\n",
    "        tuple: (healthy_chunk, unhealthy_chunk) - passes through the original chunks\n",
    "    \"\"\"\n",
    "\n",
    "    all_metadata = []\n",
    "\n",
    "    for healthy_chunk, unhealthy_chunk in aligned_pair_iterator:\n",
    "        chunk_metadata = []\n",
    "\n",
    "        if healthy_chunk is not None:\n",
    "            healthy_sample_ids = [str(sid) for sid in healthy_chunk.index.tolist()]\n",
    "            healthy_metadata = [{'sample_id': sid, 'condition': 0, 'batch': 'GTEx'} for sid in healthy_sample_ids]\n",
    "            chunk_metadata.extend(healthy_metadata)\n",
    "\n",
    "        if unhealthy_chunk is not None:\n",
    "            unhealthy_sample_ids = [str(sid) for sid in unhealthy_chunk.index.tolist()]\n",
    "            unhealthy_metadata = [{'sample_id': sid, 'condition': 1, 'batch': 'GDC'} for sid in unhealthy_sample_ids]\n",
    "            chunk_metadata.extend(unhealthy_metadata)\n",
    "\n",
    "        all_metadata.extend(chunk_metadata)\n",
    "\n",
    "        # Yield the chunks unchanged (pass-through)\n",
    "        yield healthy_chunk, unhealthy_chunk\n",
    "        del healthy_chunk, unhealthy_chunk\n",
    "\n",
    "    if all_metadata:\n",
    "        metadata_df = pd.DataFrame(all_metadata)\n",
    "        metadata_df.to_parquet(output_metadata_path, index=False)\n",
    "        print(f\"Metadata saved to {output_metadata_path} with {len(metadata_df)} records\")\n",
    "    else:\n",
    "        print(\"No metadata to save\")\n",
    "\n",
    "    # Clean up\n",
    "    if 'all_metadata' in locals() or 'all_metadata' in globals():\n",
    "        del all_metadata\n",
    "    if 'metadata_df' in locals() or 'metadata_df' in globals():\n",
    "        del metadata_df\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def check_metadata(healthy_generator, unhealthy_generator, merged_metadata_generator):\n",
    "    \"\"\"\n",
    "    Comprehensive validation of metadata against healthy and unhealthy datasets.\n",
    "\n",
    "    Args:\n",
    "        healthy_generator: Generator yielding healthy dataset chunks\n",
    "        unhealthy_generator: Generator yielding unhealthy dataset chunks\n",
    "        merged_metadata_generator: Generator yielding metadata chunks\n",
    "\n",
    "    Raises:\n",
    "        Exception: If validation fails\n",
    "    \"\"\"\n",
    "\n",
    "    # Collect all sample IDs from datasets\n",
    "    healthy_sample_ids = set()\n",
    "    unhealthy_sample_ids = set()\n",
    "\n",
    "    healthy_dataset_line_count = 0\n",
    "    unhealthy_dataset_line_count = 0\n",
    "\n",
    "    print(\"Collecting sample IDs from healthy dataset...\")\n",
    "    for chunk in healthy_generator:\n",
    "        chunk_sample_ids = set(str(sid) for sid in chunk.index.tolist())\n",
    "        healthy_sample_ids.update(chunk_sample_ids)\n",
    "        healthy_dataset_line_count += len(chunk)\n",
    "\n",
    "    print(\"Collecting sample IDs from unhealthy dataset...\")\n",
    "    for chunk in unhealthy_generator:\n",
    "        chunk_sample_ids = set(str(sid) for sid in chunk.index.tolist())\n",
    "        unhealthy_sample_ids.update(chunk_sample_ids)\n",
    "        unhealthy_dataset_line_count += len(chunk)\n",
    "\n",
    "    # Collect metadata information\n",
    "    metadata_sample_ids = set()\n",
    "    healthy_metadata_ids = set()\n",
    "    unhealthy_metadata_ids = set()\n",
    "    gtex_batch_ids = set()\n",
    "    gdc_batch_ids = set()\n",
    "    condition_0_ids = set()\n",
    "    condition_1_ids = set()\n",
    "\n",
    "    metadata_line_count = 0\n",
    "\n",
    "    print(\"Validating metadata...\")\n",
    "    for chunk in merged_metadata_generator:\n",
    "        metadata_line_count += len(chunk)\n",
    "\n",
    "        # Process each row in metadata chunk\n",
    "        for _, row in chunk.iterrows():\n",
    "            sample_id = str(row['sample_id'])\n",
    "            condition = row['condition']\n",
    "            batch = row['batch']\n",
    "\n",
    "            metadata_sample_ids.add(sample_id)\n",
    "\n",
    "            # Categorize by condition\n",
    "            if condition == 0:\n",
    "                condition_0_ids.add(sample_id)\n",
    "                healthy_metadata_ids.add(sample_id)\n",
    "            elif condition == 1:\n",
    "                condition_1_ids.add(sample_id)\n",
    "                unhealthy_metadata_ids.add(sample_id)\n",
    "            else:\n",
    "                raise Exception(f\"Invalid condition value: {condition} for sample {sample_id}\")\n",
    "\n",
    "            # Categorize by batch\n",
    "            if batch == 'GTEx':\n",
    "                gtex_batch_ids.add(sample_id)\n",
    "            elif batch == 'GDC':\n",
    "                gdc_batch_ids.add(sample_id)\n",
    "            else:\n",
    "                raise Exception(f\"Invalid batch value: {batch} for sample {sample_id}\")\n",
    "\n",
    "    # Validation checks\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"METADATA VALIDATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 1. Check row counts\n",
    "    total_dataset_count = healthy_dataset_line_count + unhealthy_dataset_line_count\n",
    "    print(f\"Dataset row counts:\")\n",
    "    print(f\"  Healthy: {healthy_dataset_line_count}\")\n",
    "    print(f\"  Unhealthy: {unhealthy_dataset_line_count}\")\n",
    "    print(f\"  Total: {total_dataset_count}\")\n",
    "    print(f\"  Metadata: {metadata_line_count}\")\n",
    "\n",
    "    if total_dataset_count != metadata_line_count:\n",
    "        raise Exception(f\"Row count mismatch! Dataset total: {total_dataset_count}, Metadata: {metadata_line_count}\")\n",
    "\n",
    "    # 2. Check sample ID completeness\n",
    "    print(f\"\\nSample ID validation:\")\n",
    "    print(f\"  Unique healthy samples in dataset: {len(healthy_sample_ids)}\")\n",
    "    print(f\"  Unique unhealthy samples in dataset: {len(unhealthy_sample_ids)}\")\n",
    "    print(f\"  Total unique samples in datasets: {len(healthy_sample_ids) + len(unhealthy_sample_ids)}\")\n",
    "    print(f\"  Unique samples in metadata: {len(metadata_sample_ids)}\")\n",
    "\n",
    "    # Check if all dataset sample IDs are in metadata\n",
    "    missing_healthy_in_metadata = healthy_sample_ids - metadata_sample_ids\n",
    "    missing_unhealthy_in_metadata = unhealthy_sample_ids - metadata_sample_ids\n",
    "    extra_metadata_samples = metadata_sample_ids - (healthy_sample_ids | unhealthy_sample_ids)\n",
    "\n",
    "    if missing_healthy_in_metadata:\n",
    "        raise Exception(f\"Missing healthy samples in metadata: {list(missing_healthy_in_metadata)[:10]}...\")\n",
    "\n",
    "    if missing_unhealthy_in_metadata:\n",
    "        raise Exception(f\"Missing unhealthy samples in metadata: {list(missing_unhealthy_in_metadata)[:10]}...\")\n",
    "\n",
    "    if extra_metadata_samples:\n",
    "        raise Exception(f\"Extra samples in metadata not in datasets: {list(extra_metadata_samples)[:10]}...\")\n",
    "\n",
    "    # 3. Check condition mapping\n",
    "    print(f\"\\nCondition validation:\")\n",
    "    print(f\"  Condition 0 (healthy) samples: {len(condition_0_ids)}\")\n",
    "    print(f\"  Condition 1 (unhealthy) samples: {len(condition_1_ids)}\")\n",
    "\n",
    "    # Verify condition 0 matches healthy dataset\n",
    "    healthy_condition_mismatch = healthy_sample_ids - condition_0_ids\n",
    "    condition_0_mismatch = condition_0_ids - healthy_sample_ids\n",
    "\n",
    "    if healthy_condition_mismatch:\n",
    "        raise Exception(f\"Healthy samples with wrong condition in metadata: {list(healthy_condition_mismatch)[:10]}...\")\n",
    "\n",
    "    if condition_0_mismatch:\n",
    "        raise Exception(f\"Condition 0 samples not in healthy dataset: {list(condition_0_mismatch)[:10]}...\")\n",
    "\n",
    "    # Verify condition 1 matches unhealthy dataset\n",
    "    unhealthy_condition_mismatch = unhealthy_sample_ids - condition_1_ids\n",
    "    condition_1_mismatch = condition_1_ids - unhealthy_sample_ids\n",
    "\n",
    "    if unhealthy_condition_mismatch:\n",
    "        raise Exception(f\"Unhealthy samples with wrong condition in metadata: {list(unhealthy_condition_mismatch)[:10]}...\")\n",
    "\n",
    "    if condition_1_mismatch:\n",
    "        raise Exception(f\"Condition 1 samples not in unhealthy dataset: {list(condition_1_mismatch)[:10]}...\")\n",
    "\n",
    "    # 4. Check batch mapping\n",
    "    print(f\"\\nBatch validation:\")\n",
    "    print(f\"  GTEx batch samples: {len(gtex_batch_ids)}\")\n",
    "    print(f\"  GDC batch samples: {len(gdc_batch_ids)}\")\n",
    "\n",
    "    # Check if condition and batch alignment is correct\n",
    "    gtex_condition_mismatch = gtex_batch_ids - condition_0_ids\n",
    "    gdc_condition_mismatch = gdc_batch_ids - condition_1_ids\n",
    "    condition_0_batch_mismatch = condition_0_ids - gtex_batch_ids\n",
    "    condition_1_batch_mismatch = condition_1_ids - gdc_batch_ids\n",
    "\n",
    "    if gtex_condition_mismatch:\n",
    "        raise Exception(f\"GTEx samples with wrong condition (should be 0): {list(gtex_condition_mismatch)[:10]}...\")\n",
    "\n",
    "    if gdc_condition_mismatch:\n",
    "        raise Exception(f\"GDC samples with wrong condition (should be 1): {list(gdc_condition_mismatch)[:10]}...\")\n",
    "\n",
    "    if condition_0_batch_mismatch:\n",
    "        raise Exception(f\"Condition 0 samples with wrong batch (should be GTEx): {list(condition_0_batch_mismatch)[:10]}...\")\n",
    "\n",
    "    if condition_1_batch_mismatch:\n",
    "        raise Exception(f\"Condition 1 samples with wrong batch (should be GDC): {list(condition_1_batch_mismatch)[:10]}...\")\n",
    "\n",
    "    # 5. Check for duplicates in metadata\n",
    "    if len(metadata_sample_ids) != metadata_line_count:\n",
    "        print(f\"\\nWARNING: Duplicate sample IDs detected in metadata!\")\n",
    "        print(f\"  Unique sample IDs: {len(metadata_sample_ids)}\")\n",
    "        print(f\"  Total metadata rows: {metadata_line_count}\")\n",
    "        print(f\"  Duplicates: {metadata_line_count - len(metadata_sample_ids)}\")\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"✅ VALIDATION PASSED!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"✅ Row counts match: {metadata_line_count} total\")\n",
    "    print(f\"✅ All sample IDs present and correct\")\n",
    "    print(f\"✅ Conditions correctly mapped: {len(condition_0_ids)} healthy, {len(condition_1_ids)} unhealthy\")\n",
    "    print(f\"✅ Batches correctly mapped: {len(gtex_batch_ids)} GTEx, {len(gdc_batch_ids)} GDC\")\n",
    "    print(f\"✅ No sample ID mismatches detected\")\n",
    "\n",
    "\n",
    "def get_healthy_whole_blood_samples(metadata_path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Get healthy whole blood samples from the metadata.\n",
    "\n",
    "    Args:\n",
    "        metadata_path (str): Path to the metadata CSV file\n",
    "        gtex_blood_ids (list[str]): List of GTEx whole blood sample IDs\n",
    "\n",
    "    Returns:\n",
    "        list[str]: List of healthy whole blood sample IDs\n",
    "    \"\"\"\n",
    "    metadata_df = pd.read_csv(metadata_path, index_col=0)\n",
    "\n",
    "    # Get all samples with 'Whole Blood' SMTSD\n",
    "    whole_blood_samples = metadata_df[metadata_df['SMTSD'] == 'Whole Blood']\n",
    "    # Filter on RNA integrity (SMRIN) to remove low quality samples\n",
    "    healthy_samples = whole_blood_samples[\"SMRIN\"] >= 7.0\n",
    "\n",
    "    # Return SAMPIDs of healthy whole blood samples\n",
    "    healthy_whole_blood_samples = healthy_samples.index.tolist()\n",
    "    return healthy_whole_blood_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Unhealthy Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthy_dataset_file = 'data/unhealthy_data.pq'\n",
    "unhealthy_output_file = 'data/unhealthy_data_preprocessed.pq'\n",
    "chunk_size = 8000\n",
    "\n",
    "chunk_iterator = load_parquet_in_chunks(unhealthy_dataset_file)\n",
    "print(f\"Starting preprocessing for Unhealthy Dataset: {unhealthy_dataset_file}...\")\n",
    "\n",
    "chunk_iterator = filter_rows(chunk_iterator)\n",
    "chunk_iterator = transpose_dataframe_chunks(chunk_iterator, output_batch_size=chunk_size, dtype='float32')\n",
    "chunk_iterator = rename_index(chunk_iterator, 'sample_id')\n",
    "chunk_iterator = set_index_column(chunk_iterator, 'sample_id')\n",
    "chunk_iterator = clean_duplicate_nans(chunk_iterator)\n",
    "\n",
    "save_data_as_parquet(chunk_iterator, unhealthy_output_file)\n",
    "\n",
    "# Clean up\n",
    "if 'chunk_iterator' in locals():\n",
    "    del chunk_iterator\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthy_iterator = load_parquet_in_chunks('data/unhealthy_data_preprocessed.pq')\n",
    "\n",
    "if unhealthy_iterator:\n",
    "    first_chunk = next(unhealthy_iterator)\n",
    "\n",
    "    # Basic info\n",
    "    print(f\"First chunk shape: {first_chunk.shape}\")\n",
    "    print(f\"Columns: {list(first_chunk.columns)}\")\n",
    "    print(f\"Data types:\\n{first_chunk.dtypes}\")\n",
    "    print(f\"Memory usage: {first_chunk.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(first_chunk.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Healthy Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = 'data/gene_tpm_2022-06-06_v10_breast_mammary_tissue.gct'\n",
    "output_file = 'data/healthy_data.pq'\n",
    "\n",
    "gct_chunk_iterator = load_csv_in_chunks(\n",
    "    file_path=dataset_file,\n",
    "    sep='\\t',\n",
    "    skiprows=2,\n",
    "    header=0,\n",
    "    index_col=0,\n",
    ")\n",
    "\n",
    "if gct_chunk_iterator:\n",
    "    print(\"Saving GCT dataset as pickle...\")\n",
    "\n",
    "    save_data_as_parquet(\n",
    "        chunk_iterator=gct_chunk_iterator,\n",
    "        output_parquet_path=output_file\n",
    "    )\n",
    "else:\n",
    "    print(f\"Failed to load {dataset_file} using load_csv_in_chunks.\")\n",
    "\n",
    "# Clean up\n",
    "if 'gct_chunk_iterator' in locals() or 'gct_chunk_iterator' in globals():\n",
    "    del gct_chunk_iterator\n",
    "if 'dataset_file' in locals() or 'dataset_file' in globals():\n",
    "    del dataset_file\n",
    "if 'output_file' in locals() or 'output_file' in globals():\n",
    "    del output_file\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthy_iterator = load_parquet_in_chunks('data/healthy_data.pq')\n",
    "\n",
    "if unhealthy_iterator:\n",
    "    first_chunk = next(unhealthy_iterator)\n",
    "\n",
    "    # Basic info\n",
    "    print(f\"First chunk shape: {first_chunk.shape}\")\n",
    "    print(f\"Columns: {list(first_chunk.columns)}\")\n",
    "    print(f\"Data types:\\n{first_chunk.dtypes}\")\n",
    "    print(f\"Memory usage: {first_chunk.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(first_chunk.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_data_input = 'data/healthy_data.pq'\n",
    "healthy_data_output = 'data/healthy_data_preprocessed.pq'\n",
    "exclude_cols = ['Description']\n",
    "\n",
    "chunk_iterator = load_parquet_in_chunks(\n",
    "    file_path=healthy_data_input\n",
    ")\n",
    "\n",
    "chunk_iterator = drop_dataframe_chunks(chunk_generator=chunk_iterator, drop_columns=['Description'])\n",
    "chunk_iterator = filter_rows(chunk_iterator)\n",
    "chunk_iterator = transpose_dataframe_chunks(chunk_generator=chunk_iterator)\n",
    "chunk_iterator = set_index_column(chunk_iterator, 'sample_id')\n",
    "chunk_iterator = clean_duplicate_nans(chunk_iterator)\n",
    "\n",
    "save_data_as_parquet(\n",
    "    chunk_iterator=chunk_iterator,\n",
    "    output_parquet_path=healthy_data_output\n",
    ")\n",
    "\n",
    "# Clean up\n",
    "if 'chunk_iterator' in locals() or 'chunk_iterator' in globals():\n",
    "    del chunk_iterator\n",
    "if 'healthy_data_input' in locals() or 'healthy_data_input' in globals():\n",
    "    del healthy_data_input\n",
    "if 'healthy_data_output' in locals() or 'healthy_data_output' in globals():\n",
    "    del healthy_data_output\n",
    "if 'exclude_cols' in locals() or 'exclude_cols' in globals():\n",
    "    del exclude_cols\n",
    "if 'chunk_size' in locals() or 'chunk_size' in globals():\n",
    "    del chunk_size\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthy_iterator = load_parquet_in_chunks('data/healthy_data_preprocessed.pq')\n",
    "\n",
    "if unhealthy_iterator:\n",
    "    first_chunk = next(unhealthy_iterator)\n",
    "\n",
    "    # Basic info\n",
    "    print(f\"First chunk shape: {first_chunk.shape}\")\n",
    "    print(f\"Columns: {list(first_chunk.columns)}\")\n",
    "    print(f\"Data types:\\n{first_chunk.dtypes}\")\n",
    "    print(f\"Memory usage: {first_chunk.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(first_chunk.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_data_path = 'data/healthy_data_preprocessed.pq'\n",
    "unhealthy_data_path = 'data/unhealthy_data_preprocessed.pq'\n",
    "\n",
    "\n",
    "healthy_iterator = load_parquet_in_chunks(healthy_data_path)\n",
    "unhealthy_iterator = load_parquet_in_chunks(unhealthy_data_path)\n",
    "\n",
    "aligned_pair_iterator = align_gene_columns_generator(healthy_iterator, unhealthy_iterator)\n",
    "aligned_pair_iterator = prepare_metadata_generator(aligned_pair_iterator, output_metadata_path=\"data/merged_metadata.pq\")\n",
    "merged_chunk_iterator = merge_datasets_generator(aligned_pair_iterator)\n",
    "\n",
    "save_data_as_parquet(chunk_iterator=merged_chunk_iterator, output_parquet_path='data/merged_dataset.pq')\n",
    "\n",
    "# Clean up\n",
    "if 'healthy_iterator' in locals() or 'healthy_iterator' in globals():\n",
    "    del healthy_iterator\n",
    "if 'unhealthy_iterator' in locals() or 'unhealthy_iterator' in globals():\n",
    "    del unhealthy_iterator\n",
    "if 'aligned_pair_iterator' in locals() or 'aligned_pair_iterator' in globals():\n",
    "    del aligned_pair_iterator\n",
    "if 'merged_chunk_iterator' in locals() or 'merged_chunk_iterator' in globals():\n",
    "    del merged_chunk_iterator\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_iterator = load_parquet_in_chunks('data/merged_dataset.pq')\n",
    "\n",
    "if merged_iterator:\n",
    "    first_chunk = next(merged_iterator)\n",
    "\n",
    "    # Basic info\n",
    "    print(f\"First chunk shape: {first_chunk.shape}\")\n",
    "    print(f\"Columns: {list(first_chunk.columns)}\")\n",
    "    print(f\"Data types:\\n{first_chunk.dtypes}\")\n",
    "    print(f\"Memory usage: {first_chunk.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(first_chunk.head())\n",
    "\n",
    "# Clean up\n",
    "if 'merged_iterator' in locals() or 'merged_iterator' in globals():\n",
    "    del merged_iterator\n",
    "if 'first_chunk' in locals() or 'first_chunk' in globals():\n",
    "    del first_chunk\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_iterator = load_parquet_in_chunks('data/merged_metadata.pq')\n",
    "\n",
    "if merged_iterator:\n",
    "    first_chunk = next(merged_iterator)\n",
    "\n",
    "    # Basic info\n",
    "    print(f\"First chunk shape: {first_chunk.shape}\")\n",
    "    print(f\"Columns: {list(first_chunk.columns)}\")\n",
    "    print(f\"Data types:\\n{first_chunk.dtypes}\")\n",
    "    print(f\"Memory usage: {first_chunk.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(first_chunk.head())\n",
    "\n",
    "# Clean up\n",
    "if 'merged_iterator' in locals() or 'merged_iterator' in globals():\n",
    "    del merged_iterator\n",
    "if 'first_chunk' in locals() or 'first_chunk' in globals():\n",
    "    del first_chunk\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_iterator = load_parquet_in_chunks('data/healthy_data_preprocessed.pq')\n",
    "unhealthy_iterator = load_parquet_in_chunks('data/unhealthy_data_preprocessed.pq')\n",
    "metadata_iterator = load_parquet_in_chunks('data/merged_metadata.pq')\n",
    "\n",
    "check_metadata(healthy_generator=healthy_iterator, unhealthy_generator=unhealthy_iterator, merged_metadata_generator=metadata_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Simple PCA plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_parquet(\"data/merged_dataset.pq\")\n",
    "metadata = pd.read_parquet(\"data/merged_metadata.pq\")\n",
    "\n",
    "# Set sample_id as index for metadata to match with df\n",
    "metadata = metadata.set_index('sample_id')\n",
    "\n",
    "# Align metadata with df samples\n",
    "y = metadata.loc[df.index, \"condition\"]\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Samples: {df.shape[0]}, Genes: {df.shape[1]}\")\n",
    "print(f\"Condition distribution: {y.value_counts()}\")\n",
    "\n",
    "# Calculate mean differences between conditions\n",
    "gene_columns = df.columns\n",
    "mean_healthy = df[y == 0][gene_columns].mean()  # Condition 0 = healthy\n",
    "mean_unhealthy = df[y == 1][gene_columns].mean()  # Condition 1 = unhealthy\n",
    "mean_diff = (mean_unhealthy - mean_healthy).abs()\n",
    "\n",
    "print(\"\\nHead of Mean Differences (for top 5 genes):\")\n",
    "print(mean_diff.head())\n",
    "\n",
    "# Select top k genes by mean difference\n",
    "k_genes = 50_000\n",
    "top_k_genes = mean_diff.nlargest(k_genes).index\n",
    "x_selected = df[top_k_genes]\n",
    "\n",
    "print(f\"\\nOriginal number of genes: {df.shape[1]}\")\n",
    "print(f\"Number of genes after selection (top {k_genes} by mean difference): {x_selected.shape[1]}\")\n",
    "\n",
    "# Scale the data\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x_selected)\n",
    "\n",
    "print(\"Shape of x_scaled:\", x_scaled.shape)\n",
    "\n",
    "# Perform PCA\n",
    "pca = sklearn.decomposition.PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(x_scaled)\n",
    "\n",
    "print(\"Shape of principal components:\", pca_result.shape)\n",
    "\n",
    "# Create PCA DataFrame\n",
    "pca_df = pd.DataFrame(\n",
    "    pca_result,\n",
    "    columns=[\"PC1\", \"PC2\"],\n",
    "    index=x_selected.index\n",
    ")\n",
    "pca_df[\"condition\"] = y\n",
    "\n",
    "print(f\"Shape of PCA DataFrame: {pca_df.shape}\")\n",
    "print(\"\\nExplained Variance Ratio:\")\n",
    "print(f\"PC1: {pca.explained_variance_ratio_[0]:.4f}\")\n",
    "print(f\"PC2: {pca.explained_variance_ratio_[1]:.4f}\")\n",
    "print(f\"Total Explained Variance (PC1 + PC2): {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Generate PCA plot\n",
    "print(\"Generating PCA plot...\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create condition labels for better visualization\n",
    "condition_labels = {0: 'Healthy', 1: 'Unhealthy'}\n",
    "pca_df['condition_label'] = pca_df['condition'].map(condition_labels)\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=pca_df,\n",
    "    x=\"PC1\",\n",
    "    y=\"PC2\",\n",
    "    hue=\"condition_label\",\n",
    "    palette=['#2E8B57', '#DC143C'],  # Green for healthy, red for unhealthy\n",
    "    alpha=0.7,\n",
    "    s=50\n",
    ")\n",
    "\n",
    "plt.title(f'PCA of Gene Expression Data (Top {k_genes} Most Discriminative Genes)')\n",
    "plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]*100:.2f}% Variance Explained)')\n",
    "plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]*100:.2f}% Variance Explained)')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(title='Condition')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some summary statistics\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(f\"Healthy samples (condition 0): {(y == 0).sum()}\")\n",
    "print(f\"Unhealthy samples (condition 1): {(y == 1).sum()}\")\n",
    "print(f\"Total samples: {len(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pyarrow.parquet as pq # Import for your generator function\n",
    "import gc # For garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGED_DATA_PATH = \"data/merged_dataset.pq\"\n",
    "\n",
    "print(\"Starting log transformation and sampling for plots...\")\n",
    "\n",
    "raw_sample_data = None\n",
    "transformed_sample_data = None\n",
    "\n",
    "for i, chunk in enumerate(load_parquet_in_chunks(MERGED_DATA_PATH, chunk_size=8000)):\n",
    "    print(f\"Processing chunk {i+1}...\")\n",
    "    if raw_sample_data is None:\n",
    "        raw_sample_data = chunk.values.flatten()\n",
    "        if len(raw_sample_data) > 1_000_000:\n",
    "             raw_sample_data = np.random.choice(raw_sample_data, size=1_000_000, replace=False)\n",
    "        print(f\"Captured {len(raw_sample_data)} raw expression values for plotting.\")\n",
    "\n",
    "    transformed_chunk = np.log2(chunk + 1)\n",
    "\n",
    "    if transformed_sample_data is None:\n",
    "        transformed_sample_data = transformed_chunk.values.flatten()\n",
    "        if len(transformed_sample_data) > 1_000_000:\n",
    "             transformed_sample_data = np.random.choice(transformed_sample_data, size=1_000_000, replace=False)\n",
    "        print(f\"Captured {len(transformed_sample_data)} transformed expression values for plotting.\")\n",
    "\n",
    "    if raw_sample_data is not None and transformed_sample_data is not None:\n",
    "        break\n",
    "\n",
    "print(\"\\nLog transformation and sampling complete. Generating plots...\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(raw_sample_data, bins=50, kde=True, color='skyblue', edgecolor='black', stat='density', log_scale=True)\n",
    "plt.title('Distribution of Raw TPM Values (Sampled Data)')\n",
    "plt.xlabel('Raw TPM Value (log scale)')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, which=\"both\", ls=\"--\", c='0.7')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(transformed_sample_data, bins=50, kde=True, color='lightcoral', edgecolor='black', stat='density')\n",
    "plt.title('Distribution of Log2(TPM + 1) Values (Sampled Data)')\n",
    "plt.xlabel('Log2(TPM + 1) Value')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, which=\"both\", ls=\"--\", c='0.7')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlots generated. Review them to confirm the transformation's effect.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# Batch correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from combat.pycombat import pycombat\n",
    "import pyarrow.parquet as pq\n",
    "import gc\n",
    "from inmoose.pycombat import pycombat_norm\n",
    "from inmoose.cohort_qc.cohort_metric import CohortMetric\n",
    "from inmoose.cohort_qc.qc_report import QCReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_log_transformed = \"data/merged_dataset_log_transformed.pq\"\n",
    "merged_metadata = \"data/merged_metadata.pq\"\n",
    "output_path = \"data/merged_dataset_batch_corrected.pq\"\n",
    "\n",
    "df = pd.read_parquet(data_log_transformed)\n",
    "\n",
    "metadata = pd.read_parquet(merged_metadata)\n",
    "metadata = metadata.set_index('sample_id')\n",
    "batches_series = metadata['batch']\n",
    "\n",
    "common_samples = df.index.intersection(batches_series.index)\n",
    "if len(common_samples) == 0:\n",
    "    print(\"Error: No common samples across data and metadata. Check indices.\")\n",
    "    exit()\n",
    "\n",
    "df_aligned = df.loc[common_samples]\n",
    "batches_aligned = batches_series.loc[common_samples]\n",
    "metadata_aligned = metadata.loc[common_samples]\n",
    "\n",
    "print(f\"Aligned data for ComBat. Data shape: {df_aligned.shape}, Batches series length: {len(batches_aligned)}\")\n",
    "print(f\"Condition distribution:\\n{metadata_aligned['condition'].value_counts()}\")\n",
    "print(\"Warning: Running ComBat without covariates due to perfect confounding\")\n",
    "\n",
    "print(\"Transposing data for ComBat (Genes x Samples)...\")\n",
    "data_for_combat = df_aligned.T\n",
    "print(f\"Data for ComBat shape: {data_for_combat.shape}\")\n",
    "\n",
    "# Apply batch correction without covariates\n",
    "corrected_data_df_t = pycombat_norm(\n",
    "    data_for_combat,\n",
    "    batches_aligned\n",
    ")\n",
    "print(\"Batch correction without covariates complete.\")\n",
    "\n",
    "corrected_data_df = corrected_data_df_t.T\n",
    "print(f\"Corrected data transposed back to Samples x Genes. Shape: {corrected_data_df.shape}\")\n",
    "\n",
    "corrected_data_df.to_parquet(output_path, index=True)\n",
    "print(\"✅ Batch-corrected dataset saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_batch_correction_with_inmoose(original_data_path, corrected_data_path, metadata_path):\n",
    "    \"\"\"\n",
    "    Validate batch correction using inmoose's built-in QC tools.\n",
    "    \"\"\"\n",
    "    print(\"Loading data for validation...\")\n",
    "\n",
    "    # Load original and corrected data\n",
    "    original_df = pd.read_parquet(original_data_path)\n",
    "    corrected_df = pd.read_parquet(corrected_data_path)\n",
    "    metadata_df = pd.read_parquet(metadata_path).set_index('sample_id')\n",
    "\n",
    "    # Align everything\n",
    "    common_samples = original_df.index.intersection(corrected_df.index).intersection(metadata_df.index)\n",
    "    original_aligned = original_df.loc[common_samples]\n",
    "    corrected_aligned = corrected_df.loc[common_samples]\n",
    "    metadata_aligned = metadata_df.loc[common_samples]\n",
    "\n",
    "    print(f\"Validation data shape: {original_aligned.shape}\")\n",
    "\n",
    "    # Transpose for inmoose (genes x samples)\n",
    "    original_t = original_aligned.T\n",
    "    corrected_t = corrected_aligned.T\n",
    "    batches = metadata_aligned['batch']\n",
    "\n",
    "    print(\"Calculating metrics for original data...\")\n",
    "    # Calculate metrics for original data\n",
    "    original_metrics = CohortMetric(\n",
    "        counts=original_t,\n",
    "        batch=batches,\n",
    "        covar_mod=None\n",
    "    )\n",
    "\n",
    "    print(\"Calculating metrics for corrected data...\")\n",
    "    # Calculate metrics for corrected data\n",
    "    corrected_metrics = CohortMetric(\n",
    "        counts=corrected_t,\n",
    "        batch=batches,\n",
    "        covar_mod=None\n",
    "    )\n",
    "\n",
    "    print(\"Generating QC Report...\")\n",
    "    # Generate QC report\n",
    "    qc_report = QCReport(\n",
    "        original_metrics,\n",
    "        corrected_metrics,\n",
    "        batch=batches\n",
    "    )\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"BATCH CORRECTION VALIDATION SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    print(f\"Original data batch effect metrics:\")\n",
    "    print(f\"  Silhouette score: {original_metrics.silhouette_score:.4f}\")\n",
    "    print(f\"  PC variance explained by batch: {original_metrics.pc_variance:.4f}\")\n",
    "\n",
    "    print(f\"\\nCorrected data batch effect metrics:\")\n",
    "    print(f\"  Silhouette score: {corrected_metrics.silhouette_score:.4f}\")\n",
    "    print(f\"  PC variance explained by batch: {corrected_metrics.pc_variance:.4f}\")\n",
    "\n",
    "    # Lower scores indicate better batch correction\n",
    "    silhouette_improvement = original_metrics.silhouette_score - corrected_metrics.silhouette_score\n",
    "    pc_improvement = original_metrics.pc_variance - corrected_metrics.pc_variance\n",
    "\n",
    "    print(f\"\\nImprovement (lower is better for batch effects):\")\n",
    "    print(f\"  Silhouette improvement: {silhouette_improvement:.4f}\")\n",
    "    print(f\"  PC variance improvement: {pc_improvement:.4f}\")\n",
    "\n",
    "    return qc_report, original_metrics, corrected_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_batch_correction_plots(original_data_path, corrected_data_path, metadata_path):\n",
    "    \"\"\"\n",
    "    Create comprehensive plots to validate batch correction.\n",
    "    \"\"\"\n",
    "    print(\"Loading data for plotting...\")\n",
    "\n",
    "    # Load data\n",
    "    original_df = pd.read_parquet(original_data_path)\n",
    "    corrected_df = pd.read_parquet(corrected_data_path)\n",
    "    metadata_df = pd.read_parquet(metadata_path).set_index('sample_id')\n",
    "\n",
    "    # Align data\n",
    "    common_samples = original_df.index.intersection(corrected_df.index).intersection(metadata_df.index)\n",
    "    original_aligned = original_df.loc[common_samples]\n",
    "    corrected_aligned = corrected_df.loc[common_samples]\n",
    "    metadata_aligned = metadata_df.loc[common_samples]\n",
    "\n",
    "    print(f\"Plot data shape: {original_aligned.shape}\")\n",
    "\n",
    "    # Select top variable genes for PCA (to speed up computation)\n",
    "    print(\"Selecting top 5000 most variable genes...\")\n",
    "    gene_vars = original_aligned.var(axis=0)\n",
    "    top_genes = gene_vars.nlargest(5000).index\n",
    "\n",
    "    original_subset = original_aligned[top_genes]\n",
    "    corrected_subset = corrected_aligned[top_genes]\n",
    "\n",
    "    # Standardize and perform PCA\n",
    "    print(\"Performing PCA...\")\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Original data PCA\n",
    "    original_scaled = scaler.fit_transform(original_subset)\n",
    "    pca_orig = PCA(n_components=2)\n",
    "    original_pca = pca_orig.fit_transform(original_scaled)\n",
    "\n",
    "    # Corrected data PCA\n",
    "    corrected_scaled = scaler.fit_transform(corrected_subset)\n",
    "    pca_corr = PCA(n_components=2)\n",
    "    corrected_pca = pca_corr.fit_transform(corrected_scaled)\n",
    "\n",
    "    # Create plotting dataframes\n",
    "    plot_df_orig = pd.DataFrame({\n",
    "        'PC1': original_pca[:, 0],\n",
    "        'PC2': original_pca[:, 1],\n",
    "        'batch': metadata_aligned['batch'],\n",
    "        'condition': metadata_aligned['condition'].map({0: 'Healthy', 1: 'Unhealthy'}),\n",
    "        'dataset': 'Original'\n",
    "    })\n",
    "\n",
    "    plot_df_corr = pd.DataFrame({\n",
    "        'PC1': corrected_pca[:, 0],\n",
    "        'PC2': corrected_pca[:, 1],\n",
    "        'batch': metadata_aligned['batch'],\n",
    "        'condition': metadata_aligned['condition'].map({0: 'Healthy', 1: 'Unhealthy'}),\n",
    "        'dataset': 'Batch Corrected'\n",
    "    })\n",
    "\n",
    "    # Create plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Batch Correction Validation', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Plot 1: Original data colored by batch\n",
    "    sns.scatterplot(data=plot_df_orig, x='PC1', y='PC2', hue='batch',\n",
    "                   alpha=0.7, s=50, ax=axes[0,0])\n",
    "    axes[0,0].set_title(f'Original Data - Colored by Batch\\nPC1: {pca_orig.explained_variance_ratio_[0]:.2%}, PC2: {pca_orig.explained_variance_ratio_[1]:.2%}')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Corrected data colored by batch\n",
    "    sns.scatterplot(data=plot_df_corr, x='PC1', y='PC2', hue='batch',\n",
    "                   alpha=0.7, s=50, ax=axes[0,1])\n",
    "    axes[0,1].set_title(f'Batch Corrected Data - Colored by Batch\\nPC1: {pca_corr.explained_variance_ratio_[0]:.2%}, PC2: {pca_corr.explained_variance_ratio_[1]:.2%}')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Original data colored by condition\n",
    "    sns.scatterplot(data=plot_df_orig, x='PC1', y='PC2', hue='condition',\n",
    "                   palette=['#2E8B57', '#DC143C'], alpha=0.7, s=50, ax=axes[1,0])\n",
    "    axes[1,0].set_title('Original Data - Colored by Condition')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: Corrected data colored by condition\n",
    "    sns.scatterplot(data=plot_df_corr, x='PC1', y='PC2', hue='condition',\n",
    "                   palette=['#2E8B57', '#DC143C'], alpha=0.7, s=50, ax=axes[1,1])\n",
    "    axes[1,1].set_title('Batch Corrected Data - Colored by Condition')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"VISUAL VALIDATION SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Expected results after good batch correction:\")\n",
    "    print(\"• Top row: Batch separation should be REDUCED\")\n",
    "    print(\"• Bottom row: Condition separation should be PRESERVED\")\n",
    "    print(\"\\nIf batch effects were successfully removed:\")\n",
    "    print(\"• GTEx and GDC samples should mix better in corrected data\")\n",
    "    print(\"• Healthy vs Unhealthy separation should remain clear\")\n",
    "\n",
    "    return plot_df_orig, plot_df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run both validation approaches\n",
    "print(\"=\"*60)\n",
    "print(\"COMPREHENSIVE BATCH CORRECTION VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Quantitative validation with inmoose\n",
    "print(\"\\n1. QUANTITATIVE METRICS (inmoose)\")\n",
    "print(\"-\" * 40)\n",
    "# qc_report, orig_metrics, corr_metrics = validate_batch_correction_with_inmoose(\n",
    "#     original_data_path=\"data/merged_dataset_log_transformed.pq\",\n",
    "#     corrected_data_path=\"data/merged_dataset_batch_corrected.pq\",\n",
    "#     metadata_path=\"data/merged_metadata.pq\"\n",
    "# )\n",
    "\n",
    "print(\"\\n2. VISUAL VALIDATION (Custom Plots)\")\n",
    "print(\"-\" * 40)\n",
    "orig_plot_df, corr_plot_df = create_batch_correction_plots(\n",
    "    original_data_path=\"data/merged_dataset_log_transformed.pq\",\n",
    "    corrected_data_path=\"data/merged_dataset_batch_corrected.pq\",\n",
    "    metadata_path=\"data/merged_metadata.pq\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
