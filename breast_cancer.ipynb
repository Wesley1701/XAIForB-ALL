{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Base functions and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Optional, Union\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import mygene\n",
    "import os\n",
    "import gc\n",
    "from IPython.display import display\n",
    "\n",
    "def load_csv_in_chunks(file_path, chunk_size=8000, **kwargs):\n",
    "    \"\"\"\n",
    "    Loads a CSV file in chunks to avoid memory issues.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "        chunk_size (int): The number of rows per chunk.\n",
    "        **kwargs: Additional keyword arguments to pass to pd.read_csv()\n",
    "                  (e.g., sep=',', header=0, index_col=None, usecols=None).\n",
    "\n",
    "    Returns:\n",
    "        A pandas TextFileReader object (iterator) that yields DataFrame chunks\n",
    "        if the file exists, otherwise None.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Preparing to load {file_path} in chunks of size {chunk_size}...\")\n",
    "    try:\n",
    "        chunk_iterator = pd.read_csv(file_path, chunksize=chunk_size, **kwargs)\n",
    "        return chunk_iterator\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Clean up\n",
    "    finally:\n",
    "        if 'chunk_iterator' in locals():\n",
    "            del chunk_iterator\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def load_parquet_in_chunks(file_path, chunk_size=8000):\n",
    "    \"\"\"\n",
    "    Loads a parquet file in true chunks to avoid memory issues.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the parquet file.\n",
    "        chunk_size (int): The number of rows per chunk.\n",
    "\n",
    "    Yields:\n",
    "        pandas.DataFrame: DataFrame chunks of the specified size.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Preparing to load {file_path} in chunks of size {chunk_size}...\")\n",
    "\n",
    "    try:\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        total_rows = parquet_file.metadata.num_rows\n",
    "        print(f\"Total rows in file: {total_rows}\")\n",
    "\n",
    "        for batch in parquet_file.iter_batches(batch_size=chunk_size):\n",
    "            chunk_df = batch.to_pandas()\n",
    "            yield chunk_df\n",
    "\n",
    "            del chunk_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading parquet file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Clean up\n",
    "    finally:\n",
    "        if 'parquet_file' in locals():\n",
    "            del parquet_file\n",
    "        if 'batch' in locals():\n",
    "            del batch\n",
    "        if 'chunk_df' in locals():\n",
    "            del chunk_df\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def save_data_as_parquet(chunk_iterator, output_parquet_path, preserve_index=True):\n",
    "    \"\"\"\n",
    "    Process DataFrame chunks and save as parquet using PyArrow for maximum efficiency.\n",
    "    Handles index preservation properly across chunks.\n",
    "\n",
    "    Args:\n",
    "        chunk_iterator: Iterator yielding DataFrame chunks\n",
    "        output_parquet_path (str): Path to save the parquet file\n",
    "        preserve_index (bool): Whether to preserve the DataFrame index\n",
    "    \"\"\"\n",
    "\n",
    "    writer = None\n",
    "    total_rows = 0\n",
    "    all_index_values = set()\n",
    "\n",
    "    try:\n",
    "        for chunk_idx, chunk in enumerate(chunk_iterator):\n",
    "            if chunk.empty:\n",
    "                continue\n",
    "\n",
    "            if preserve_index and chunk.index.name is not None:\n",
    "                chunk_index_values = set(chunk.index)\n",
    "                duplicates = all_index_values.intersection(chunk_index_values)\n",
    "                if duplicates:\n",
    "                    print(f\"Warning: Found {len(duplicates)} duplicate index values in chunk {chunk_idx}\")\n",
    "                    print(f\"First few duplicates: {list(duplicates)[:5]}\")\n",
    "                all_index_values.update(chunk_index_values)\n",
    "\n",
    "            table = pa.Table.from_pandas(chunk, preserve_index=preserve_index)\n",
    "\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(output_parquet_path, table.schema)\n",
    "                if preserve_index and chunk.index.name:\n",
    "                    print(f\"Preserving index: '{chunk.index.name}' (dtype: {chunk.index.dtype})\")\n",
    "\n",
    "            writer.write_table(table)\n",
    "            total_rows += chunk.shape[0]\n",
    "\n",
    "            del table\n",
    "\n",
    "    finally:\n",
    "        if writer:\n",
    "            writer.close()\n",
    "\n",
    "        # Clean up\n",
    "        if 'writer' in locals():\n",
    "            del writer\n",
    "        if 'chunk_iterator' in locals():\n",
    "            del chunk_iterator\n",
    "        gc.collect()\n",
    "\n",
    "    if total_rows > 0:\n",
    "        print(f\"Successfully saved {total_rows} rows to parquet\")\n",
    "        if preserve_index:\n",
    "            print(f\"Total unique index values: {len(all_index_values)}\")\n",
    "    else:\n",
    "        print(\"No data to save!\")\n",
    "\n",
    "\n",
    "def drop_dataframe_chunks(\n",
    "    chunk_generator: Iterator[pd.DataFrame],\n",
    "    drop_rows: Optional[Union[List[Union[int, str]], List[int], List[str]]] = None,\n",
    "    drop_columns: Optional[Union[List[Union[int, str]], List[int], List[str]]] = None\n",
    ") -> Iterator[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generator function that drops specified rows and/or columns from DataFrame chunks\n",
    "    in a memory-efficient way. Designed to be chained with other generators.\n",
    "\n",
    "    Args:\n",
    "        chunk_generator: An iterable that yields pandas DataFrame chunks\n",
    "        drop_rows: List of row indices/names to drop, or None to keep all rows\n",
    "        drop_columns: List of column indices/names to drop, or None to keep all columns\n",
    "\n",
    "    Yields:\n",
    "        pd.DataFrame: Chunks with specified rows/columns dropped\n",
    "    \"\"\"\n",
    "    for chunk in chunk_generator:\n",
    "        if chunk.empty:\n",
    "            yield chunk\n",
    "            continue\n",
    "\n",
    "        processed_chunk = chunk.copy()\n",
    "\n",
    "        if drop_columns is not None:\n",
    "            columns_to_drop = [col for col in drop_columns if col in processed_chunk.columns]\n",
    "            if columns_to_drop:\n",
    "                processed_chunk = processed_chunk.drop(columns=columns_to_drop)\n",
    "\n",
    "        if drop_rows is not None:\n",
    "            if drop_rows and not all(isinstance(row, int) for row in drop_rows):\n",
    "                chunk_index_str = processed_chunk.index.astype(str)\n",
    "                drop_rows_str = [str(row) for row in drop_rows]\n",
    "\n",
    "                rows_to_drop = processed_chunk.index[chunk_index_str.isin(drop_rows_str)]\n",
    "                if len(rows_to_drop) > 0:\n",
    "                    processed_chunk = processed_chunk.drop(index=rows_to_drop)\n",
    "            else:\n",
    "                valid_indices = [idx for idx in drop_rows if 0 <= idx < len(processed_chunk)]\n",
    "                if valid_indices:\n",
    "                    processed_chunk = processed_chunk.drop(processed_chunk.index[valid_indices])\n",
    "\n",
    "        if not processed_chunk.empty:\n",
    "            yield processed_chunk\n",
    "\n",
    "\n",
    "def keep_dataframe_chunks(\n",
    "    chunk_generator: Iterator[pd.DataFrame],\n",
    "    keep_rows: Optional[Union[List[Union[int, str]], List[int], List[str]]] = None,\n",
    "    keep_columns: Optional[Union[List[Union[int, str]], List[int], List[str]]] = None\n",
    ") -> Iterator[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generator function that keeps only specified rows and/or columns from DataFrame chunks\n",
    "    in a memory-efficient way. Designed to be chained with other generators.\n",
    "\n",
    "    Args:\n",
    "        chunk_generator: An iterable that yields pandas DataFrame chunks\n",
    "        keep_rows: List of row indices/names to keep, or None to keep all rows\n",
    "        keep_columns: List of column indices/names to keep, or None to keep all columns\n",
    "\n",
    "    Yields:\n",
    "        pd.DataFrame: Chunks with only specified rows/columns kept\n",
    "    \"\"\"\n",
    "    for chunk in chunk_generator:\n",
    "        if chunk.empty:\n",
    "            yield chunk\n",
    "            continue\n",
    "\n",
    "        processed_chunk = chunk.copy()\n",
    "\n",
    "        if keep_columns is not None:\n",
    "            columns_to_keep = [col for col in keep_columns if col in processed_chunk.columns]\n",
    "            if columns_to_keep:\n",
    "                processed_chunk = processed_chunk[columns_to_keep]\n",
    "            else:\n",
    "                processed_chunk = processed_chunk.iloc[:0]\n",
    "\n",
    "        if keep_rows is not None:\n",
    "            if keep_rows and not all(isinstance(row, int) for row in keep_rows):\n",
    "                chunk_index_str = processed_chunk.index.astype(str)\n",
    "                keep_rows_str = [str(row) for row in keep_rows]\n",
    "\n",
    "                rows_to_keep = processed_chunk.index[chunk_index_str.isin(keep_rows_str)]\n",
    "                if len(rows_to_keep) > 0:\n",
    "                    processed_chunk = processed_chunk.loc[rows_to_keep]\n",
    "                else:\n",
    "                    processed_chunk = processed_chunk.iloc[:0]\n",
    "            else:\n",
    "                valid_indices = [idx for idx in keep_rows if 0 <= idx < len(processed_chunk)]\n",
    "                if valid_indices:\n",
    "                    processed_chunk = processed_chunk.iloc[valid_indices]\n",
    "                else:\n",
    "                    processed_chunk = processed_chunk.iloc[:0]\n",
    "\n",
    "        if not processed_chunk.empty:\n",
    "            yield processed_chunk\n",
    "\n",
    "\n",
    "def transpose_dataframe_chunks(\n",
    "    chunk_generator,\n",
    "    skip_rows=None,\n",
    "    skip_columns=None,\n",
    "    output_batch_size=1000,\n",
    "    temp_dir=\"/tmp\",\n",
    "    dtype='uint32'\n",
    "):\n",
    "    \"\"\"\n",
    "    Generator function that collects DataFrame chunks, transposes the complete dataset,\n",
    "    and yields the transposed result in batches. Designed to be chained with other generators.\n",
    "\n",
    "    Args:\n",
    "        chunk_generator: An iterable that yields pandas DataFrame chunks\n",
    "        skip_rows: List of row indices/names to skip, or None\n",
    "        skip_columns: List of column indices/names to skip, or None\n",
    "        output_batch_size: Number of rows to yield in each output batch\n",
    "        temp_dir: Directory for temporary memory-mapped file\n",
    "        dtype: Data type for memory-mapped array (default: 'uint32')\n",
    "\n",
    "    Yields:\n",
    "        pd.DataFrame: Batches of the transposed DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    chunk_list = list(chunk_generator)\n",
    "    if not chunk_list:\n",
    "        print(\"No chunks received from generator. Exiting.\")\n",
    "        return\n",
    "\n",
    "    first_chunk = chunk_list[0].copy()\n",
    "\n",
    "    if skip_columns is not None:\n",
    "        columns_to_keep = [col for col in first_chunk.columns if col not in skip_columns]\n",
    "        first_chunk = first_chunk[columns_to_keep]\n",
    "        print(f\"Skipping columns: {skip_columns}\")\n",
    "\n",
    "    if skip_rows is not None:\n",
    "        if isinstance(skip_rows[0], str):\n",
    "            rows_to_keep = [idx for idx in first_chunk.index if idx not in skip_rows]\n",
    "        else:\n",
    "            rows_to_keep = [idx for i, idx in enumerate(first_chunk.index) if i not in skip_rows]\n",
    "        first_chunk = first_chunk.loc[rows_to_keep]\n",
    "        print(f\"Skipping rows: {skip_rows}\")\n",
    "\n",
    "    original_rows = sum(len(chunk) for chunk in chunk_list)\n",
    "    original_cols = len(first_chunk.columns)\n",
    "\n",
    "    filtered_cols = first_chunk.columns.tolist()\n",
    "    n_output_rows = len(filtered_cols)\n",
    "\n",
    "    all_row_indices = []\n",
    "    for chunk in chunk_list:\n",
    "        chunk_filtered = chunk.copy()\n",
    "\n",
    "        if skip_columns is not None:\n",
    "            chunk_filtered = chunk_filtered[filtered_cols]\n",
    "        if skip_rows is not None:\n",
    "            if isinstance(skip_rows[0], str):\n",
    "                chunk_rows_to_keep = [idx for idx in chunk_filtered.index if idx not in skip_rows]\n",
    "            else:\n",
    "                chunk_rows_to_keep = [idx for i, idx in enumerate(chunk_filtered.index) if i not in skip_rows]\n",
    "            chunk_filtered = chunk_filtered.loc[chunk_rows_to_keep]\n",
    "\n",
    "        all_row_indices.extend(chunk_filtered.index.tolist())\n",
    "\n",
    "    n_output_cols = len(all_row_indices)\n",
    "\n",
    "    print(f\"Dataset dimensions:\")\n",
    "    print(f\"    Original: {original_rows} rows x {original_cols} columns\")\n",
    "    print(f\"    After filtering: {len(all_row_indices)} rows x {len(filtered_cols)} columns\")\n",
    "    print(f\"    After transpose: {n_output_rows} rows x {n_output_cols} columns\")\n",
    "\n",
    "    # Create memory-mapped file\n",
    "    try:\n",
    "        temp_file = tempfile.NamedTemporaryFile(\n",
    "            dir=temp_dir,\n",
    "            delete=False,\n",
    "            suffix='.mmap'\n",
    "        )\n",
    "        temp_filename = temp_file.name\n",
    "        temp_file.close()\n",
    "\n",
    "        # Create memory-mapped array in transposed orientation: [samples, features]\n",
    "        mmap_array = np.memmap(\n",
    "            temp_filename,\n",
    "            dtype=dtype,\n",
    "            mode='w+',\n",
    "            shape=(n_output_rows, n_output_cols)\n",
    "        )\n",
    "        print(f\"Memory map created successfully\")\n",
    "\n",
    "        print(f\"Filling memory map with data...\")\n",
    "        current_feature_idx = 0\n",
    "\n",
    "        for chunk_idx, chunk in enumerate(chunk_list):\n",
    "            chunk_filtered = chunk.copy()\n",
    "            if skip_columns is not None:\n",
    "                chunk_filtered = chunk_filtered[filtered_cols]\n",
    "            if skip_rows is not None:\n",
    "                if isinstance(skip_rows[0], str):\n",
    "                    chunk_rows_to_keep = [idx for idx in chunk_filtered.index if idx not in skip_rows]\n",
    "                else:\n",
    "                    chunk_rows_to_keep = [idx for i, idx in enumerate(chunk_filtered.index) if i not in skip_rows]\n",
    "                chunk_filtered = chunk_filtered.loc[chunk_rows_to_keep]\n",
    "\n",
    "            chunk_data = chunk_filtered.values.T.astype(dtype)\n",
    "            chunk_feature_count = chunk_filtered.shape[0]\n",
    "\n",
    "            mmap_array[:, current_feature_idx:current_feature_idx + chunk_feature_count] = chunk_data\n",
    "            current_feature_idx += chunk_feature_count\n",
    "\n",
    "            mmap_array.flush()\n",
    "            del chunk_data, chunk_filtered\n",
    "\n",
    "            print(f\"    Processed chunk {chunk_idx + 1}/{len(chunk_list)}\")\n",
    "\n",
    "        del chunk_list\n",
    "        gc.collect()\n",
    "\n",
    "        print(f\"Memory map filled successfully\")\n",
    "\n",
    "        print(f\"Yielding transposed batches (size: {output_batch_size})...\")\n",
    "        total_batches = (n_output_rows + output_batch_size - 1) // output_batch_size\n",
    "\n",
    "        for batch_idx in range(total_batches):\n",
    "            start_row = batch_idx * output_batch_size\n",
    "            end_row = min(start_row + output_batch_size, n_output_rows)\n",
    "\n",
    "            batch_data = mmap_array[start_row:end_row, :].copy()\n",
    "            batch_sample_ids = filtered_cols[start_row:end_row]\n",
    "\n",
    "            batch_df = pd.DataFrame(\n",
    "                data=batch_data,\n",
    "                index=batch_sample_ids,\n",
    "                columns=all_row_indices\n",
    "            )\n",
    "\n",
    "            batch_df = batch_df.reset_index().rename(columns={'index': 'sample_id'})\n",
    "\n",
    "            yield batch_df\n",
    "\n",
    "            del batch_data, batch_df\n",
    "            gc.collect()\n",
    "\n",
    "            print(f\"    Yielded batch {batch_idx + 1}/{total_batches}\")\n",
    "\n",
    "        print(f\"Transposition completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during memory-mapped transposition: {e}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        print(f\"Cleaning up temporary files...\")\n",
    "        try:\n",
    "            if 'mmap_array' in locals():\n",
    "                del mmap_array\n",
    "            if 'temp_filename' in locals() and os.path.exists(temp_filename):\n",
    "                os.unlink(temp_filename)\n",
    "                print(f\"Temporary file removed: {temp_filename}\")\n",
    "        except Exception as cleanup_error:\n",
    "            print(f\"Warning: Could not clean up temp file: {cleanup_error}\")\n",
    "\n",
    "\n",
    "def convert_gene_ids_to_symbols(dataset_chunk_iterator, mygene_client, gene_column_prefix=\"ENSG\"):\n",
    "    \"\"\"\n",
    "    Convert gene IDs to gene symbols using MyGene API in a memory-efficient way.\n",
    "    Only calls MyGene API once for all gene columns from the first chunk.\n",
    "\n",
    "    Args:\n",
    "        dataset_chunk_iterator: Iterator yielding DataFrame chunks\n",
    "        mygene_client: MyGene client instance\n",
    "        gene_column_prefix (str): Prefix to identify gene columns\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Complete dataset with gene symbols as column names\n",
    "    \"\"\"\n",
    "    print(\"Converting gene IDs to symbols (chunked processing)...\")\n",
    "\n",
    "    # Get first chunk to determine gene columns and create mapping\n",
    "    try:\n",
    "        first_chunk = next(dataset_chunk_iterator)\n",
    "        print(f\"Processing first chunk: {first_chunk.shape}\")\n",
    "    except StopIteration:\n",
    "        print(\"❌ Error: Dataset is empty\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Identify gene columns from first chunk\n",
    "    gene_columns = [col for col in first_chunk.columns\n",
    "                   if isinstance(col, str) and col.startswith(gene_column_prefix)]\n",
    "\n",
    "    if not gene_columns:\n",
    "        print(\"No gene columns found - returning original data\")\n",
    "        # Concatenate all chunks and return\n",
    "        all_chunks = [first_chunk]\n",
    "        for chunk in dataset_chunk_iterator:\n",
    "            all_chunks.append(chunk)\n",
    "        return pd.concat(all_chunks, axis=0, ignore_index=False)\n",
    "\n",
    "    print(f\"Found {len(gene_columns)} gene columns\")\n",
    "\n",
    "    # Single MyGene API call for all gene IDs\n",
    "    gene_id_to_symbol = {}\n",
    "    symbols_found = 0\n",
    "\n",
    "    try:\n",
    "        print(\"Making single MyGene API call...\")\n",
    "        import warnings\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            results = mygene_client.querymany(\n",
    "                gene_columns,\n",
    "                scopes='ensembl.gene',\n",
    "                fields='symbol',\n",
    "                species='human',\n",
    "                verbose=False,\n",
    "                silent=True\n",
    "            )\n",
    "\n",
    "        # Process API results\n",
    "        for result in results:\n",
    "            gene_id = result['query']\n",
    "            if 'symbol' in result and result['symbol']:\n",
    "                gene_id_to_symbol[gene_id] = result['symbol']\n",
    "                symbols_found += 1\n",
    "            else:\n",
    "                gene_id_to_symbol[gene_id] = gene_id\n",
    "\n",
    "        print(f\"✅ Successfully converted {symbols_found}/{len(gene_columns)} genes to symbols\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ MyGene API error: {e}. Using original gene IDs\")\n",
    "        gene_id_to_symbol = {gene_id: gene_id for gene_id in gene_columns}\n",
    "\n",
    "    # Create final column mapping (preserve non-gene columns like 'condition')\n",
    "    final_column_mapping = {}\n",
    "    for col in first_chunk.columns:\n",
    "        if col == 'condition':\n",
    "            final_column_mapping[col] = col\n",
    "        elif col in gene_id_to_symbol:\n",
    "            final_column_mapping[col] = gene_id_to_symbol[col]\n",
    "        else:\n",
    "            final_column_mapping[col] = col\n",
    "\n",
    "    print(\"Applying gene symbol mapping to all chunks...\")\n",
    "\n",
    "    # Process first chunk\n",
    "    renamed_first_chunk = first_chunk.rename(columns=final_column_mapping)\n",
    "    processed_chunks = [renamed_first_chunk]\n",
    "    chunk_count = 1\n",
    "\n",
    "    # Process remaining chunks with same mapping\n",
    "    for chunk in dataset_chunk_iterator:\n",
    "        renamed_chunk = chunk.rename(columns=final_column_mapping)\n",
    "        processed_chunks.append(renamed_chunk)\n",
    "        chunk_count += 1\n",
    "\n",
    "        if chunk_count % 3 == 0:\n",
    "            print(f\"  ✓ Processed {chunk_count} chunks...\")\n",
    "\n",
    "    print(f\"Concatenating {chunk_count} processed chunks...\")\n",
    "    final_dataset = pd.concat(processed_chunks, axis=0, ignore_index=False)\n",
    "\n",
    "    print(f\"✅ Final dataset shape: {final_dataset.shape}\")\n",
    "    print(f\"   Symbols converted: {symbols_found}/{len(gene_columns)}\")\n",
    "\n",
    "    return final_dataset\n",
    "\n",
    "\n",
    "def add_condition_labels_to_chunks(chunk_iterator, condition_label, dataset_name):\n",
    "    \"\"\"\n",
    "    Add condition labels to dataset chunks.\n",
    "\n",
    "    Args:\n",
    "        chunk_iterator: Iterator yielding DataFrame chunks\n",
    "        condition_label (int): Binary label (0 for healthy, 1 for unhealthy)\n",
    "        dataset_name (str): Name for logging purposes\n",
    "\n",
    "    Returns:\n",
    "        list: List of labeled DataFrame chunks\n",
    "    \"\"\"\n",
    "    print(f\"Adding label '{condition_label}' to {dataset_name} dataset...\")\n",
    "\n",
    "    labeled_chunks = []\n",
    "    for chunk in chunk_iterator:\n",
    "        chunk['condition'] = condition_label\n",
    "        labeled_chunks.append(chunk)\n",
    "\n",
    "    print(f\"✅ Completed {len(labeled_chunks)} {dataset_name} chunks\")\n",
    "    return labeled_chunks\n",
    "\n",
    "\n",
    "def merge_labeled_datasets(healthy_chunks, unhealthy_chunks):\n",
    "    \"\"\"\n",
    "    Merge healthy and unhealthy dataset chunks into one DataFrame.\n",
    "\n",
    "    Args:\n",
    "        healthy_chunks (list): List of healthy DataFrame chunks\n",
    "        unhealthy_chunks (list): List of unhealthy DataFrame chunks\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Merged dataset\n",
    "    \"\"\"\n",
    "    print(\"Merging datasets...\")\n",
    "\n",
    "    all_chunks = healthy_chunks + unhealthy_chunks\n",
    "    merged_dataset = pd.concat(all_chunks, axis=0, ignore_index=False)\n",
    "\n",
    "    print(f\"✅ Merged dataset: {len(merged_dataset)} samples, {len(merged_dataset.columns)} features\")\n",
    "    print(f\"   Healthy (0): {(merged_dataset['condition'] == 0).sum()}\")\n",
    "    print(f\"   Unhealthy (1): {(merged_dataset['condition'] == 1).sum()}\")\n",
    "\n",
    "    return merged_dataset\n",
    "\n",
    "\n",
    "def clean_duplicate_nans(chunk_iterator):\n",
    "    \"\"\"\n",
    "    Processes an iterator of DataFrame chunks to drop duplicates and NaNs.\n",
    "    Yields cleaned DataFrame chunks one at a time.\n",
    "    \"\"\"\n",
    "\n",
    "    chunks_processed = 0\n",
    "    for i, chunk_df in enumerate(chunk_iterator):\n",
    "        original_rows = len(chunk_df)\n",
    "\n",
    "        chunk_df = chunk_df.dropna()\n",
    "\n",
    "        if len(chunk_df) < original_rows:\n",
    "            print(f\"    Chunk {i+1}: Dropped {original_rows - len(chunk_df)} rows with null values.\")\n",
    "\n",
    "        if chunk_df.empty:\n",
    "            print(f\"    Chunk {i+1}: Empty after dropping NaNs, skipping...\", flush=True)\n",
    "            continue\n",
    "\n",
    "        rows_before_dedup = len(chunk_df)\n",
    "        chunk_df = chunk_df.drop_duplicates()\n",
    "\n",
    "        if rows_before_dedup > len(chunk_df):\n",
    "            print(f\"    Chunk {i+1}: Dropped {rows_before_dedup - len(chunk_df)} duplicate rows.\")\n",
    "\n",
    "        if not chunk_df.empty:\n",
    "            chunks_processed += 1\n",
    "            print(f\"    Chunk {i+1}: Yielded chunk with shape {chunk_df.shape}\")\n",
    "            yield chunk_df\n",
    "        else:\n",
    "            print(f\"    Chunk {i+1}: Empty after deduplication, skipping...\")\n",
    "\n",
    "    print(f\"Finished processing. {chunks_processed} non-empty chunks processed.\")\n",
    "\n",
    "\n",
    "def rename_index(chunk_iterator, index_name):\n",
    "    \"\"\"\n",
    "    Rename the index of DataFrame chunks.\n",
    "    \"\"\"\n",
    "    for chunk in chunk_iterator:\n",
    "        chunk.index.name = index_name\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "def filter_rows(chunk_iterator):\n",
    "    \"\"\"\n",
    "    Generator function that filters out rows where the sum of all numeric values equals 0.\n",
    "    Excludes the sample_id column from sum calculation.\n",
    "    \"\"\"\n",
    "\n",
    "    total_rows_processed = 0\n",
    "    total_rows_removed = 0\n",
    "    chunk_count = 0\n",
    "\n",
    "    for chunk_df in chunk_iterator:\n",
    "        if chunk_df.empty:\n",
    "            continue\n",
    "\n",
    "        chunk_count += 1\n",
    "        original_rows = len(chunk_df)\n",
    "\n",
    "        numeric_columns = [col for col in chunk_df.columns if col != 'sample_id']\n",
    "\n",
    "        if len(numeric_columns) == 0:\n",
    "            print(f\"    Warning: Chunk {chunk_count} has no numeric columns, keeping all rows\")\n",
    "            yield chunk_df\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            row_sums = chunk_df[numeric_columns].sum(axis=1)\n",
    "            filtered_chunk = chunk_df[row_sums != 0]\n",
    "        except Exception as e:\n",
    "            print(f\"    Error in chunk {chunk_count}: {e}\")\n",
    "            print(f\"    Column types: {chunk_df.dtypes}\")\n",
    "            raise\n",
    "\n",
    "        rows_removed = original_rows - len(filtered_chunk)\n",
    "        total_rows_processed += original_rows\n",
    "        total_rows_removed += rows_removed\n",
    "\n",
    "        if rows_removed > 0:\n",
    "            print(f\"    Chunk {chunk_count}: Removed {rows_removed}/{original_rows} rows with zero sum\")\n",
    "\n",
    "        yield filtered_chunk\n",
    "\n",
    "    print(f\"Filtering complete: {total_rows_removed}/{total_rows_processed} rows removed\")\n",
    "\n",
    "\n",
    "def get_healthy_whole_blood_samples(metadata_path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Get healthy whole blood samples from the metadata.\n",
    "\n",
    "    Args:\n",
    "        metadata_path (str): Path to the metadata CSV file\n",
    "        gtex_blood_ids (list[str]): List of GTEx whole blood sample IDs\n",
    "\n",
    "    Returns:\n",
    "        list[str]: List of healthy whole blood sample IDs\n",
    "    \"\"\"\n",
    "    metadata_df = pd.read_csv(metadata_path, index_col=0)\n",
    "\n",
    "    # Get all samples with 'Whole Blood' SMTSD\n",
    "    whole_blood_samples = metadata_df[metadata_df['SMTSD'] == 'Whole Blood']\n",
    "    # Filter on RNA integrity (SMRIN) to remove low quality samples\n",
    "    healthy_samples = whole_blood_samples[\"SMRIN\"] >= 7.0\n",
    "\n",
    "    # Return SAMPIDs of healthy whole blood samples\n",
    "    healthy_whole_blood_samples = healthy_samples.index.tolist()\n",
    "    return healthy_whole_blood_samples\n",
    "\n",
    "def align_gene_columns_generator(healthy_chunk_generator, unhealthy_chunk_generator,\n",
    "                                gene_column_prefix=\"ENSG\"):\n",
    "    \"\"\"\n",
    "    Generator that aligns gene columns between healthy and unhealthy datasets.\n",
    "    Handles datasets of different sizes by yielding None for exhausted iterators.\n",
    "\n",
    "    Args:\n",
    "        healthy_chunk_generator: Generator yielding healthy dataset chunks\n",
    "        unhealthy_chunk_generator: Generator yielding unhealthy dataset chunks\n",
    "        gene_column_prefix (str): Prefix to identify gene columns (default: \"ENSG\")\n",
    "\n",
    "    Yields:\n",
    "        tuple: (aligned_healthy_chunk, aligned_unhealthy_chunk) for each chunk pair\n",
    "               None is yielded for exhausted datasets\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting gene column alignment...\")\n",
    "\n",
    "    healthy_iter = iter(healthy_chunk_generator)\n",
    "    unhealthy_iter = iter(unhealthy_chunk_generator)\n",
    "\n",
    "    try:\n",
    "        first_healthy_chunk = next(healthy_iter)\n",
    "        first_unhealthy_chunk = next(unhealthy_iter)\n",
    "    except StopIteration:\n",
    "        print(\"ERROR: One or both datasets are empty!\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nProcessing gene columns and stripping version suffixes...\")\n",
    "    def extract_gene_info(chunk, dataset_name):\n",
    "        \"\"\"Extract gene columns and create mapping from original_column to base_id\"\"\"\n",
    "        print(f\"Processing {dataset_name} dataset...\")\n",
    "\n",
    "        original_to_base = {}\n",
    "        base_gene_ids = set()\n",
    "        total_gene_columns = 0\n",
    "        duplicate_count = 0\n",
    "\n",
    "        for column in chunk.columns:\n",
    "            if isinstance(column, str) and column.startswith(gene_column_prefix):\n",
    "                total_gene_columns += 1\n",
    "                base_gene_id = column.split('.')[0]\n",
    "\n",
    "                # Handle duplicates (keep first occurrence)\n",
    "                if base_gene_id not in base_gene_ids:\n",
    "                    original_to_base[column] = base_gene_id\n",
    "                    base_gene_ids.add(base_gene_id)\n",
    "                else:\n",
    "                    duplicate_count += 1\n",
    "                    print(f\"Warning: Duplicate base gene {base_gene_id} found, skipping {column}\")\n",
    "\n",
    "        print(f\"Total gene columns found: {total_gene_columns}\")\n",
    "        print(f\"Unique base gene IDs: {len(base_gene_ids)}\")\n",
    "        if duplicate_count > 0:\n",
    "            print(f\"Duplicates removed: {duplicate_count}\")\n",
    "\n",
    "        return original_to_base, base_gene_ids\n",
    "\n",
    "    healthy_rename_mapping, healthy_base_genes = extract_gene_info(first_healthy_chunk, \"HEALTHY\")\n",
    "    unhealthy_rename_mapping, unhealthy_base_genes = extract_gene_info(first_unhealthy_chunk, \"UNHEALTHY\")\n",
    "\n",
    "    common_base_genes = healthy_base_genes & unhealthy_base_genes\n",
    "\n",
    "    if not common_base_genes:\n",
    "        print(\"ERROR: No common genes found between datasets!\")\n",
    "        return\n",
    "\n",
    "    print(f\"Common genes: {len(common_base_genes)}\")\n",
    "    print(f\"Genes exclusive to healthy dataset: {len(healthy_base_genes - common_base_genes)}\")\n",
    "    print(f\"Genes exclusive to unhealthy dataset: {len(unhealthy_base_genes - common_base_genes)}\")\n",
    "\n",
    "    renamed_first_healthy = first_healthy_chunk.rename(columns=healthy_rename_mapping)\n",
    "    renamed_first_unhealthy = first_unhealthy_chunk.rename(columns=unhealthy_rename_mapping)\n",
    "\n",
    "    healthy_non_gene_cols = [col for col in renamed_first_healthy.columns\n",
    "                           if not (isinstance(col, str) and col.startswith(gene_column_prefix))]\n",
    "    unhealthy_non_gene_cols = [col for col in renamed_first_unhealthy.columns\n",
    "                             if not (isinstance(col, str) and col.startswith(gene_column_prefix))]\n",
    "\n",
    "    # Validate that non-gene columns match\n",
    "    if set(healthy_non_gene_cols) != set(unhealthy_non_gene_cols):\n",
    "        print(\"Warning: Non-gene columns differ between datasets!\")\n",
    "        print(f\"Healthy only: {set(healthy_non_gene_cols) - set(unhealthy_non_gene_cols)}\")\n",
    "        print(f\"Unhealthy only: {set(unhealthy_non_gene_cols) - set(healthy_non_gene_cols)}\")\n",
    "\n",
    "    non_gene_cols = healthy_non_gene_cols\n",
    "    common_gene_base_ids = sorted(common_base_genes)\n",
    "    final_columns = non_gene_cols + common_gene_base_ids\n",
    "\n",
    "    print(\"\\nProcessing and yielding aligned chunks...\")\n",
    "    def process_chunk(chunk, rename_mapping):\n",
    "        \"\"\"Process a single chunk: rename and select columns\"\"\"\n",
    "        if chunk is None:\n",
    "            return None\n",
    "\n",
    "        renamed_chunk = chunk.rename(columns=rename_mapping)\n",
    "        aligned_chunk = renamed_chunk[final_columns].copy()\n",
    "\n",
    "        return aligned_chunk\n",
    "\n",
    "    aligned_first_healthy = process_chunk(first_healthy_chunk, healthy_rename_mapping)\n",
    "    aligned_first_unhealthy = process_chunk(first_unhealthy_chunk, unhealthy_rename_mapping)\n",
    "\n",
    "    yield aligned_first_healthy, aligned_first_unhealthy\n",
    "\n",
    "    chunk_count = 1\n",
    "    healthy_exhausted = False\n",
    "    unhealthy_exhausted = False\n",
    "\n",
    "    while not (healthy_exhausted and unhealthy_exhausted):\n",
    "        chunk_count += 1\n",
    "\n",
    "        healthy_chunk = None\n",
    "        if not healthy_exhausted:\n",
    "            try:\n",
    "                healthy_chunk = next(healthy_iter)\n",
    "            except StopIteration:\n",
    "                healthy_exhausted = True\n",
    "                print(f\"Healthy dataset exhausted after {chunk_count-1} chunks\")\n",
    "\n",
    "        unhealthy_chunk = None\n",
    "        if not unhealthy_exhausted:\n",
    "            try:\n",
    "                unhealthy_chunk = next(unhealthy_iter)\n",
    "            except StopIteration:\n",
    "                unhealthy_exhausted = True\n",
    "                print(f\"Unhealthy dataset exhausted after {chunk_count-1} chunks\")\n",
    "\n",
    "        if healthy_chunk is None and unhealthy_chunk is None:\n",
    "            break\n",
    "\n",
    "        aligned_healthy = process_chunk(healthy_chunk, healthy_rename_mapping)\n",
    "        aligned_unhealthy = process_chunk(unhealthy_chunk, unhealthy_rename_mapping)\n",
    "\n",
    "        yield aligned_healthy, aligned_unhealthy\n",
    "\n",
    "    print(f\"\\nAlignment complete!\")\n",
    "\n",
    "    # Clean up\n",
    "    if 'healthy_iter' in locals():\n",
    "        del healthy_iter\n",
    "    if 'unhealthy_iter' in locals():\n",
    "        del unhealthy_iter\n",
    "    if 'first_healthy_chunk' in locals():\n",
    "        del first_healthy_chunk\n",
    "    if 'first_unhealthy_chunk' in locals():\n",
    "        del first_unhealthy_chunk\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def merge_datasets_generator(aligned_pair_iterator):\n",
    "    \"\"\"\n",
    "    Generator that merges healthy and unhealthy dataset chunks into single merged chunks.\n",
    "\n",
    "    Args:\n",
    "        aligned_chunk_pairs_generator: Generator yielding (healthy_chunk, unhealthy_chunk) tuples\n",
    "        chunk_size (int, optional): Target size for merged chunks. If None, merges each pair directly.\n",
    "\n",
    "    Yields:\n",
    "        pandas.DataFrame: Merged chunks containing both healthy and unhealthy data\n",
    "    \"\"\"\n",
    "\n",
    "    for healthy_chunk, unhealthy_chunk in aligned_pair_iterator:\n",
    "        chunks_to_merge = []\n",
    "\n",
    "        if healthy_chunk is not None:\n",
    "            chunks_to_merge.append(healthy_chunk)\n",
    "\n",
    "        if unhealthy_chunk is not None:\n",
    "            chunks_to_merge.append(unhealthy_chunk)\n",
    "\n",
    "        if chunks_to_merge:\n",
    "            # Concatenate available chunks\n",
    "            merged_chunk = pd.concat(chunks_to_merge)\n",
    "            yield merged_chunk\n",
    "            del merged_chunk\n",
    "\n",
    "    # Clean up\n",
    "    if 'aligned_pair_iterator' in locals():\n",
    "        del aligned_pair_iterator\n",
    "    if 'healthy_chunk' in locals():\n",
    "        del healthy_chunk\n",
    "    if 'unhealthy_chunk' in locals():\n",
    "        del unhealthy_chunk\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def set_index_column(chunk_generator, column_name, drop=True):\n",
    "    \"\"\"\n",
    "    Generator function that sets a specified column as the index for each DataFrame chunk.\n",
    "\n",
    "    Args:\n",
    "        chunk_generator: An iterable that yields pandas DataFrame chunks\n",
    "        column_name: Name of the column to set as index\n",
    "        drop: Whether to drop the column after setting it as index (default: True)\n",
    "\n",
    "    Yields:\n",
    "        pd.DataFrame: Chunks with the specified column set as index\n",
    "    \"\"\"\n",
    "    for chunk in chunk_generator:\n",
    "        if chunk.empty:\n",
    "            yield chunk\n",
    "            continue\n",
    "\n",
    "        if column_name not in chunk.columns:\n",
    "            yield chunk\n",
    "            continue\n",
    "\n",
    "        chunk_with_index = chunk.set_index(column_name, drop=drop)\n",
    "\n",
    "        yield chunk_with_index\n",
    "\n",
    "\n",
    "def prepare_metadata_generator(aligned_pair_iterator, output_metadata_path=\"data/merged_metadata.pq\"):\n",
    "    \"\"\"\n",
    "    Generator that creates metadata from aligned chunk pairs and saves to parquet.\n",
    "\n",
    "    Args:\n",
    "        aligned_chunk_pairs_generator: Generator yielding (healthy_chunk, unhealthy_chunk) tuples\n",
    "        output_metadata_path: Path to save metadata parquet file\n",
    "\n",
    "    Yields:\n",
    "        tuple: (healthy_chunk, unhealthy_chunk) - passes through the original chunks\n",
    "    \"\"\"\n",
    "\n",
    "    all_metadata = []\n",
    "\n",
    "    for healthy_chunk, unhealthy_chunk in aligned_pair_iterator:\n",
    "        chunk_metadata = []\n",
    "\n",
    "        if healthy_chunk is not None:\n",
    "            healthy_sample_ids = [str(sid) for sid in healthy_chunk.index.tolist()]\n",
    "            # healthy_metadata = [{'sample_id': sid, 'condition': 0, 'batch': 'GTEx'} for sid in healthy_sample_ids]\n",
    "            healthy_metadata = [{'sample_id': sid, 'condition': 0} for sid in healthy_sample_ids]\n",
    "            chunk_metadata.extend(healthy_metadata)\n",
    "\n",
    "        if unhealthy_chunk is not None:\n",
    "            unhealthy_sample_ids = [str(sid) for sid in unhealthy_chunk.index.tolist()]\n",
    "            # unhealthy_metadata = [{'sample_id': sid, 'condition': 1, 'batch': 'GDC'} for sid in unhealthy_sample_ids]\n",
    "            unhealthy_metadata = [{'sample_id': sid, 'condition': 1} for sid in unhealthy_sample_ids]\n",
    "            chunk_metadata.extend(unhealthy_metadata)\n",
    "\n",
    "        all_metadata.extend(chunk_metadata)\n",
    "\n",
    "        # Yield the chunks unchanged (pass-through)\n",
    "        yield healthy_chunk, unhealthy_chunk\n",
    "        del healthy_chunk, unhealthy_chunk\n",
    "\n",
    "    if all_metadata:\n",
    "        metadata_df = pd.DataFrame(all_metadata)\n",
    "        metadata_df.to_parquet(output_metadata_path, index=False)\n",
    "        print(f\"Metadata saved to {output_metadata_path} with {len(metadata_df)} records\")\n",
    "    else:\n",
    "        print(\"No metadata to save\")\n",
    "\n",
    "    # Clean up\n",
    "    if 'all_metadata' in locals() or 'all_metadata' in globals():\n",
    "        del all_metadata\n",
    "    if 'metadata_df' in locals() or 'metadata_df' in globals():\n",
    "        del metadata_df\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def check_metadata(healthy_generator, unhealthy_generator, merged_metadata_generator):\n",
    "    \"\"\"\n",
    "    Comprehensive validation of metadata against healthy and unhealthy datasets.\n",
    "\n",
    "    Args:\n",
    "        healthy_generator: Generator yielding healthy dataset chunks\n",
    "        unhealthy_generator: Generator yielding unhealthy dataset chunks\n",
    "        merged_metadata_generator: Generator yielding metadata chunks\n",
    "\n",
    "    Raises:\n",
    "        Exception: If validation fails\n",
    "    \"\"\"\n",
    "\n",
    "    # Collect all sample IDs from datasets\n",
    "    healthy_sample_ids = set()\n",
    "    unhealthy_sample_ids = set()\n",
    "\n",
    "    healthy_dataset_line_count = 0\n",
    "    unhealthy_dataset_line_count = 0\n",
    "\n",
    "    print(\"Collecting sample IDs from healthy dataset...\")\n",
    "    for chunk in healthy_generator:\n",
    "        chunk_sample_ids = set(str(sid) for sid in chunk.index.tolist())\n",
    "        healthy_sample_ids.update(chunk_sample_ids)\n",
    "        healthy_dataset_line_count += len(chunk)\n",
    "\n",
    "    print(\"Collecting sample IDs from unhealthy dataset...\")\n",
    "    for chunk in unhealthy_generator:\n",
    "        chunk_sample_ids = set(str(sid) for sid in chunk.index.tolist())\n",
    "        unhealthy_sample_ids.update(chunk_sample_ids)\n",
    "        unhealthy_dataset_line_count += len(chunk)\n",
    "\n",
    "    # Collect metadata information\n",
    "    metadata_sample_ids = set()\n",
    "    healthy_metadata_ids = set()\n",
    "    unhealthy_metadata_ids = set()\n",
    "    # gtex_batch_ids = set()\n",
    "    # gdc_batch_ids = set()\n",
    "    condition_0_ids = set()\n",
    "    condition_1_ids = set()\n",
    "\n",
    "    metadata_line_count = 0\n",
    "\n",
    "    print(\"Validating metadata...\")\n",
    "    for chunk in merged_metadata_generator:\n",
    "        metadata_line_count += len(chunk)\n",
    "\n",
    "        # Process each row in metadata chunk\n",
    "        for _, row in chunk.iterrows():\n",
    "            sample_id = str(row['sample_id'])\n",
    "            condition = row['condition']\n",
    "            # batch = row['batch']\n",
    "\n",
    "            metadata_sample_ids.add(sample_id)\n",
    "\n",
    "            # Categorize by condition\n",
    "            if condition == 0:\n",
    "                condition_0_ids.add(sample_id)\n",
    "                healthy_metadata_ids.add(sample_id)\n",
    "            elif condition == 1:\n",
    "                condition_1_ids.add(sample_id)\n",
    "                unhealthy_metadata_ids.add(sample_id)\n",
    "            else:\n",
    "                raise Exception(f\"Invalid condition value: {condition} for sample {sample_id}\")\n",
    "\n",
    "            # # Categorize by batch\n",
    "            # if batch == 'GTEx':\n",
    "            #     gtex_batch_ids.add(sample_id)\n",
    "            # elif batch == 'GDC':\n",
    "            #     gdc_batch_ids.add(sample_id)\n",
    "            # else:\n",
    "            #     raise Exception(f\"Invalid batch value: {batch} for sample {sample_id}\")\n",
    "\n",
    "    # Validation checks\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"METADATA VALIDATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 1. Check row counts\n",
    "    total_dataset_count = healthy_dataset_line_count + unhealthy_dataset_line_count\n",
    "    print(f\"Dataset row counts:\")\n",
    "    print(f\"  Healthy: {healthy_dataset_line_count}\")\n",
    "    print(f\"  Unhealthy: {unhealthy_dataset_line_count}\")\n",
    "    print(f\"  Total: {total_dataset_count}\")\n",
    "    print(f\"  Metadata: {metadata_line_count}\")\n",
    "\n",
    "    if total_dataset_count != metadata_line_count:\n",
    "        raise Exception(f\"Row count mismatch! Dataset total: {total_dataset_count}, Metadata: {metadata_line_count}\")\n",
    "\n",
    "    # 2. Check sample ID completeness\n",
    "    print(f\"\\nSample ID validation:\")\n",
    "    print(f\"  Unique healthy samples in dataset: {len(healthy_sample_ids)}\")\n",
    "    print(f\"  Unique unhealthy samples in dataset: {len(unhealthy_sample_ids)}\")\n",
    "    print(f\"  Total unique samples in datasets: {len(healthy_sample_ids) + len(unhealthy_sample_ids)}\")\n",
    "    print(f\"  Unique samples in metadata: {len(metadata_sample_ids)}\")\n",
    "\n",
    "    # Check if all dataset sample IDs are in metadata\n",
    "    missing_healthy_in_metadata = healthy_sample_ids - metadata_sample_ids\n",
    "    missing_unhealthy_in_metadata = unhealthy_sample_ids - metadata_sample_ids\n",
    "    extra_metadata_samples = metadata_sample_ids - (healthy_sample_ids | unhealthy_sample_ids)\n",
    "\n",
    "    if missing_healthy_in_metadata:\n",
    "        raise Exception(f\"Missing healthy samples in metadata: {list(missing_healthy_in_metadata)[:10]}...\")\n",
    "\n",
    "    if missing_unhealthy_in_metadata:\n",
    "        raise Exception(f\"Missing unhealthy samples in metadata: {list(missing_unhealthy_in_metadata)[:10]}...\")\n",
    "\n",
    "    if extra_metadata_samples:\n",
    "        raise Exception(f\"Extra samples in metadata not in datasets: {list(extra_metadata_samples)[:10]}...\")\n",
    "\n",
    "    # 3. Check condition mapping\n",
    "    print(f\"\\nCondition validation:\")\n",
    "    print(f\"  Condition 0 (healthy) samples: {len(condition_0_ids)}\")\n",
    "    print(f\"  Condition 1 (unhealthy) samples: {len(condition_1_ids)}\")\n",
    "\n",
    "    # Verify condition 0 matches healthy dataset\n",
    "    healthy_condition_mismatch = healthy_sample_ids - condition_0_ids\n",
    "    condition_0_mismatch = condition_0_ids - healthy_sample_ids\n",
    "\n",
    "    if healthy_condition_mismatch:\n",
    "        raise Exception(f\"Healthy samples with wrong condition in metadata: {list(healthy_condition_mismatch)[:10]}...\")\n",
    "\n",
    "    if condition_0_mismatch:\n",
    "        raise Exception(f\"Condition 0 samples not in healthy dataset: {list(condition_0_mismatch)[:10]}...\")\n",
    "\n",
    "    # Verify condition 1 matches unhealthy dataset\n",
    "    unhealthy_condition_mismatch = unhealthy_sample_ids - condition_1_ids\n",
    "    condition_1_mismatch = condition_1_ids - unhealthy_sample_ids\n",
    "\n",
    "    if unhealthy_condition_mismatch:\n",
    "        raise Exception(f\"Unhealthy samples with wrong condition in metadata: {list(unhealthy_condition_mismatch)[:10]}...\")\n",
    "\n",
    "    if condition_1_mismatch:\n",
    "        raise Exception(f\"Condition 1 samples not in unhealthy dataset: {list(condition_1_mismatch)[:10]}...\")\n",
    "\n",
    "    # # 4. Check batch mapping\n",
    "    # print(f\"\\nBatch validation:\")\n",
    "    # print(f\"  GTEx batch samples: {len(gtex_batch_ids)}\")\n",
    "    # print(f\"  GDC batch samples: {len(gdc_batch_ids)}\")\n",
    "\n",
    "    # Check if condition and batch alignment is correct\n",
    "    # gtex_condition_mismatch = gtex_batch_ids - condition_0_ids\n",
    "    # gdc_condition_mismatch = gdc_batch_ids - condition_1_ids\n",
    "    # condition_0_batch_mismatch = condition_0_ids - gtex_batch_ids\n",
    "    # condition_1_batch_mismatch = condition_1_ids - gdc_batch_ids\n",
    "\n",
    "    # if gtex_condition_mismatch:\n",
    "    #     raise Exception(f\"GTEx samples with wrong condition (should be 0): {list(gtex_condition_mismatch)[:10]}...\")\n",
    "\n",
    "    # if gdc_condition_mismatch:\n",
    "    #     raise Exception(f\"GDC samples with wrong condition (should be 1): {list(gdc_condition_mismatch)[:10]}...\")\n",
    "\n",
    "    # if condition_0_batch_mismatch:\n",
    "    #     raise Exception(f\"Condition 0 samples with wrong batch (should be GTEx): {list(condition_0_batch_mismatch)[:10]}...\")\n",
    "\n",
    "    # if condition_1_batch_mismatch:\n",
    "    #     raise Exception(f\"Condition 1 samples with wrong batch (should be GDC): {list(condition_1_batch_mismatch)[:10]}...\")\n",
    "\n",
    "    # 5. Check for duplicates in metadata\n",
    "    if len(metadata_sample_ids) != metadata_line_count:\n",
    "        print(f\"\\nWARNING: Duplicate sample IDs detected in metadata!\")\n",
    "        print(f\"  Unique sample IDs: {len(metadata_sample_ids)}\")\n",
    "        print(f\"  Total metadata rows: {metadata_line_count}\")\n",
    "        print(f\"  Duplicates: {metadata_line_count - len(metadata_sample_ids)}\")\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"✅ VALIDATION PASSED!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"✅ Row counts match: {metadata_line_count} total\")\n",
    "    print(f\"✅ All sample IDs present and correct\")\n",
    "    print(f\"✅ Conditions correctly mapped: {len(condition_0_ids)} healthy, {len(condition_1_ids)} unhealthy\")\n",
    "    # print(f\"✅ Batches correctly mapped: {len(gtex_batch_ids)} GTEx, {len(gdc_batch_ids)} GDC\")\n",
    "    print(f\"✅ No sample ID mismatches detected\")\n",
    "\n",
    "\n",
    "def get_healthy_whole_blood_samples(metadata_path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Get healthy whole blood samples from the metadata.\n",
    "\n",
    "    Args:\n",
    "        metadata_path (str): Path to the metadata CSV file\n",
    "        gtex_blood_ids (list[str]): List of GTEx whole blood sample IDs\n",
    "\n",
    "    Returns:\n",
    "        list[str]: List of healthy whole blood sample IDs\n",
    "    \"\"\"\n",
    "    metadata_df = pd.read_csv(metadata_path, index_col=0)\n",
    "\n",
    "    # Get all samples with 'Whole Blood' SMTSD\n",
    "    whole_blood_samples = metadata_df[metadata_df['SMTSD'] == 'Whole Blood']\n",
    "    # Filter on RNA integrity (SMRIN) to remove low quality samples\n",
    "    healthy_samples = whole_blood_samples[\"SMRIN\"] >= 7.0\n",
    "\n",
    "    # Return SAMPIDs of healthy whole blood samples\n",
    "    healthy_whole_blood_samples = healthy_samples.index.tolist()\n",
    "    return healthy_whole_blood_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# GTEx and GDC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Unhealthy Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthy_dataset_file = 'data/unhealthy_data.pq'\n",
    "unhealthy_output_file = 'data/unhealthy_data_preprocessed.pq'\n",
    "chunk_size = 8000\n",
    "\n",
    "chunk_iterator = load_parquet_in_chunks(unhealthy_dataset_file)\n",
    "print(f\"Starting preprocessing for Unhealthy Dataset: {unhealthy_dataset_file}...\")\n",
    "\n",
    "chunk_iterator = filter_rows(chunk_iterator)\n",
    "chunk_iterator = transpose_dataframe_chunks(chunk_iterator, output_batch_size=chunk_size, dtype='float32')\n",
    "chunk_iterator = rename_index(chunk_iterator, 'sample_id')\n",
    "chunk_iterator = set_index_column(chunk_iterator, 'sample_id')\n",
    "chunk_iterator = clean_duplicate_nans(chunk_iterator)\n",
    "\n",
    "save_data_as_parquet(chunk_iterator, unhealthy_output_file)\n",
    "\n",
    "# Clean up\n",
    "if 'chunk_iterator' in locals():\n",
    "    del chunk_iterator\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthy_iterator = load_parquet_in_chunks('data/unhealthy_data_preprocessed.pq')\n",
    "\n",
    "if unhealthy_iterator:\n",
    "    first_chunk = next(unhealthy_iterator)\n",
    "\n",
    "    # Basic info\n",
    "    print(f\"First chunk shape: {first_chunk.shape}\")\n",
    "    print(f\"Columns: {list(first_chunk.columns)}\")\n",
    "    print(f\"Data types:\\n{first_chunk.dtypes}\")\n",
    "    print(f\"Memory usage: {first_chunk.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(first_chunk.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Healthy Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = 'data/gene_tpm_2022-06-06_v10_breast_mammary_tissue.gct'\n",
    "output_file = 'data/healthy_data.pq'\n",
    "\n",
    "gct_chunk_iterator = load_csv_in_chunks(\n",
    "    file_path=dataset_file,\n",
    "    sep='\\t',\n",
    "    skiprows=2,\n",
    "    header=0,\n",
    "    index_col=0,\n",
    ")\n",
    "\n",
    "if gct_chunk_iterator:\n",
    "    print(\"Saving GCT dataset as pickle...\")\n",
    "\n",
    "    save_data_as_parquet(\n",
    "        chunk_iterator=gct_chunk_iterator,\n",
    "        output_parquet_path=output_file\n",
    "    )\n",
    "else:\n",
    "    print(f\"Failed to load {dataset_file} using load_csv_in_chunks.\")\n",
    "\n",
    "# Clean up\n",
    "if 'gct_chunk_iterator' in locals() or 'gct_chunk_iterator' in globals():\n",
    "    del gct_chunk_iterator\n",
    "if 'dataset_file' in locals() or 'dataset_file' in globals():\n",
    "    del dataset_file\n",
    "if 'output_file' in locals() or 'output_file' in globals():\n",
    "    del output_file\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthy_iterator = load_parquet_in_chunks('data/healthy_data.pq')\n",
    "\n",
    "if unhealthy_iterator:\n",
    "    first_chunk = next(unhealthy_iterator)\n",
    "\n",
    "    # Basic info\n",
    "    print(f\"First chunk shape: {first_chunk.shape}\")\n",
    "    print(f\"Columns: {list(first_chunk.columns)}\")\n",
    "    print(f\"Data types:\\n{first_chunk.dtypes}\")\n",
    "    print(f\"Memory usage: {first_chunk.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(first_chunk.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_data_input = 'data/healthy_data.pq'\n",
    "healthy_data_output = 'data/healthy_data_preprocessed.pq'\n",
    "exclude_cols = ['Description']\n",
    "\n",
    "chunk_iterator = load_parquet_in_chunks(\n",
    "    file_path=healthy_data_input\n",
    ")\n",
    "\n",
    "chunk_iterator = drop_dataframe_chunks(chunk_generator=chunk_iterator, drop_columns=['Description'])\n",
    "chunk_iterator = filter_rows(chunk_iterator)\n",
    "chunk_iterator = transpose_dataframe_chunks(chunk_generator=chunk_iterator)\n",
    "chunk_iterator = set_index_column(chunk_iterator, 'sample_id')\n",
    "chunk_iterator = clean_duplicate_nans(chunk_iterator)\n",
    "\n",
    "save_data_as_parquet(\n",
    "    chunk_iterator=chunk_iterator,\n",
    "    output_parquet_path=healthy_data_output\n",
    ")\n",
    "\n",
    "# Clean up\n",
    "if 'chunk_iterator' in locals() or 'chunk_iterator' in globals():\n",
    "    del chunk_iterator\n",
    "if 'healthy_data_input' in locals() or 'healthy_data_input' in globals():\n",
    "    del healthy_data_input\n",
    "if 'healthy_data_output' in locals() or 'healthy_data_output' in globals():\n",
    "    del healthy_data_output\n",
    "if 'exclude_cols' in locals() or 'exclude_cols' in globals():\n",
    "    del exclude_cols\n",
    "if 'chunk_size' in locals() or 'chunk_size' in globals():\n",
    "    del chunk_size\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthy_iterator = load_parquet_in_chunks('data/healthy_data_preprocessed.pq')\n",
    "\n",
    "if unhealthy_iterator:\n",
    "    first_chunk = next(unhealthy_iterator)\n",
    "\n",
    "    # Basic info\n",
    "    print(f\"First chunk shape: {first_chunk.shape}\")\n",
    "    print(f\"Columns: {list(first_chunk.columns)}\")\n",
    "    print(f\"Data types:\\n{first_chunk.dtypes}\")\n",
    "    print(f\"Memory usage: {first_chunk.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(first_chunk.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_data_path = 'data/healthy_data_preprocessed.pq'\n",
    "unhealthy_data_path = 'data/unhealthy_data_preprocessed.pq'\n",
    "\n",
    "\n",
    "healthy_iterator = load_parquet_in_chunks(healthy_data_path)\n",
    "unhealthy_iterator = load_parquet_in_chunks(unhealthy_data_path)\n",
    "\n",
    "aligned_pair_iterator = align_gene_columns_generator(healthy_iterator, unhealthy_iterator)\n",
    "aligned_pair_iterator = prepare_metadata_generator(aligned_pair_iterator, output_metadata_path=\"data/merged_metadata.pq\")\n",
    "merged_chunk_iterator = merge_datasets_generator(aligned_pair_iterator)\n",
    "\n",
    "save_data_as_parquet(chunk_iterator=merged_chunk_iterator, output_parquet_path='data/merged_dataset.pq')\n",
    "\n",
    "# Clean up\n",
    "if 'healthy_iterator' in locals() or 'healthy_iterator' in globals():\n",
    "    del healthy_iterator\n",
    "if 'unhealthy_iterator' in locals() or 'unhealthy_iterator' in globals():\n",
    "    del unhealthy_iterator\n",
    "if 'aligned_pair_iterator' in locals() or 'aligned_pair_iterator' in globals():\n",
    "    del aligned_pair_iterator\n",
    "if 'merged_chunk_iterator' in locals() or 'merged_chunk_iterator' in globals():\n",
    "    del merged_chunk_iterator\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_iterator = load_parquet_in_chunks('data/merged_dataset.pq')\n",
    "\n",
    "if merged_iterator:\n",
    "    first_chunk = next(merged_iterator)\n",
    "\n",
    "    # Basic info\n",
    "    print(f\"First chunk shape: {first_chunk.shape}\")\n",
    "    print(f\"Columns: {list(first_chunk.columns)}\")\n",
    "    print(f\"Data types:\\n{first_chunk.dtypes}\")\n",
    "    print(f\"Memory usage: {first_chunk.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(first_chunk.head())\n",
    "\n",
    "# Clean up\n",
    "if 'merged_iterator' in locals() or 'merged_iterator' in globals():\n",
    "    del merged_iterator\n",
    "if 'first_chunk' in locals() or 'first_chunk' in globals():\n",
    "    del first_chunk\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_iterator = load_parquet_in_chunks('data/merged_metadata.pq')\n",
    "\n",
    "if merged_iterator:\n",
    "    first_chunk = next(merged_iterator)\n",
    "\n",
    "    # Basic info\n",
    "    print(f\"First chunk shape: {first_chunk.shape}\")\n",
    "    print(f\"Columns: {list(first_chunk.columns)}\")\n",
    "    print(f\"Data types:\\n{first_chunk.dtypes}\")\n",
    "    print(f\"Memory usage: {first_chunk.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(first_chunk.head())\n",
    "\n",
    "# Clean up\n",
    "if 'merged_iterator' in locals() or 'merged_iterator' in globals():\n",
    "    del merged_iterator\n",
    "if 'first_chunk' in locals() or 'first_chunk' in globals():\n",
    "    del first_chunk\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_iterator = load_parquet_in_chunks('data/healthy_data_preprocessed.pq')\n",
    "unhealthy_iterator = load_parquet_in_chunks('data/unhealthy_data_preprocessed.pq')\n",
    "metadata_iterator = load_parquet_in_chunks('data/merged_metadata.pq')\n",
    "\n",
    "check_metadata(healthy_generator=healthy_iterator, unhealthy_generator=unhealthy_iterator, merged_metadata_generator=metadata_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Simple PCA plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_parquet(\"data/merged_dataset.pq\")\n",
    "metadata = pd.read_parquet(\"data/merged_metadata.pq\")\n",
    "\n",
    "# Set sample_id as index for metadata to match with df\n",
    "metadata = metadata.set_index('sample_id')\n",
    "\n",
    "# Align metadata with df samples\n",
    "y = metadata.loc[df.index, \"condition\"]\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Samples: {df.shape[0]}, Genes: {df.shape[1]}\")\n",
    "print(f\"Condition distribution: {y.value_counts()}\")\n",
    "\n",
    "# Calculate mean differences between conditions\n",
    "gene_columns = df.columns\n",
    "mean_healthy = df[y == 0][gene_columns].mean()  # Condition 0 = healthy\n",
    "mean_unhealthy = df[y == 1][gene_columns].mean()  # Condition 1 = unhealthy\n",
    "mean_diff = (mean_unhealthy - mean_healthy).abs()\n",
    "\n",
    "print(\"\\nHead of Mean Differences (for top 5 genes):\")\n",
    "print(mean_diff.head())\n",
    "\n",
    "# Select top k genes by mean difference\n",
    "k_genes = 50_000\n",
    "top_k_genes = mean_diff.nlargest(k_genes).index\n",
    "x_selected = df[top_k_genes]\n",
    "\n",
    "print(f\"\\nOriginal number of genes: {df.shape[1]}\")\n",
    "print(f\"Number of genes after selection (top {k_genes} by mean difference): {x_selected.shape[1]}\")\n",
    "\n",
    "# Scale the data\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x_selected)\n",
    "\n",
    "print(\"Shape of x_scaled:\", x_scaled.shape)\n",
    "\n",
    "# Perform PCA\n",
    "pca = sklearn.decomposition.PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(x_scaled)\n",
    "\n",
    "print(\"Shape of principal components:\", pca_result.shape)\n",
    "\n",
    "# Create PCA DataFrame\n",
    "pca_df = pd.DataFrame(\n",
    "    pca_result,\n",
    "    columns=[\"PC1\", \"PC2\"],\n",
    "    index=x_selected.index\n",
    ")\n",
    "pca_df[\"condition\"] = y\n",
    "\n",
    "print(f\"Shape of PCA DataFrame: {pca_df.shape}\")\n",
    "print(\"\\nExplained Variance Ratio:\")\n",
    "print(f\"PC1: {pca.explained_variance_ratio_[0]:.4f}\")\n",
    "print(f\"PC2: {pca.explained_variance_ratio_[1]:.4f}\")\n",
    "print(f\"Total Explained Variance (PC1 + PC2): {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Generate PCA plot\n",
    "print(\"Generating PCA plot...\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create condition labels for better visualization\n",
    "condition_labels = {0: 'Healthy', 1: 'Unhealthy'}\n",
    "pca_df['condition_label'] = pca_df['condition'].map(condition_labels)\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=pca_df,\n",
    "    x=\"PC1\",\n",
    "    y=\"PC2\",\n",
    "    hue=\"condition_label\",\n",
    "    palette=['#2E8B57', '#DC143C'],  # Green for healthy, red for unhealthy\n",
    "    alpha=0.7,\n",
    "    s=50\n",
    ")\n",
    "\n",
    "plt.title(f'PCA of Gene Expression Data (Top {k_genes} Most Discriminative Genes)')\n",
    "plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]*100:.2f}% Variance Explained)')\n",
    "plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]*100:.2f}% Variance Explained)')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(title='Condition')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some summary statistics\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(f\"Healthy samples (condition 0): {(y == 0).sum()}\")\n",
    "print(f\"Unhealthy samples (condition 1): {(y == 1).sum()}\")\n",
    "print(f\"Total samples: {len(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pyarrow.parquet as pq # Import for your generator function\n",
    "import gc # For garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGED_DATA_PATH = \"data/merged_dataset.pq\"\n",
    "\n",
    "print(\"Starting log transformation and sampling for plots...\")\n",
    "\n",
    "raw_sample_data = None\n",
    "transformed_sample_data = None\n",
    "\n",
    "for i, chunk in enumerate(load_parquet_in_chunks(MERGED_DATA_PATH, chunk_size=8000)):\n",
    "    print(f\"Processing chunk {i+1}...\")\n",
    "    if raw_sample_data is None:\n",
    "        raw_sample_data = chunk.values.flatten()\n",
    "        if len(raw_sample_data) > 1_000_000:\n",
    "             raw_sample_data = np.random.choice(raw_sample_data, size=1_000_000, replace=False)\n",
    "        print(f\"Captured {len(raw_sample_data)} raw expression values for plotting.\")\n",
    "\n",
    "    transformed_chunk = np.log2(chunk + 1)\n",
    "\n",
    "    if transformed_sample_data is None:\n",
    "        transformed_sample_data = transformed_chunk.values.flatten()\n",
    "        if len(transformed_sample_data) > 1_000_000:\n",
    "             transformed_sample_data = np.random.choice(transformed_sample_data, size=1_000_000, replace=False)\n",
    "        print(f\"Captured {len(transformed_sample_data)} transformed expression values for plotting.\")\n",
    "\n",
    "    if raw_sample_data is not None and transformed_sample_data is not None:\n",
    "        break\n",
    "\n",
    "print(\"\\nLog transformation and sampling complete. Generating plots...\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(raw_sample_data, bins=50, kde=True, color='skyblue', edgecolor='black', stat='density', log_scale=True)\n",
    "plt.title('Distribution of Raw TPM Values (Sampled Data)')\n",
    "plt.xlabel('Raw TPM Value (log scale)')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, which=\"both\", ls=\"--\", c='0.7')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(transformed_sample_data, bins=50, kde=True, color='lightcoral', edgecolor='black', stat='density')\n",
    "plt.title('Distribution of Log2(TPM + 1) Values (Sampled Data)')\n",
    "plt.xlabel('Log2(TPM + 1) Value')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, which=\"both\", ls=\"--\", c='0.7')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlots generated. Review them to confirm the transformation's effect.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Batch correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from combat.pycombat import pycombat\n",
    "import pyarrow.parquet as pq\n",
    "import gc\n",
    "from inmoose.pycombat import pycombat_norm\n",
    "from inmoose.cohort_qc.cohort_metric import CohortMetric\n",
    "from inmoose.cohort_qc.qc_report import QCReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_log_transformed = \"data/merged_dataset_log_transformed.pq\"\n",
    "merged_metadata = \"data/merged_metadata.pq\"\n",
    "output_path = \"data/merged_dataset_batch_corrected.pq\"\n",
    "\n",
    "df = pd.read_parquet(data_log_transformed)\n",
    "\n",
    "metadata = pd.read_parquet(merged_metadata)\n",
    "metadata = metadata.set_index('sample_id')\n",
    "batches_series = metadata['batch']\n",
    "\n",
    "common_samples = df.index.intersection(batches_series.index)\n",
    "if len(common_samples) == 0:\n",
    "    print(\"Error: No common samples across data and metadata. Check indices.\")\n",
    "    exit()\n",
    "\n",
    "df_aligned = df.loc[common_samples]\n",
    "batches_aligned = batches_series.loc[common_samples]\n",
    "metadata_aligned = metadata.loc[common_samples]\n",
    "\n",
    "print(f\"Aligned data for ComBat. Data shape: {df_aligned.shape}, Batches series length: {len(batches_aligned)}\")\n",
    "print(f\"Condition distribution:\\n{metadata_aligned['condition'].value_counts()}\")\n",
    "print(\"Warning: Running ComBat without covariates due to perfect confounding\")\n",
    "\n",
    "print(\"Transposing data for ComBat (Genes x Samples)...\")\n",
    "data_for_combat = df_aligned.T\n",
    "print(f\"Data for ComBat shape: {data_for_combat.shape}\")\n",
    "\n",
    "# Apply batch correction without covariates\n",
    "corrected_data_df_t = pycombat_norm(\n",
    "    data_for_combat,\n",
    "    batches_aligned\n",
    ")\n",
    "print(\"Batch correction without covariates complete.\")\n",
    "\n",
    "corrected_data_df = corrected_data_df_t.T\n",
    "print(f\"Corrected data transposed back to Samples x Genes. Shape: {corrected_data_df.shape}\")\n",
    "\n",
    "corrected_data_df.to_parquet(output_path, index=True)\n",
    "print(\"Batch-corrected dataset saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_batch_correction_plots(original_data_path, corrected_data_path, metadata_path):\n",
    "    \"\"\"\n",
    "    Create comprehensive plots to validate batch correction.\n",
    "    \"\"\"\n",
    "    print(\"Loading data for plotting...\")\n",
    "\n",
    "    # Load data\n",
    "    original_df = pd.read_parquet(original_data_path)\n",
    "    corrected_df = pd.read_parquet(corrected_data_path)\n",
    "    metadata_df = pd.read_parquet(metadata_path).set_index('sample_id')\n",
    "\n",
    "    # Align data\n",
    "    common_samples = original_df.index.intersection(corrected_df.index).intersection(metadata_df.index)\n",
    "    original_aligned = original_df.loc[common_samples]\n",
    "    corrected_aligned = corrected_df.loc[common_samples]\n",
    "    metadata_aligned = metadata_df.loc[common_samples]\n",
    "\n",
    "    print(f\"Plot data shape: {original_aligned.shape}\")\n",
    "\n",
    "    # Select top variable genes for PCA (to speed up computation)\n",
    "    print(\"Selecting top 5000 most variable genes...\")\n",
    "    gene_vars = original_aligned.var(axis=0)\n",
    "    top_genes = gene_vars.nlargest(5000).index\n",
    "\n",
    "    original_subset = original_aligned[top_genes]\n",
    "    corrected_subset = corrected_aligned[top_genes]\n",
    "\n",
    "    # Standardize and perform PCA\n",
    "    print(\"Performing PCA...\")\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Original data PCA\n",
    "    original_scaled = scaler.fit_transform(original_subset)\n",
    "    pca_orig = PCA(n_components=2)\n",
    "    original_pca = pca_orig.fit_transform(original_scaled)\n",
    "\n",
    "    # Corrected data PCA\n",
    "    corrected_scaled = scaler.fit_transform(corrected_subset)\n",
    "    pca_corr = PCA(n_components=2)\n",
    "    corrected_pca = pca_corr.fit_transform(corrected_scaled)\n",
    "\n",
    "    # Create plotting dataframes\n",
    "    plot_df_orig = pd.DataFrame({\n",
    "        'PC1': original_pca[:, 0],\n",
    "        'PC2': original_pca[:, 1],\n",
    "        'batch': metadata_aligned['batch'],\n",
    "        'condition': metadata_aligned['condition'].map({0: 'Healthy', 1: 'Unhealthy'}),\n",
    "        'dataset': 'Original'\n",
    "    })\n",
    "\n",
    "    plot_df_corr = pd.DataFrame({\n",
    "        'PC1': corrected_pca[:, 0],\n",
    "        'PC2': corrected_pca[:, 1],\n",
    "        'batch': metadata_aligned['batch'],\n",
    "        'condition': metadata_aligned['condition'].map({0: 'Healthy', 1: 'Unhealthy'}),\n",
    "        'dataset': 'Batch Corrected'\n",
    "    })\n",
    "\n",
    "    # Create plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Batch Correction Validation', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Plot 1: Original data colored by batch\n",
    "    sns.scatterplot(data=plot_df_orig, x='PC1', y='PC2', hue='batch',\n",
    "                   alpha=0.7, s=50, ax=axes[0,0])\n",
    "    axes[0,0].set_title(f'Original Data - Colored by Batch\\nPC1: {pca_orig.explained_variance_ratio_[0]:.2%}, PC2: {pca_orig.explained_variance_ratio_[1]:.2%}')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Corrected data colored by batch\n",
    "    sns.scatterplot(data=plot_df_corr, x='PC1', y='PC2', hue='batch',\n",
    "                   alpha=0.7, s=50, ax=axes[0,1])\n",
    "    axes[0,1].set_title(f'Batch Corrected Data - Colored by Batch\\nPC1: {pca_corr.explained_variance_ratio_[0]:.2%}, PC2: {pca_corr.explained_variance_ratio_[1]:.2%}')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Original data colored by condition\n",
    "    sns.scatterplot(data=plot_df_orig, x='PC1', y='PC2', hue='condition',\n",
    "                   palette=['#2E8B57', '#DC143C'], alpha=0.7, s=50, ax=axes[1,0])\n",
    "    axes[1,0].set_title('Original Data - Colored by Condition')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: Corrected data colored by condition\n",
    "    sns.scatterplot(data=plot_df_corr, x='PC1', y='PC2', hue='condition',\n",
    "                   palette=['#2E8B57', '#DC143C'], alpha=0.7, s=50, ax=axes[1,1])\n",
    "    axes[1,1].set_title('Batch Corrected Data - Colored by Condition')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"VISUAL VALIDATION SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Expected results after good batch correction:\")\n",
    "    print(\"• Top row: Batch separation should be REDUCED\")\n",
    "    print(\"• Bottom row: Condition separation should be PRESERVED\")\n",
    "    print(\"\\nIf batch effects were successfully removed:\")\n",
    "    print(\"• GTEx and GDC samples should mix better in corrected data\")\n",
    "    print(\"• Healthy vs Unhealthy separation should remain clear\")\n",
    "\n",
    "    return plot_df_orig, plot_df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n1. QUANTITATIVE METRICS (inmoose)\")\n",
    "# print(\"-\" * 40)\n",
    "# qc_report, cohort_qc = validate_batch_correction_with_inmoose(\n",
    "#     original_data_path=\"data/merged_dataset_log_transformed.pq\",\n",
    "#     corrected_data_path=\"data/merged_dataset_batch_corrected.pq\",\n",
    "#     metadata_path=\"data/merged_metadata.pq\"\n",
    "# )\n",
    "\n",
    "print(\"\\n2. VISUAL VALIDATION (Custom Plots)\")\n",
    "print(\"-\" * 40)\n",
    "orig_plot_df, corr_plot_df = create_batch_correction_plots(\n",
    "    original_data_path=\"data/merged_dataset_log_transformed.pq\",\n",
    "    corrected_data_path=\"data/merged_dataset_batch_corrected.pq\",\n",
    "    metadata_path=\"data/merged_metadata.pq\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_feature_selection(data_path, metadata_path, output_path,\n",
    "                            variance_threshold=0.1, top_k_genes=5000,\n",
    "                            create_plots=True):\n",
    "    \"\"\"\n",
    "    Perform variance-based feature selection on batch-corrected data.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to batch-corrected dataset\n",
    "        metadata_path (str): Path to metadata\n",
    "        output_path (str): Path to save selected features\n",
    "        variance_threshold (float): Minimum variance threshold (removes low-variance genes)\n",
    "        top_k_genes (int): Number of top variable genes to select\n",
    "        create_plots (bool): Whether to generate before/after plots\n",
    "\n",
    "    Returns:\n",
    "        tuple: (original_df, selected_df, selected_genes, metadata)\n",
    "    \"\"\"\n",
    "    print(\"Loading batch-corrected data for feature selection...\")\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_parquet(data_path)\n",
    "    metadata = pd.read_parquet(metadata_path).set_index('sample_id')\n",
    "\n",
    "    # Align data\n",
    "    common_samples = df.index.intersection(metadata.index)\n",
    "    df_aligned = df.loc[common_samples]\n",
    "    metadata_aligned = metadata.loc[common_samples]\n",
    "\n",
    "    print(f\"Original dataset shape: {df_aligned.shape}\")\n",
    "    print(f\"Total genes before selection: {df_aligned.shape[1]}\")\n",
    "\n",
    "    # Step 1: Remove low-variance genes\n",
    "    print(f\"\\nStep 1: Removing genes with variance < {variance_threshold}\")\n",
    "    gene_variances = df_aligned.var(axis=0)\n",
    "    high_var_genes = gene_variances[gene_variances >= variance_threshold].index\n",
    "\n",
    "    df_high_var = df_aligned[high_var_genes]\n",
    "    print(f\"Genes after variance filtering: {len(high_var_genes)}\")\n",
    "    print(f\"Genes removed (low variance): {df_aligned.shape[1] - len(high_var_genes)}\")\n",
    "\n",
    "    # Step 2: Select top k most variable genes\n",
    "    print(f\"\\nStep 2: Selecting top {top_k_genes} most variable genes\")\n",
    "    top_var_genes = gene_variances.nlargest(min(top_k_genes, len(high_var_genes))).index\n",
    "\n",
    "    df_selected = df_aligned[top_var_genes]\n",
    "    print(f\"Final dataset shape: {df_selected.shape}\")\n",
    "    print(f\"Feature reduction: {df_aligned.shape[1]} → {df_selected.shape[1]} ({df_selected.shape[1]/df_aligned.shape[1]*100:.1f}%)\")\n",
    "\n",
    "    # Calculate some statistics\n",
    "    original_var_stats = {\n",
    "        'mean': gene_variances.mean(),\n",
    "        'median': gene_variances.median(),\n",
    "        'min': gene_variances.min(),\n",
    "        'max': gene_variances.max()\n",
    "    }\n",
    "\n",
    "    selected_var_stats = {\n",
    "        'mean': gene_variances[top_var_genes].mean(),\n",
    "        'median': gene_variances[top_var_genes].median(),\n",
    "        'min': gene_variances[top_var_genes].min(),\n",
    "        'max': gene_variances[top_var_genes].max()\n",
    "    }\n",
    "\n",
    "    print(f\"\\nVariance Statistics:\")\n",
    "    print(f\"Original genes - Mean: {original_var_stats['mean']:.3f}, Median: {original_var_stats['median']:.3f}\")\n",
    "    print(f\"Selected genes - Mean: {selected_var_stats['mean']:.3f}, Median: {selected_var_stats['median']:.3f}\")\n",
    "\n",
    "    # Create plots if requested\n",
    "    if create_plots:\n",
    "        create_feature_selection_plots(df_aligned, df_selected, gene_variances,\n",
    "                                     top_var_genes, metadata_aligned, variance_threshold)\n",
    "\n",
    "    # Save selected features\n",
    "    df_selected.to_parquet(output_path, index=True)\n",
    "    print(f\"\\nFeature-selected dataset saved to: {output_path}\")\n",
    "\n",
    "    return df_aligned, df_selected, top_var_genes, metadata_aligned\n",
    "\n",
    "\n",
    "def create_feature_selection_plots(original_df, selected_df, gene_variances,\n",
    "                                 selected_genes, metadata, variance_threshold):\n",
    "    \"\"\"\n",
    "    Create visualization plots for feature selection validation.\n",
    "    \"\"\"\n",
    "    print(\"\\nGenerating feature selection plots...\")\n",
    "\n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Feature Selection Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Plot 1: Variance distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    sns.histplot(gene_variances, bins=50, kde=True, alpha=0.7, ax=ax1)\n",
    "    ax1.axvline(variance_threshold, color='red', linestyle='--',\n",
    "                label=f'Threshold: {variance_threshold}')\n",
    "    ax1.axvline(gene_variances[selected_genes].min(), color='green', linestyle='--',\n",
    "                label=f'Selected min: {gene_variances[selected_genes].min():.3f}')\n",
    "    ax1.set_xlabel('Gene Variance')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.set_title('Distribution of Gene Variances')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Cumulative variance explained\n",
    "    ax2 = axes[0, 1]\n",
    "    sorted_variances = gene_variances.sort_values(ascending=False)\n",
    "    cumsum_var = sorted_variances.cumsum()\n",
    "    total_var = sorted_variances.sum()\n",
    "\n",
    "    x_range = range(1, len(sorted_variances) + 1)\n",
    "    ax2.plot(x_range, cumsum_var / total_var * 100, 'b-', alpha=0.7)\n",
    "\n",
    "    # Mark where selected genes end\n",
    "    n_selected = len(selected_genes)\n",
    "    ax2.axvline(n_selected, color='red', linestyle='--',\n",
    "                label=f'Selected genes: {n_selected}')\n",
    "    var_explained = cumsum_var.iloc[n_selected-1] / total_var * 100\n",
    "    ax2.axhline(var_explained, color='red', linestyle=':', alpha=0.7,\n",
    "                label=f'Variance captured: {var_explained:.1f}%')\n",
    "\n",
    "    ax2.set_xlabel('Number of Genes (ranked by variance)')\n",
    "    ax2.set_ylabel('Cumulative Variance Explained (%)')\n",
    "    ax2.set_title('Cumulative Variance Explained')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xlim(0, min(10000, len(sorted_variances)))  # Zoom in for better view\n",
    "\n",
    "    # Plot 3 & 4: PCA before and after feature selection\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Sample data if too large for plotting\n",
    "    n_samples_plot = min(2000, original_df.shape[0])\n",
    "    if original_df.shape[0] > n_samples_plot:\n",
    "        plot_indices = np.random.choice(original_df.index, n_samples_plot, replace=False)\n",
    "        orig_plot = original_df.loc[plot_indices]\n",
    "        sel_plot = selected_df.loc[plot_indices]\n",
    "        meta_plot = metadata.loc[plot_indices]\n",
    "    else:\n",
    "        orig_plot = original_df\n",
    "        sel_plot = selected_df\n",
    "        meta_plot = metadata\n",
    "\n",
    "    # Sample genes if too many for PCA\n",
    "    n_genes_pca = min(5000, orig_plot.shape[1])\n",
    "    if orig_plot.shape[1] > n_genes_pca:\n",
    "        pca_genes = gene_variances.nlargest(n_genes_pca).index\n",
    "        orig_plot_pca = orig_plot[pca_genes]\n",
    "    else:\n",
    "        orig_plot_pca = orig_plot\n",
    "\n",
    "    # PCA on original data\n",
    "    scaler = StandardScaler()\n",
    "    orig_scaled = scaler.fit_transform(orig_plot_pca)\n",
    "    pca_orig = PCA(n_components=2)\n",
    "    orig_pca_result = pca_orig.fit_transform(orig_scaled)\n",
    "\n",
    "    # PCA on selected data\n",
    "    sel_scaled = scaler.fit_transform(sel_plot)\n",
    "    pca_sel = PCA(n_components=2)\n",
    "    sel_pca_result = pca_sel.fit_transform(sel_scaled)\n",
    "\n",
    "    # Plot original data PCA\n",
    "    ax3 = axes[1, 0]\n",
    "    condition_labels = meta_plot['condition'].map({0: 'Healthy', 1: 'Unhealthy'})\n",
    "    scatter3 = ax3.scatter(orig_pca_result[:, 0], orig_pca_result[:, 1],\n",
    "                          c=meta_plot['condition'], cmap='RdYlBu', alpha=0.6, s=30)\n",
    "    ax3.set_xlabel(f'PC1 ({pca_orig.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    ax3.set_ylabel(f'PC2 ({pca_orig.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    ax3.set_title(f'PCA - Before Selection\\n({orig_plot_pca.shape[1]} genes)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter3, ax=ax3, label='Condition')\n",
    "\n",
    "    # Plot selected data PCA\n",
    "    ax4 = axes[1, 1]\n",
    "    scatter4 = ax4.scatter(sel_pca_result[:, 0], sel_pca_result[:, 1],\n",
    "                          c=meta_plot['condition'], cmap='RdYlBu', alpha=0.6, s=30)\n",
    "    ax4.set_xlabel(f'PC1 ({pca_sel.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    ax4.set_ylabel(f'PC2 ({pca_sel.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    ax4.set_title(f'PCA - After Selection\\n({sel_plot.shape[1]} genes)')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter4, ax=ax4, label='Condition')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Summary information\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FEATURE SELECTION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Original genes: {original_df.shape[1]:,}\")\n",
    "    print(f\"Selected genes: {selected_df.shape[1]:,}\")\n",
    "    print(f\"Reduction: {(1 - selected_df.shape[1]/original_df.shape[1])*100:.1f}%\")\n",
    "    print(f\"Variance explained by selected genes: {var_explained:.1f}%\")\n",
    "    print(f\"PC1+PC2 variance (before): {(pca_orig.explained_variance_ratio_[0] + pca_orig.explained_variance_ratio_[1])*100:.1f}%\")\n",
    "    print(f\"PC1+PC2 variance (after): {(pca_sel.explained_variance_ratio_[0] + pca_sel.explained_variance_ratio_[1])*100:.1f}%\")\n",
    "\n",
    "    return orig_pca_result, sel_pca_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data, selected_data, selected_gene_list, metadata = perform_feature_selection(\n",
    "    data_path=\"data/merged_dataset_batch_corrected.pq\",\n",
    "    metadata_path=\"data/merged_metadata.pq\",\n",
    "    output_path=\"data/merged_dataset_feature_selected.pq\",\n",
    "    variance_threshold=0.1,\n",
    "    top_k_genes=5000,\n",
    "    create_plots=True\n",
    ")\n",
    "\n",
    "print(f\"\\nFeature selection complete!\")\n",
    "print(f\"Ready for model training with {selected_data.shape[1]} selected features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_feature_selection_safe(data_path, metadata_path, output_path,\n",
    "                                 variance_threshold=0.1, top_k_genes=3000,\n",
    "                                 create_plots=True, exclude_perfect_separation=True):\n",
    "    \"\"\"\n",
    "    Perform variance-based feature selection with optional perfect separation removal.\n",
    "    \"\"\"\n",
    "    print(\"Loading batch-corrected data for SAFE feature selection...\")\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_parquet(data_path)\n",
    "    metadata = pd.read_parquet(metadata_path).set_index('sample_id')\n",
    "\n",
    "    # Align data\n",
    "    common_samples = df.index.intersection(metadata.index)\n",
    "    df_aligned = df.loc[common_samples]\n",
    "    metadata_aligned = metadata.loc[common_samples]\n",
    "    y = metadata_aligned['condition']\n",
    "\n",
    "    print(f\"Original dataset shape: {df_aligned.shape}\")\n",
    "    print(f\"Total genes before selection: {df_aligned.shape[1]}\")\n",
    "\n",
    "    excluded_features = {'perfect_separation': [], 'low_variance': []}\n",
    "\n",
    "    # Step 0: Identify and exclude perfectly separating features\n",
    "    if exclude_perfect_separation:\n",
    "        print(f\"\\nStep 0: Identifying perfectly separating features...\")\n",
    "\n",
    "        perfect_features = []\n",
    "        for i, col in enumerate(df_aligned.columns):\n",
    "            if i % 1000 == 0 and i > 0:\n",
    "                print(f\"  Checked {i:,} features...\")\n",
    "\n",
    "            # Get unique values for each class\n",
    "            healthy_values = set(df_aligned.loc[y == 0, col].dropna().unique())\n",
    "            unhealthy_values = set(df_aligned.loc[y == 1, col].dropna().unique())\n",
    "\n",
    "            # Check if completely non-overlapping\n",
    "            overlap = len(healthy_values.intersection(unhealthy_values))\n",
    "\n",
    "            if overlap == 0 and len(healthy_values) > 0 and len(unhealthy_values) > 0:\n",
    "                perfect_features.append(col)\n",
    "\n",
    "        print(f\"Found {len(perfect_features)} perfectly separating features\")\n",
    "        if len(perfect_features) > 0:\n",
    "            print(f\"Examples: {perfect_features[:5]}\")\n",
    "\n",
    "        # Remove perfect features\n",
    "        df_cleaned = df_aligned.drop(columns=perfect_features)\n",
    "        excluded_features['perfect_separation'] = perfect_features\n",
    "\n",
    "        print(f\"After removing perfect features: {df_cleaned.shape[1]} genes remaining\")\n",
    "    else:\n",
    "        df_cleaned = df_aligned\n",
    "        print(\"Skipping perfect separation check...\")\n",
    "\n",
    "    # Step 1: Remove low-variance genes\n",
    "    print(f\"\\nStep 1: Removing genes with variance < {variance_threshold}\")\n",
    "    gene_variances = df_cleaned.var(axis=0)\n",
    "\n",
    "    # Identify low variance genes\n",
    "    low_var_genes = gene_variances[gene_variances < variance_threshold].index\n",
    "    high_var_genes = gene_variances[gene_variances >= variance_threshold].index\n",
    "\n",
    "    df_high_var = df_cleaned[high_var_genes]\n",
    "    excluded_features['low_variance'] = low_var_genes.tolist()\n",
    "\n",
    "    print(f\"Genes after variance filtering: {len(high_var_genes)}\")\n",
    "    print(f\"Genes removed (low variance): {len(low_var_genes)}\")\n",
    "\n",
    "    # Step 2: Select top k most variable genes\n",
    "    print(f\"\\nStep 2: Selecting top {top_k_genes} most variable genes\")\n",
    "\n",
    "    # Ensure we don't select more genes than available\n",
    "    k_actual = min(top_k_genes, len(high_var_genes))\n",
    "    top_var_genes = gene_variances.nlargest(k_actual).index\n",
    "\n",
    "    df_selected = df_high_var[top_var_genes]\n",
    "    print(f\"Final dataset shape: {df_selected.shape}\")\n",
    "    print(f\"Feature reduction: {df_aligned.shape[1]} → {df_selected.shape[1]} ({df_selected.shape[1]/df_aligned.shape[1]*100:.1f}%)\")\n",
    "\n",
    "    # Final check: ensure no perfect separation remains\n",
    "    if exclude_perfect_separation:\n",
    "        print(f\"\\nStep 3: Final validation - checking for remaining perfect separation...\")\n",
    "        remaining_perfect = []\n",
    "        for col in df_selected.columns:\n",
    "            healthy_values = set(df_selected.loc[y == 0, col].dropna().unique())\n",
    "            unhealthy_values = set(df_selected.loc[y == 1, col].dropna().unique())\n",
    "\n",
    "            if len(healthy_values.intersection(unhealthy_values)) == 0 and len(healthy_values) > 0 and len(unhealthy_values) > 0:\n",
    "                remaining_perfect.append(col)\n",
    "\n",
    "        if remaining_perfect:\n",
    "            print(f\"⚠️  WARNING: {len(remaining_perfect)} perfect features still remain!\")\n",
    "            print(f\"Examples: {remaining_perfect[:5]}\")\n",
    "            print(\"Consider more aggressive filtering...\")\n",
    "        else:\n",
    "            print(\"✅ No perfect separation detected in final feature set\")\n",
    "\n",
    "    # Create plots if requested (with error handling)\n",
    "    if create_plots:\n",
    "        try:\n",
    "            create_safe_feature_selection_plots(df_aligned, df_selected, gene_variances,\n",
    "                                               top_var_genes, metadata_aligned, variance_threshold,\n",
    "                                               excluded_features)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Plotting failed: {str(e)}\")\n",
    "            print(\"Continuing without plots...\")\n",
    "\n",
    "    # Save selected features\n",
    "    df_selected.to_parquet(output_path, index=True)\n",
    "    print(f\"\\nSAFE feature-selected dataset saved to: {output_path}\")\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SAFE FEATURE SELECTION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Original features: {df_aligned.shape[1]:,}\")\n",
    "    print(f\"Perfect separation features removed: {len(excluded_features['perfect_separation']):,}\")\n",
    "    print(f\"Low variance features removed: {len(excluded_features['low_variance']):,}\")\n",
    "    print(f\"Final selected features: {df_selected.shape[1]:,}\")\n",
    "    print(f\"Total reduction: {(1 - df_selected.shape[1]/df_aligned.shape[1])*100:.1f}%\")\n",
    "    print(f\"Features-to-samples ratio: {df_selected.shape[1]/df_selected.shape[0]:.2f}\")\n",
    "\n",
    "    return df_aligned, df_selected, top_var_genes, metadata_aligned, excluded_features\n",
    "\n",
    "\n",
    "def create_safe_feature_selection_plots(original_df, selected_df, gene_variances,\n",
    "                                       selected_genes, metadata, variance_threshold,\n",
    "                                       excluded_features):\n",
    "    \"\"\"\n",
    "    Create visualization plots for safe feature selection with robust error handling.\n",
    "    \"\"\"\n",
    "    print(\"\\nGenerating safe feature selection plots...\")\n",
    "\n",
    "    try:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Safe Feature Selection Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # Plot 1: Feature removal breakdown\n",
    "        ax1 = axes[0, 0]\n",
    "        categories = ['Original', 'Perfect\\nRemoved', 'Low Var\\nRemoved', 'Final\\nSelected']\n",
    "        counts = [\n",
    "            original_df.shape[1],\n",
    "            original_df.shape[1] - len(excluded_features['perfect_separation']),\n",
    "            original_df.shape[1] - len(excluded_features['perfect_separation']) - len(excluded_features['low_variance']),\n",
    "            selected_df.shape[1]\n",
    "        ]\n",
    "\n",
    "        bars = ax1.bar(categories, counts, color=['red', 'orange', 'yellow', 'green'], alpha=0.7)\n",
    "        ax1.set_ylabel('Number of Features')\n",
    "        ax1.set_title('Feature Selection Pipeline')\n",
    "\n",
    "        # Add count labels on bars\n",
    "        for bar, count in zip(bars, counts):\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
    "                    str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "        # Plot 2: Variance distribution with error checking\n",
    "        ax2 = axes[0, 1]\n",
    "\n",
    "        # Check if gene_variances is valid\n",
    "        if len(gene_variances) > 0 and not gene_variances.isna().all():\n",
    "            # Remove any NaN values for plotting\n",
    "            valid_variances = gene_variances.dropna()\n",
    "\n",
    "            if len(valid_variances) > 0:\n",
    "                sns.histplot(valid_variances, bins=min(50, len(valid_variances)//10),\n",
    "                           kde=True, alpha=0.7, ax=ax2)\n",
    "\n",
    "                ax2.axvline(variance_threshold, color='red', linestyle='--',\n",
    "                           label=f'Threshold: {variance_threshold}')\n",
    "\n",
    "                # Check if selected_genes exist in gene_variances\n",
    "                selected_variances = gene_variances[gene_variances.index.isin(selected_genes)]\n",
    "                if len(selected_variances) > 0:\n",
    "                    ax2.axvline(selected_variances.min(), color='green', linestyle='--',\n",
    "                               label=f'Selected min: {selected_variances.min():.3f}')\n",
    "\n",
    "                ax2.set_xlabel('Gene Variance')\n",
    "                ax2.set_ylabel('Count')\n",
    "                ax2.set_title('Gene Variance Distribution')\n",
    "                ax2.legend()\n",
    "                ax2.grid(True, alpha=0.3)\n",
    "            else:\n",
    "                ax2.text(0.5, 0.5, 'No valid variance data',\n",
    "                        ha='center', va='center', transform=ax2.transAxes)\n",
    "                ax2.set_title('Gene Variance Distribution - No Data')\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, 'No variance data available',\n",
    "                    ha='center', va='center', transform=ax2.transAxes)\n",
    "            ax2.set_title('Gene Variance Distribution - No Data')\n",
    "\n",
    "        # Plot 3 & 4: PCA before and after with error checking\n",
    "        try:\n",
    "            from sklearn.decomposition import PCA\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "            # Sample data for plotting if too large\n",
    "            n_samples_plot = min(1000, original_df.shape[0])\n",
    "            if original_df.shape[0] > n_samples_plot:\n",
    "                plot_indices = np.random.choice(original_df.index, n_samples_plot, replace=False)\n",
    "                orig_plot = original_df.loc[plot_indices]\n",
    "                sel_plot = selected_df.loc[plot_indices]\n",
    "                meta_plot = metadata.loc[plot_indices]\n",
    "            else:\n",
    "                orig_plot = original_df\n",
    "                sel_plot = selected_df\n",
    "                meta_plot = metadata\n",
    "\n",
    "            # Check if we have enough data for PCA\n",
    "            if orig_plot.shape[1] > 1 and orig_plot.shape[0] > 1:\n",
    "                # Sample genes for PCA if too many\n",
    "                n_genes_pca = min(1000, orig_plot.shape[1])  # Reduced from 3000\n",
    "                if orig_plot.shape[1] > n_genes_pca:\n",
    "                    # Use the most variable genes that exist in the data\n",
    "                    available_genes = gene_variances.index.intersection(orig_plot.columns)\n",
    "                    if len(available_genes) > 0:\n",
    "                        pca_genes = gene_variances[available_genes].nlargest(min(n_genes_pca, len(available_genes))).index\n",
    "                        orig_plot_pca = orig_plot[pca_genes]\n",
    "                    else:\n",
    "                        orig_plot_pca = orig_plot.iloc[:, :n_genes_pca]\n",
    "                else:\n",
    "                    orig_plot_pca = orig_plot\n",
    "\n",
    "                # Remove any columns with NaN or infinite values\n",
    "                orig_plot_pca = orig_plot_pca.select_dtypes(include=[np.number])\n",
    "                orig_plot_pca = orig_plot_pca.dropna(axis=1)\n",
    "\n",
    "                if orig_plot_pca.shape[1] > 1:\n",
    "                    # PCA on original data\n",
    "                    scaler = StandardScaler()\n",
    "                    orig_scaled = scaler.fit_transform(orig_plot_pca)\n",
    "                    pca_orig = PCA(n_components=2)\n",
    "                    orig_pca_result = pca_orig.fit_transform(orig_scaled)\n",
    "\n",
    "                    # Plot original data PCA\n",
    "                    ax3 = axes[1, 0]\n",
    "                    scatter3 = ax3.scatter(orig_pca_result[:, 0], orig_pca_result[:, 1],\n",
    "                                          c=meta_plot['condition'], cmap='RdYlBu', alpha=0.6, s=30)\n",
    "                    ax3.set_xlabel(f'PC1 ({pca_orig.explained_variance_ratio_[0]:.2%})')\n",
    "                    ax3.set_ylabel(f'PC2 ({pca_orig.explained_variance_ratio_[1]:.2%})')\n",
    "                    ax3.set_title(f'PCA - Before Safe Selection\\n({orig_plot_pca.shape[1]} genes)')\n",
    "                    ax3.grid(True, alpha=0.3)\n",
    "                else:\n",
    "                    ax3 = axes[1, 0]\n",
    "                    ax3.text(0.5, 0.5, 'Insufficient features for PCA',\n",
    "                            ha='center', va='center', transform=ax3.transAxes)\n",
    "                    ax3.set_title('PCA - Before Selection (No Data)')\n",
    "            else:\n",
    "                ax3 = axes[1, 0]\n",
    "                ax3.text(0.5, 0.5, 'Insufficient data for PCA',\n",
    "                        ha='center', va='center', transform=ax3.transAxes)\n",
    "                ax3.set_title('PCA - Before Selection (No Data)')\n",
    "\n",
    "            # PCA on selected data\n",
    "            if sel_plot.shape[1] > 1 and sel_plot.shape[0] > 1:\n",
    "                # Remove any columns with NaN or infinite values\n",
    "                sel_plot_clean = sel_plot.select_dtypes(include=[np.number])\n",
    "                sel_plot_clean = sel_plot_clean.dropna(axis=1)\n",
    "\n",
    "                if sel_plot_clean.shape[1] > 1:\n",
    "                    sel_scaled = scaler.fit_transform(sel_plot_clean)\n",
    "                    pca_sel = PCA(n_components=2)\n",
    "                    sel_pca_result = pca_sel.fit_transform(sel_scaled)\n",
    "\n",
    "                    # Plot selected data PCA\n",
    "                    ax4 = axes[1, 1]\n",
    "                    scatter4 = ax4.scatter(sel_pca_result[:, 0], sel_pca_result[:, 1],\n",
    "                                          c=meta_plot['condition'], cmap='RdYlBu', alpha=0.6, s=30)\n",
    "                    ax4.set_xlabel(f'PC1 ({pca_sel.explained_variance_ratio_[0]:.2%})')\n",
    "                    ax4.set_ylabel(f'PC2 ({pca_sel.explained_variance_ratio_[1]:.2%})')\n",
    "                    ax4.set_title(f'PCA - After Safe Selection\\n({sel_plot_clean.shape[1]} genes)')\n",
    "                    ax4.grid(True, alpha=0.3)\n",
    "                else:\n",
    "                    ax4 = axes[1, 1]\n",
    "                    ax4.text(0.5, 0.5, 'Insufficient features for PCA',\n",
    "                            ha='center', va='center', transform=ax4.transAxes)\n",
    "                    ax4.set_title('PCA - After Selection (No Data)')\n",
    "            else:\n",
    "                ax4 = axes[1, 1]\n",
    "                ax4.text(0.5, 0.5, 'Insufficient data for PCA',\n",
    "                        ha='center', va='center', transform=ax4.transAxes)\n",
    "                ax4.set_title('PCA - After Selection (No Data)')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  PCA plotting failed: {str(e)}\")\n",
    "            # Fill remaining plots with error message\n",
    "            for i, ax in enumerate([axes[1, 0], axes[1, 1]]):\n",
    "                ax.text(0.5, 0.5, f'PCA Error:\\n{str(e)[:50]}...',\n",
    "                       ha='center', va='center', transform=ax.transAxes)\n",
    "                ax.set_title(f'PCA Plot {i+1} - Error')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"✅ Safe feature selection plots generated!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Plotting failed with error: {str(e)}\")\n",
    "        print(\"Continuing without plots...\")\n",
    "\n",
    "        # Print basic statistics instead\n",
    "        print(f\"\\n📊 FEATURE SELECTION SUMMARY:\")\n",
    "        print(f\"Original features: {original_df.shape[1]:,}\")\n",
    "        print(f\"Perfect separation removed: {len(excluded_features.get('perfect_separation', [])):,}\")\n",
    "        print(f\"Low variance removed: {len(excluded_features.get('low_variance', [])):,}\")\n",
    "        print(f\"Final selected features: {selected_df.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data, selected_data, selected_gene_list, metadata, excluded_features = perform_feature_selection_safe(\n",
    "    data_path=\"data/merged_dataset_batch_corrected.pq\",\n",
    "    metadata_path=\"data/merged_metadata.pq\",\n",
    "    output_path=\"data/merged_dataset_feature_selected_safe.pq\",\n",
    "    variance_threshold=0.1,\n",
    "    top_k_genes=3000,\n",
    "    create_plots=True,  # Will fall back gracefully if fails\n",
    "    exclude_perfect_separation=True\n",
    ")\n",
    "\n",
    "print(f\"\\n🎯 SAFE feature selection complete!\")\n",
    "print(f\"Perfect separating features excluded: {len(excluded_features['perfect_separation'])}\")\n",
    "print(f\"Ready for training with {selected_data.shape[1]} safe features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_split(feature_selected_path, metadata_path, output_dir=\"data/splits\",\n",
    "                           test_size=0.2, random_state=42, create_validation=False):\n",
    "    \"\"\"\n",
    "    Create train-test splits for the feature-selected dataset.\n",
    "\n",
    "    Args:\n",
    "        feature_selected_path (str): Path to feature-selected dataset\n",
    "        metadata_path (str): Path to metadata\n",
    "        output_dir (str): Directory to save split datasets\n",
    "        test_size (float): Proportion of data for test set (default: 0.2)\n",
    "        random_state (int): Random state for reproducibility\n",
    "        create_validation (bool): Whether to create a validation set from training data\n",
    "\n",
    "    Returns:\n",
    "        dict: Paths to saved split files\n",
    "    \"\"\"\n",
    "    print(\"Loading feature-selected data for splitting...\")\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_parquet(feature_selected_path)\n",
    "    metadata = pd.read_parquet(metadata_path).set_index('sample_id')\n",
    "\n",
    "    # Align data\n",
    "    common_samples = df.index.intersection(metadata.index)\n",
    "    df_aligned = df.loc[common_samples]\n",
    "    metadata_aligned = metadata.loc[common_samples]\n",
    "\n",
    "    print(f\"Dataset shape: {df_aligned.shape}\")\n",
    "    print(f\"Condition distribution: {metadata_aligned['condition'].value_counts()}\")\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare features and labels\n",
    "    X = df_aligned\n",
    "    y = metadata_aligned['condition']\n",
    "\n",
    "    # Initial train-test split (stratified)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTrain-Test Split Results:\")\n",
    "    print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"Training condition distribution: {y_train.value_counts()}\")\n",
    "    print(f\"Test condition distribution: {y_test.value_counts()}\")\n",
    "\n",
    "    # Create validation set if requested\n",
    "    if create_validation:\n",
    "        X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "            X_train, y_train,\n",
    "            test_size=0.25,  # 25% of training = 20% of total (if test_size=0.2)\n",
    "            random_state=random_state,\n",
    "            stratify=y_train\n",
    "        )\n",
    "\n",
    "        print(f\"\\nWith Validation Set:\")\n",
    "        print(f\"Final training: {X_train_final.shape[0]} samples ({X_train_final.shape[0]/len(X)*100:.1f}%)\")\n",
    "        print(f\"Validation: {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "        print(f\"Test: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "    # Save the splits\n",
    "    file_paths = {}\n",
    "\n",
    "    if create_validation:\n",
    "        # Save with validation split\n",
    "        X_train_final.to_parquet(f\"{output_dir}/X_train.pq\", index=True)\n",
    "        X_val.to_parquet(f\"{output_dir}/X_val.pq\", index=True)\n",
    "        X_test.to_parquet(f\"{output_dir}/X_test.pq\", index=True)\n",
    "\n",
    "        y_train_final.to_frame('condition').to_parquet(f\"{output_dir}/y_train.pq\", index=True)\n",
    "        y_val.to_frame('condition').to_parquet(f\"{output_dir}/y_val.pq\", index=True)\n",
    "        y_test.to_frame('condition').to_parquet(f\"{output_dir}/y_test.pq\", index=True)\n",
    "\n",
    "        file_paths = {\n",
    "            'X_train': f\"{output_dir}/X_train.pq\",\n",
    "            'X_val': f\"{output_dir}/X_val.pq\",\n",
    "            'X_test': f\"{output_dir}/X_test.pq\",\n",
    "            'y_train': f\"{output_dir}/y_train.pq\",\n",
    "            'y_val': f\"{output_dir}/y_val.pq\",\n",
    "            'y_test': f\"{output_dir}/y_test.pq\"\n",
    "        }\n",
    "\n",
    "        print(f\"\\nSaved train/validation/test splits to {output_dir}/\")\n",
    "\n",
    "    else:\n",
    "        # Save without validation split\n",
    "        X_train.to_parquet(f\"{output_dir}/X_train.pq\", index=True)\n",
    "        X_test.to_parquet(f\"{output_dir}/X_test.pq\", index=True)\n",
    "\n",
    "        y_train.to_frame('condition').to_parquet(f\"{output_dir}/y_train.pq\", index=True)\n",
    "        y_test.to_frame('condition').to_parquet(f\"{output_dir}/y_test.pq\", index=True)\n",
    "\n",
    "        file_paths = {\n",
    "            'X_train': f\"{output_dir}/X_train.pq\",\n",
    "            'X_test': f\"{output_dir}/X_test.pq\",\n",
    "            'y_train': f\"{output_dir}/y_train.pq\",\n",
    "            'y_test': f\"{output_dir}/y_test.pq\"\n",
    "        }\n",
    "\n",
    "        print(f\"\\nSaved train/test splits to {output_dir}/\")\n",
    "\n",
    "    create_split_visualization(X_train, X_test, y_train, y_test,\n",
    "                              X_val if create_validation else None,\n",
    "                              y_val if create_validation else None)\n",
    "\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "def create_split_visualization(X_train, X_test, y_train, y_test, X_val=None, y_val=None):\n",
    "    \"\"\"\n",
    "    Create visualization of the train-test split.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nCreating split visualization...\")\n",
    "\n",
    "    # Combine data for consistent PCA\n",
    "    if X_val is not None:\n",
    "        X_combined = pd.concat([X_train, X_val, X_test])\n",
    "        y_combined = pd.concat([y_train, y_val, y_test])\n",
    "        split_labels = (['Train'] * len(X_train) +\n",
    "                       ['Validation'] * len(X_val) +\n",
    "                       ['Test'] * len(X_test))\n",
    "    else:\n",
    "        X_combined = pd.concat([X_train, X_test])\n",
    "        y_combined = pd.concat([y_train, y_test])\n",
    "        split_labels = ['Train'] * len(X_train) + ['Test'] * len(X_test)\n",
    "\n",
    "    # Perform PCA\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_combined)\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(X_scaled)\n",
    "\n",
    "    # Create plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Plot 1: Colored by split\n",
    "    ax1 = axes[0]\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c'] if X_val is not None else ['#1f77b4', '#ff7f0e']\n",
    "    for i, split_type in enumerate(set(split_labels)):\n",
    "        mask = [label == split_type for label in split_labels]\n",
    "        ax1.scatter(pca_result[mask, 0], pca_result[mask, 1],\n",
    "                   c=colors[i], label=split_type, alpha=0.7, s=50)\n",
    "\n",
    "    ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    ax1.set_title('Data Split Visualization')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Colored by condition\n",
    "    ax2 = axes[1]\n",
    "    condition_colors = {0: '#2E8B57', 1: '#DC143C'}  # Green for healthy, red for unhealthy\n",
    "    for condition in [0, 1]:\n",
    "        mask = y_combined == condition\n",
    "        label = 'Healthy' if condition == 0 else 'Unhealthy'\n",
    "        ax2.scatter(pca_result[mask, 0], pca_result[mask, 1],\n",
    "                   c=condition_colors[condition], label=label, alpha=0.7, s=50)\n",
    "\n",
    "    ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    ax2.set_title('Condition Distribution Across Splits')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Split visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simple train-test split (recommended for initial testing)\n",
    "split_paths = create_train_test_split(\n",
    "    feature_selected_path=\"data/merged_dataset_feature_selected.pq\",\n",
    "    metadata_path=\"data/merged_metadata.pq\",\n",
    "    output_dir=\"data/splits\",\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    create_validation=False\n",
    ")\n",
    "\n",
    "print(\"Split files created:\")\n",
    "for split_name, path in split_paths.items():\n",
    "    print(f\"  {split_name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Handle class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_class_imbalance_lightgbm(y_train_path, y_test_path, create_plots=True):\n",
    "    \"\"\"\n",
    "    Analyze class imbalance specifically for LightGBM training.\n",
    "\n",
    "    Args:\n",
    "        y_train_path (str): Path to training labels\n",
    "        y_test_path (str): Path to test labels\n",
    "        create_plots (bool): Whether to create visualization plots\n",
    "\n",
    "    Returns:\n",
    "        dict: Class distribution statistics and LightGBM scale_pos_weight\n",
    "    \"\"\"\n",
    "    print(\"Analyzing class imbalance for LightGBM...\")\n",
    "\n",
    "    # Load labels\n",
    "    y_train = pd.read_parquet(y_train_path)['condition']\n",
    "    y_test = pd.read_parquet(y_test_path)['condition']\n",
    "\n",
    "    # Calculate class distributions\n",
    "    train_counts = y_train.value_counts().sort_index()\n",
    "    test_counts = y_test.value_counts().sort_index()\n",
    "\n",
    "    total_train = len(y_train)\n",
    "    total_test = len(y_test)\n",
    "\n",
    "    healthy_train = train_counts[0]\n",
    "    unhealthy_train = train_counts[1]\n",
    "\n",
    "    # Calculate scale_pos_weight for LightGBM\n",
    "    scale_pos_weight = healthy_train / unhealthy_train\n",
    "\n",
    "    train_ratio = unhealthy_train / healthy_train\n",
    "    test_ratio = test_counts[1] / test_counts[0]\n",
    "\n",
    "    stats = {\n",
    "        'train_healthy': healthy_train,\n",
    "        'train_unhealthy': unhealthy_train,\n",
    "        'test_healthy': test_counts[0],\n",
    "        'test_unhealthy': test_counts[1],\n",
    "        'train_total': total_train,\n",
    "        'test_total': total_test,\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'train_imbalance_ratio': train_ratio,\n",
    "        'test_imbalance_ratio': test_ratio,\n",
    "        'train_minority_percentage': (unhealthy_train / total_train) * 100,\n",
    "        'test_minority_percentage': (test_counts[1] / total_test) * 100\n",
    "    }\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"LIGHTGBM CLASS IMBALANCE ANALYSIS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Training Set:\")\n",
    "    print(f\"  Healthy (0): {healthy_train:,} samples ({healthy_train/total_train*100:.1f}%)\")\n",
    "    print(f\"  Unhealthy (1): {unhealthy_train:,} samples ({unhealthy_train/total_train*100:.1f}%)\")\n",
    "    print(f\"  Imbalance ratio: 1:{train_ratio:.2f}\")\n",
    "\n",
    "    print(f\"\\nTest Set:\")\n",
    "    print(f\"  Healthy (0): {test_counts[0]:,} samples ({test_counts[0]/total_test*100:.1f}%)\")\n",
    "    print(f\"  Unhealthy (1): {test_counts[1]:,} samples ({test_counts[1]/total_test*100:.1f}%)\")\n",
    "    print(f\"  Imbalance ratio: 1:{test_ratio:.2f}\")\n",
    "\n",
    "    print(f\"\\nLightGBM Parameter:\")\n",
    "    print(f\"  scale_pos_weight = {scale_pos_weight:.3f}\")\n",
    "\n",
    "    # Determine severity\n",
    "    if train_ratio < 0.2:\n",
    "        severity = \"SEVERE\"\n",
    "        recommendation = \"Consider using scale_pos_weight + focal loss\"\n",
    "    elif train_ratio < 0.5:\n",
    "        severity = \"MODERATE\"\n",
    "        recommendation = \"scale_pos_weight should handle this well\"\n",
    "    else:\n",
    "        severity = \"MILD\"\n",
    "        recommendation = \"scale_pos_weight may not be necessary\"\n",
    "\n",
    "    print(f\"  Imbalance severity: {severity}\")\n",
    "    print(f\"  Recommendation: {recommendation}\")\n",
    "\n",
    "    if create_plots:\n",
    "        create_lightgbm_imbalance_plots(stats)\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def create_lightgbm_imbalance_plots(stats):\n",
    "    \"\"\"\n",
    "    Create focused visualization plots for LightGBM class imbalance.\n",
    "    \"\"\"\n",
    "    print(\"\\nGenerating LightGBM-focused imbalance plots...\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('LightGBM Class Imbalance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Plot 1: Training set distribution with scale_pos_weight annotation\n",
    "    ax1 = axes[0, 0]\n",
    "    train_labels = ['Healthy', 'Unhealthy']\n",
    "    train_counts = [stats['train_healthy'], stats['train_unhealthy']]\n",
    "    colors = ['#2E8B57', '#DC143C']\n",
    "\n",
    "    bars1 = ax1.bar(train_labels, train_counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax1.set_title(f'Training Set Distribution\\nscale_pos_weight = {stats[\"scale_pos_weight\"]:.3f}')\n",
    "    ax1.set_ylabel('Number of Samples')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    for bar, count in zip(bars1, train_counts):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(train_counts)*0.01,\n",
    "                f'{count:,}\\n({count/stats[\"train_total\"]*100:.1f}%)',\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # Plot 2: Test set distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    test_counts = [stats['test_healthy'], stats['test_unhealthy']]\n",
    "\n",
    "    bars2 = ax2.bar(train_labels, test_counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax2.set_title('Test Set Distribution')\n",
    "    ax2.set_ylabel('Number of Samples')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    for bar, count in zip(bars2, test_counts):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(test_counts)*0.01,\n",
    "                f'{count:,}\\n({count/stats[\"test_total\"]*100:.1f}%)',\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # Plot 3: Imbalance ratio visualization\n",
    "    ax3 = axes[1, 0]\n",
    "    ratio_data = [1, stats['train_imbalance_ratio']]\n",
    "    ratio_labels = ['Healthy\\n(Reference)', f'Unhealthy\\n(1:{stats[\"train_imbalance_ratio\"]:.2f})']\n",
    "\n",
    "    bars3 = ax3.bar(ratio_labels, ratio_data, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax3.set_title('Class Imbalance Ratio\\n(Training Set)')\n",
    "    ax3.set_ylabel('Relative Frequency')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    for bar, ratio in zip(bars3, ratio_data):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(ratio_data)*0.01,\n",
    "                f'{ratio:.2f}',\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # Plot 4: Train vs Test comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    x = np.arange(2)\n",
    "    width = 0.35\n",
    "\n",
    "    train_props = [stats['train_healthy']/stats['train_total']*100,\n",
    "                   stats['train_unhealthy']/stats['train_total']*100]\n",
    "    test_props = [stats['test_healthy']/stats['test_total']*100,\n",
    "                  stats['test_unhealthy']/stats['test_total']*100]\n",
    "\n",
    "    bars1 = ax4.bar(x - width/2, train_props, width, label='Training',\n",
    "                   color=colors, alpha=0.7, edgecolor='black')\n",
    "    bars2 = ax4.bar(x + width/2, test_props, width, label='Test',\n",
    "                   color=colors, alpha=0.5, edgecolor='black')\n",
    "\n",
    "    ax4.set_xlabel('Class')\n",
    "    ax4.set_ylabel('Percentage (%)')\n",
    "    ax4.set_title('Distribution Consistency Check')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(['Healthy', 'Unhealthy'])\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                   f'{height:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"LightGBM imbalance analysis plots generated!\")\n",
    "    print(f\"\\n✅ Ready for LightGBM training with scale_pos_weight = {stats['scale_pos_weight']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class imbalance specifically for LightGBM\n",
    "lightgbm_stats = analyze_class_imbalance_lightgbm(\n",
    "    y_train_path=\"data/splits/y_train.pq\",\n",
    "    y_test_path=\"data/splits/y_test.pq\",\n",
    "    create_plots=True\n",
    ")\n",
    "\n",
    "print(f\"\\n🚀 LightGBM scale_pos_weight ready: {lightgbm_stats['scale_pos_weight']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Hyper param tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_lightgbm_hyperparameters(X_train_path, y_train_path, scale_pos_weight,\n",
    "                                    n_trials=100, cv_folds=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Optimize LightGBM hyperparameters using Optuna for your breast cancer dataset.\n",
    "\n",
    "    Args:\n",
    "        X_train_path (str): Path to training features\n",
    "        y_train_path (str): Path to training labels\n",
    "        scale_pos_weight (float): Class imbalance weight from previous analysis\n",
    "        n_trials (int): Number of optimization trials\n",
    "        cv_folds (int): Number of cross-validation folds\n",
    "        random_state (int): Random seed\n",
    "\n",
    "    Returns:\n",
    "        dict: Best hyperparameters and study results\n",
    "    \"\"\"\n",
    "    print(\"Loading training data for hyperparameter optimization...\")\n",
    "\n",
    "    # Load training data\n",
    "    X_train = pd.read_parquet(X_train_path)\n",
    "    y_train = pd.read_parquet(y_train_path)['condition']\n",
    "\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Class distribution: {y_train.value_counts()}\")\n",
    "\n",
    "    # Define the objective function for Optuna\n",
    "    def objective(trial):\n",
    "        # Suggest hyperparameters to optimize\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'binary_logloss',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'scale_pos_weight': scale_pos_weight,  # Fixed from class imbalance analysis\n",
    "            'random_state': random_state,\n",
    "            'verbose': -1,\n",
    "\n",
    "            # Hyperparameters to optimize\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 10, 300),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        }\n",
    "\n",
    "        # Perform cross-validation\n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "        cv_scores = []\n",
    "        for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "            X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "            # Create LightGBM datasets\n",
    "            train_data = lgb.Dataset(X_fold_train, label=y_fold_train)\n",
    "            val_data = lgb.Dataset(X_fold_val, label=y_fold_val, reference=train_data)\n",
    "\n",
    "            # Train model with proper validation dataset for early stopping\n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                train_data,\n",
    "                valid_sets=[val_data],  # Provide validation dataset\n",
    "                valid_names=['eval'],\n",
    "                num_boost_round=1000,  # Increased for early stopping\n",
    "                callbacks=[\n",
    "                    lgb.early_stopping(stopping_rounds=50),  # Early stopping\n",
    "                    lgb.log_evaluation(0)  # Silent logging\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Predict and score\n",
    "            val_pred = model.predict(X_fold_val, num_iteration=model.best_iteration)\n",
    "            auc_score = roc_auc_score(y_fold_val, val_pred)\n",
    "            cv_scores.append(auc_score)\n",
    "\n",
    "        return np.mean(cv_scores)\n",
    "\n",
    "    # Create and run optimization study\n",
    "    print(f\"\\nStarting hyperparameter optimization with {n_trials} trials...\")\n",
    "    print(\"This may take several minutes...\")\n",
    "\n",
    "    # Set random seed for reproducibility using optuna sampler\n",
    "    sampler = optuna.samplers.TPESampler(seed=random_state)\n",
    "    study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Get best parameters\n",
    "    best_params = study.best_params.copy()\n",
    "    best_params.update({\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'random_state': random_state,\n",
    "        'verbose': -1\n",
    "    })\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"HYPERPARAMETER OPTIMIZATION RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Best CV AUC Score: {study.best_value:.4f}\")\n",
    "    print(f\"Best Parameters:\")\n",
    "    for param, value in study.best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "\n",
    "    # Plot optimization history\n",
    "    create_optimization_plots(study)\n",
    "\n",
    "    return {\n",
    "        'best_params': best_params,\n",
    "        'best_score': study.best_value,\n",
    "        'study': study,\n",
    "        'n_trials': n_trials\n",
    "    }\n",
    "\n",
    "def create_optimization_plots(study):\n",
    "    \"\"\"\n",
    "    Create plots to visualize the hyperparameter optimization process.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    print(\"\\nGenerating optimization plots...\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Hyperparameter Optimization Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Plot 1: Optimization history\n",
    "    ax1 = axes[0, 0]\n",
    "    trials = study.trials\n",
    "    trial_numbers = [t.number for t in trials]\n",
    "    trial_values = [t.value for t in trials if t.value is not None]\n",
    "    trial_nums_valid = [t.number for t in trials if t.value is not None]\n",
    "\n",
    "    ax1.plot(trial_nums_valid, trial_values, 'b-', alpha=0.6)\n",
    "    ax1.axhline(study.best_value, color='red', linestyle='--',\n",
    "                label=f'Best: {study.best_value:.4f}')\n",
    "    ax1.set_xlabel('Trial Number')\n",
    "    ax1.set_ylabel('CV AUC Score')\n",
    "    ax1.set_title('Optimization History')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Parameter importance\n",
    "    ax2 = axes[0, 1]\n",
    "    try:\n",
    "        importance = optuna.importance.get_param_importances(study)\n",
    "        params = list(importance.keys())\n",
    "        values = list(importance.values())\n",
    "\n",
    "        ax2.barh(params, values, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax2.set_xlabel('Importance')\n",
    "        ax2.set_title('Parameter Importance')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    except:\n",
    "        ax2.text(0.5, 0.5, 'Parameter importance\\nnot available',\n",
    "                ha='center', va='center', transform=ax2.transAxes)\n",
    "        ax2.set_title('Parameter Importance')\n",
    "\n",
    "    # Plot 3: Learning rate vs performance\n",
    "    ax3 = axes[1, 0]\n",
    "    lr_values = [t.params.get('learning_rate') for t in trials if t.value is not None and 'learning_rate' in t.params]\n",
    "    auc_values = [t.value for t in trials if t.value is not None and 'learning_rate' in t.params]\n",
    "\n",
    "    if lr_values and auc_values:\n",
    "        ax3.scatter(lr_values, auc_values, alpha=0.6, color='green', s=30)\n",
    "        ax3.set_xlabel('Learning Rate')\n",
    "        ax3.set_ylabel('CV AUC Score')\n",
    "        ax3.set_title('Learning Rate vs Performance')\n",
    "        ax3.set_xscale('log')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'No learning rate data', ha='center', va='center', transform=ax3.transAxes)\n",
    "\n",
    "    # Plot 4: Num leaves vs performance\n",
    "    ax4 = axes[1, 1]\n",
    "    leaves_values = [t.params.get('num_leaves') for t in trials if t.value is not None and 'num_leaves' in t.params]\n",
    "    auc_values_leaves = [t.value for t in trials if t.value is not None and 'num_leaves' in t.params]\n",
    "\n",
    "    if leaves_values and auc_values_leaves:\n",
    "        ax4.scatter(leaves_values, auc_values_leaves, alpha=0.6, color='orange', s=30)\n",
    "        ax4.set_xlabel('Number of Leaves')\n",
    "        ax4.set_ylabel('CV AUC Score')\n",
    "        ax4.set_title('Num Leaves vs Performance')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'No num_leaves data', ha='center', va='center', transform=ax4.transAxes)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Optimization plots generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_results = optimize_lightgbm_hyperparameters(\n",
    "    X_train_path=\"data/splits/X_train.pq\",\n",
    "    y_train_path=\"data/splits/y_train.pq\",\n",
    "    scale_pos_weight=lightgbm_stats['scale_pos_weight'],\n",
    "    n_trials=50,\n",
    "    cv_folds=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\n🎯 Hyperparameter optimization complete!\")\n",
    "print(f\"Best parameters ready for final model training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
