{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "TSV_DIRECTORY = \"./data/GDC/downloads\"\n",
    "METADATA_FILE = \"./data/GDC/metadata/AML_METADATA.json\"\n",
    "OUTPUT_FILE = \"./data/AML.pq\"\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata_mapping(metadata_file):\n",
    "    \"\"\"\n",
    "    Load metadata and create a mapping from file_name to entity_submitter_id.\n",
    "\n",
    "    Args:\n",
    "        metadata_file: Path to the metadata JSON file\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping file_name to entity_submitter_id\n",
    "    \"\"\"\n",
    "    with open(metadata_file, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    file_to_sample_mapping = {}\n",
    "\n",
    "    for entry in metadata:\n",
    "        file_name = entry.get('file_name')\n",
    "        if file_name and 'associated_entities' in entry and entry['associated_entities']:\n",
    "            # Get the entity_submitter_id from the first associated entity\n",
    "            entity_submitter_id = entry['associated_entities'][0].get('entity_submitter_id')\n",
    "            if entity_submitter_id:\n",
    "                file_to_sample_mapping[file_name] = entity_submitter_id\n",
    "\n",
    "    logger.info(f\"Loaded metadata mapping for {len(file_to_sample_mapping)} files\")\n",
    "    return file_to_sample_mapping\n",
    "\n",
    "\n",
    "def process_tsv_file(file_path):\n",
    "    \"\"\"\n",
    "    Process a single TSV file and extract TPM unstranded values.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the TSV file\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with gene_id as index and tpm_unstranded values\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the TSV file, skipping the first comment line\n",
    "        df = pd.read_csv(file_path, skiprows=1, sep='\\t')\n",
    "\n",
    "        # Check if required columns exist\n",
    "        required_cols = ['gene_id', 'gene_name', 'gene_type', 'tpm_unstranded']\n",
    "        if not all(col in df.columns for col in required_cols):\n",
    "            logger.warning(f\"Required columns not found in {file_path}. Available columns: {df.columns.tolist()}\")\n",
    "            return None\n",
    "\n",
    "        # Filter out rows that start with 'N_' (metadata rows)\n",
    "        # Use .copy() to ensure you're working on a separate DataFrame, avoiding SettingWithCopyWarning\n",
    "        df_filtered = df[~df['gene_id'].str.startswith('N_', na=False)]\n",
    "\n",
    "        # Select the desired columns and then set gene_id as the index\n",
    "        df_processed = df_filtered[['gene_id', 'tpm_unstranded', 'gene_name', 'gene_type']].set_index('gene_id')\n",
    "\n",
    "        return df_processed\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# def combine_gdc_files(tsv_directory, metadata_file, output_file):\n",
    "#     \"\"\"\n",
    "#     Combine all GDC TSV files into a single dataset with proper sample IDs.\n",
    "\n",
    "#     Args:\n",
    "#         tsv_directory: Directory containing all TSV files\n",
    "#         metadata_file: Path to the metadata JSON file\n",
    "#         output_file: Output parquet file path\n",
    "#     \"\"\"\n",
    "#     # Load metadata mapping\n",
    "#     file_to_sample_mapping = load_metadata_mapping(metadata_file)\n",
    "\n",
    "#     # Get all TSV files\n",
    "#     tsv_path = Path(tsv_directory)\n",
    "#     tsv_files = list(tsv_path.glob(\"*.tsv\"))\n",
    "\n",
    "#     logger.info(f\"Found {len(tsv_files)} TSV files\")\n",
    "\n",
    "#     # Dictionary to store all sample data\n",
    "#     all_samples_data = []\n",
    "#     gene_ids = None\n",
    "\n",
    "#     processed_count = 0\n",
    "#     skipped_count = 0\n",
    "\n",
    "#     for tsv_file in tsv_files:\n",
    "#         file_name = tsv_file.name\n",
    "\n",
    "#         # Get sample ID from metadata\n",
    "#         if file_name not in file_to_sample_mapping:\n",
    "#             logger.warning(f\"File {file_name} not found in metadata mapping, skipping\")\n",
    "#             skipped_count += 1\n",
    "#             continue\n",
    "\n",
    "#         sample_id = file_to_sample_mapping[file_name]\n",
    "\n",
    "#         # Process the TSV file\n",
    "#         sample_data = process_tsv_file(str(tsv_file))\n",
    "\n",
    "#         if sample_data is None:\n",
    "#             skipped_count += 1\n",
    "#             continue\n",
    "\n",
    "#         # Store gene IDs from first successful file\n",
    "#         if gene_ids is None:\n",
    "#             gene_ids = sample_data[['gene_name', 'gene_type']]\n",
    "#             logger.info(f\"Using gene set from {file_name} with {len(gene_ids)} genes\")\n",
    "\n",
    "#         # Ensure consistent gene ordering\n",
    "#         sample_data = sample_data[\"tpm_unstranded\"].reindex(gene_ids.index, fill_value=0.0)\n",
    "\n",
    "#         # Store the data\n",
    "#         all_samples_data[sample_id] = sample_data.values\n",
    "#             processed_count += 1\n",
    "\n",
    "#             if processed_count % 50 == 0:\n",
    "#                 logger.info(f\"Processed {processed_count} files...\")\n",
    "\n",
    "#         logger.info(f\"Successfully processed {processed_count} files, skipped {skipped_count} files\")\n",
    "\n",
    "#         if not all_samples_data:\n",
    "#             logger.error(\"No data was successfully processed!\")\n",
    "#             return\n",
    "\n",
    "#         # Create the combined DataFrame\n",
    "#         logger.info(\"Creating combined DataFrame...\")\n",
    "#         combined_df = pd.DataFrame(all_samples_data, index=gene_ids)\n",
    "\n",
    "#         # Set the index name to 'gene_id'\n",
    "#         combined_df.index.name = 'gene_id'\n",
    "\n",
    "#         # Ensure proper data types\n",
    "#         combined_df = combined_df.astype('float32')  # TPM values as float32 to save space\n",
    "\n",
    "#         logger.info(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "#         logger.info(f\"Genes (rows): {combined_df.shape[0]}\")\n",
    "#         logger.info(f\"Samples (columns): {combined_df.shape[1]}\")\n",
    "#         logger.info(f\"Index name: {combined_df.index.name}\")\n",
    "\n",
    "#         # Save as parquet\n",
    "#         logger.info(f\"Saving to {output_file}...\")\n",
    "#         table = pa.Table.from_pandas(combined_df)\n",
    "#         pq.write_table(table, output_file, compression='snappy')\n",
    "\n",
    "#         logger.info(\"Dataset creation completed successfully!\")\n",
    "\n",
    "#         # Print some basic statistics\n",
    "#         logger.info(f\"Sample statistics:\")\n",
    "#         logger.info(f\"  - Min TPM value: {combined_df.values.min()}\")\n",
    "#         logger.info(f\"  - Max TPM value: {combined_df.values.max()}\")\n",
    "#         logger.info(f\"  - Mean TPM value: {combined_df.values.mean():.4f}\")\n",
    "#         logger.info(f\"  - Median TPM value: {float(pd.Series(combined_df.values.flatten()).median()):.4f}\")\n",
    "\n",
    "def combine_gdc_files(tsv_directory, metadata_file, output_file):\n",
    "    \"\"\"\n",
    "    Combine all GDC TSV files into a single dataset with proper sample IDs.\n",
    "\n",
    "    Args:\n",
    "        tsv_directory: Directory containing all TSV files\n",
    "        metadata_file: Path to the metadata JSON file\n",
    "        output_file: Output parquet file path\n",
    "    \"\"\"\n",
    "    # Load metadata mapping\n",
    "    file_to_sample_mapping = load_metadata_mapping(metadata_file)\n",
    "\n",
    "    # Get all TSV files\n",
    "    tsv_path = Path(tsv_directory)\n",
    "    tsv_files = list(tsv_path.glob(\"*.tsv\"))\n",
    "\n",
    "    logger.info(f\"Found {len(tsv_files)} TSV files\")\n",
    "\n",
    "    # >>>>>>>>>>>>>>>>>> START OF CHANGES <<<<<<<<<<<<<<<<<<<<\n",
    "    # Change these two lines\n",
    "    # From:\n",
    "    # all_samples_data = {}\n",
    "    # gene_ids = None\n",
    "\n",
    "    # To:\n",
    "    processed_tpm_series = [] # List to store processed Series (each Series will be a sample's TPM data)\n",
    "    gene_metadata_df = None   # Store gene metadata from the first file\n",
    "    # >>>>>>>>>>>>>>>>>> END OF CHANGES <<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "\n",
    "    for tsv_file in tsv_files:\n",
    "        file_name = tsv_file.name\n",
    "\n",
    "        # Get sample ID from metadata\n",
    "        if file_name not in file_to_sample_mapping:\n",
    "            logger.warning(f\"File {file_name} not found in metadata mapping, skipping\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        sample_id = file_to_sample_mapping[file_name]\n",
    "\n",
    "        # Process the TSV file\n",
    "        sample_data_df = process_tsv_file(str(tsv_file)) # Renamed from sample_data for clarity\n",
    "\n",
    "        if sample_data_df is None:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        # >>>>>>>>>>>>>>>>>> START OF CHANGES <<<<<<<<<<<<<<<<<<<<\n",
    "        # Replace the following block:\n",
    "        # if gene_ids is None:\n",
    "        #     gene_ids = sample_data.index.tolist()\n",
    "        #     logger.info(f\"Using gene set from {file_name} with {len(gene_ids)} genes\")\n",
    "        #\n",
    "        # # Ensure consistent gene ordering\n",
    "        # sample_data = sample_data.reindex(gene_ids, fill_value=0.0)\n",
    "        #\n",
    "        # # Store the data\n",
    "        # all_samples_data[sample_id] = sample_data.values\n",
    "\n",
    "        # With this new block:\n",
    "        # Extract gene metadata (gene_name, gene_type) from the first successful file\n",
    "        # This assumes gene_name and gene_type are consistent for the same gene_id across files\n",
    "        if gene_metadata_df is None:\n",
    "            # Create a DataFrame for gene_name and gene_type with gene_id as index\n",
    "            gene_metadata_df = sample_data_df[['gene_name', 'gene_type']]\n",
    "            logger.info(f\"Using gene set and metadata from {file_name} with {len(gene_metadata_df)} genes\")\n",
    "            # The index of gene_metadata_df (gene_ids) will be the canonical order for the final dataset\n",
    "\n",
    "        # Ensure consistent gene ordering and extract only 'tpm_unstranded' for the main matrix\n",
    "        # Reindex with the index from gene_metadata_df to ensure alignment\n",
    "        sample_tpm_series = sample_data_df['tpm_unstranded'].reindex(gene_metadata_df.index, fill_value=0.0)\n",
    "\n",
    "        # Name the series with the sample_id\n",
    "        sample_tpm_series.name = sample_id\n",
    "        processed_tpm_series.append(sample_tpm_series)\n",
    "        # >>>>>>>>>>>>>>>>>> END OF CHANGES <<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "        processed_count += 1\n",
    "\n",
    "        if processed_count % 50 == 0:\n",
    "            logger.info(f\"Processed {processed_count} files...\")\n",
    "\n",
    "    logger.info(f\"Successfully processed {processed_count} files, skipped {skipped_count} files\")\n",
    "\n",
    "    # >>>>>>>>>>>>>>>>>> START OF CHANGES <<<<<<<<<<<<<<<<<<<<\n",
    "    # Change this condition\n",
    "    # From:\n",
    "    # if not all_samples_data:\n",
    "    # To:\n",
    "    if not processed_tpm_series:\n",
    "    # >>>>>>>>>>>>>>>>>> END OF CHANGES <<<<<<<<<<<<<<<<<<<<\n",
    "        logger.error(\"No data was successfully processed!\")\n",
    "        return\n",
    "\n",
    "    # >>>>>>>>>>>>>>>>>> START OF CHANGES <<<<<<<<<<<<<<<<<<<<\n",
    "    # Replace the following block:\n",
    "    # # Create the combined DataFrame\n",
    "    # logger.info(\"Creating combined DataFrame...\")\n",
    "    # combined_df = pd.DataFrame(all_samples_data, index=gene_ids)\n",
    "    #\n",
    "    # # Set the index name to 'gene_id'\n",
    "    # combined_df.index.name = 'gene_id'\n",
    "    #\n",
    "    # # Ensure proper data types\n",
    "    # combined_df = combined_df.astype('float32')  # TPM values as float32 to save space\n",
    "\n",
    "    # With this new block:\n",
    "    # Create the combined TPM DataFrame by concatenating the list of Series\n",
    "    logger.info(\"Creating combined TPM DataFrame...\")\n",
    "    combined_tpm_df = pd.concat(processed_tpm_series, axis=1)\n",
    "\n",
    "    # Set the index name to 'gene_id' (it should already be 'gene_id' from reindex)\n",
    "    combined_tpm_df.index.name = 'gene_id'\n",
    "\n",
    "    # Ensure proper data types\n",
    "    combined_tpm_df = combined_tpm_df.astype('float32')  # TPM values as float32 to save space\n",
    "\n",
    "    logger.info(f\"Combined TPM dataset shape: {combined_tpm_df.shape}\")\n",
    "    logger.info(f\"Genes (rows): {combined_tpm_df.shape[0]}\")\n",
    "    logger.info(f\"Samples (columns): {combined_tpm_df.shape[1]}\")\n",
    "    logger.info(f\"Index name: {combined_tpm_df.index.name}\")\n",
    "\n",
    "    # Merge gene_metadata_df with combined_tpm_df\n",
    "    # This will add 'gene_name' and 'gene_type' as the first columns\n",
    "    # We join on the index (gene_id)\n",
    "    final_combined_df = gene_metadata_df.join(combined_tpm_df)\n",
    "\n",
    "    logger.info(f\"Final combined dataset shape (with gene metadata): {final_combined_df.shape}\")\n",
    "    logger.info(f\"Final combined dataset columns: {final_combined_df.columns.tolist()}\")\n",
    "    # >>>>>>>>>>>>>>>>>> END OF CHANGES <<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "    # >>>>>>>>>>>>>>>>>> START OF CHANGES <<<<<<<<<<<<<<<<<<<<\n",
    "    # Change these two lines:\n",
    "    # From:\n",
    "    # table = pa.Table.from_pandas(combined_df)\n",
    "    # pq.write_table(table, output_file, compression='snappy')\n",
    "    #\n",
    "    # To:\n",
    "    table = pa.Table.from_pandas(final_combined_df)\n",
    "    pq.write_table(table, output_file, compression='snappy')\n",
    "    # >>>>>>>>>>>>>>>>>> END OF CHANGES <<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "    logger.info(\"Dataset creation completed successfully!\")\n",
    "\n",
    "    # >>>>>>>>>>>>>>>>>> START OF CHANGES <<<<<<<<<<<<<<<<<<<<\n",
    "    # Change these lines for statistics\n",
    "    # From:\n",
    "    # logger.info(f\"Sample statistics:\")\n",
    "    # logger.info(f\"  - Min TPM value: {combined_df.values.min()}\")\n",
    "    # logger.info(f\"  - Max TPM value: {combined_df.values.max()}\")\n",
    "    # logger.info(f\"  - Mean TPM value: {combined_df.values.mean():.4f}\")\n",
    "    # logger.info(f\"  - Median TPM value: {float(pd.Series(combined_df.values.flatten()).median()):.4f}\")\n",
    "    #\n",
    "    # To:\n",
    "    tpm_values = combined_tpm_df.values.flatten()\n",
    "    logger.info(f\"Sample statistics (TPM values only):\")\n",
    "    logger.info(f\"   - Min TPM value: {tpm_values.min()}\")\n",
    "    logger.info(f\"   - Max TPM value: {tpm_values.max()}\")\n",
    "    logger.info(f\"   - Mean TPM value: {tpm_values.mean():.4f}\")\n",
    "    logger.info(f\"   - Median TPM value: {float(np.median(tpm_values)):.4f}\")\n",
    "    # >>>>>>>>>>>>>>>>>> END OF CHANGES <<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "def verify_dataset(parquet_file):\n",
    "    \"\"\"\n",
    "    Verify the created dataset by loading and checking basic properties.\n",
    "\n",
    "    Args:\n",
    "        parquet_file: Path to the parquet file to verify\n",
    "    \"\"\"\n",
    "    logger.info(\"Verifying created dataset...\")\n",
    "\n",
    "    # Load the dataset\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "\n",
    "    # Basic info\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATASET OVERVIEW\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Dataset shape and basic info\n",
    "    info_df = pd.DataFrame({\n",
    "        'Metric': ['Number of Genes (rows)', 'Number of Samples (columns)', 'Data Type', 'Memory Usage (MB)'],\n",
    "        'Value': [df.shape[0], df.shape[1], str(df.dtypes.iloc[0]), f\"{df.memory_usage(deep=True).sum() / 1024**2:.2f}\"]\n",
    "    })\n",
    "    display(info_df)\n",
    "\n",
    "    # Sample preview\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DATA PREVIEW\")\n",
    "    print(\"=\" * 60)\n",
    "    display(df.iloc[:10, :5])  # First 10 genes, first 5 samples\n",
    "\n",
    "    # Basic statistics\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXPRESSION STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    stats_df = pd.DataFrame({\n",
    "        'Statistic': ['Min TPM', 'Max TPM', 'Mean TPM', 'Median TPM', 'Std TPM',\n",
    "                     'Zero Values (%)', 'Non-zero Values (%)'],\n",
    "        'Value': [\n",
    "            f\"{df.values.min():.4f}\",\n",
    "            f\"{df.values.max():.4f}\",\n",
    "            f\"{df.values.mean():.4f}\",\n",
    "            f\"{np.median(df.values):.4f}\",\n",
    "            f\"{df.values.std():.4f}\",\n",
    "            f\"{(df.values == 0).sum() / df.size * 100:.2f}%\",\n",
    "            f\"{(df.values > 0).sum() / df.size * 100:.2f}%\"\n",
    "        ]\n",
    "    })\n",
    "    display(stats_df)\n",
    "\n",
    "\n",
    "def essential_plots_for_ml(parquet_file):\n",
    "    \"\"\"\n",
    "    Generate essential plots for breast cancer ML project preparation.\n",
    "\n",
    "    Args:\n",
    "        parquet_file: Path to the parquet file to analyze\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ESSENTIAL PLOTS FOR ML PROJECT\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Calculate basic statistics\n",
    "    gene_stats = pd.DataFrame({\n",
    "        'Mean_Expression': df.mean(axis=1),\n",
    "        'Zero_Percentage': (df == 0).sum(axis=1) / df.shape[1] * 100,\n",
    "        'CV': df.std(axis=1) / df.mean(axis=1)\n",
    "    })\n",
    "\n",
    "    sample_stats = pd.DataFrame({\n",
    "        'Total_Expression': df.sum(axis=0),\n",
    "        'Expressed_Genes': (df > 0).sum(axis=0)\n",
    "    })\n",
    "\n",
    "    # Create 4 essential plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Plot 1: Expression Distribution (Need for Log Transformation)\n",
    "    axes[0,0].hist(np.log10(df.values.flatten() + 1), bins=100, alpha=0.7, edgecolor='black')\n",
    "    axes[0,0].set_xlabel('Log10(TPM + 1)')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    axes[0,0].set_title('1. Expression Value Distribution\\n(Determines if log transform needed)')\n",
    "    axes[0,0].axvline(np.log10(1), color='red', linestyle='--', label='TPM = 1')\n",
    "    axes[0,0].legend()\n",
    "\n",
    "    # Plot 2: Gene Sparsity (Feature Filtering)\n",
    "    axes[0,1].hist(gene_stats['Zero_Percentage'], bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[0,1].set_xlabel('Percentage of Zero Values per Gene')\n",
    "    axes[0,1].set_ylabel('Number of Genes')\n",
    "    axes[0,1].set_title('2. Gene Sparsity Distribution\\n(Guides feature filtering thresholds)')\n",
    "    axes[0,1].axvline(50, color='red', linestyle='--', label='50% threshold')\n",
    "    axes[0,1].axvline(90, color='orange', linestyle='--', label='90% threshold')\n",
    "    axes[0,1].legend()\n",
    "\n",
    "    # Plot 3: Sample Quality (Outlier Detection)\n",
    "    axes[1,0].scatter(sample_stats['Total_Expression']/1000, sample_stats['Expressed_Genes'],\n",
    "                     alpha=0.6, s=30)\n",
    "    axes[1,0].set_xlabel('Total Expression (thousands TPM)')\n",
    "    axes[1,0].set_ylabel('Number of Expressed Genes')\n",
    "    axes[1,0].set_title('3. Sample Quality Check\\n(Outlier detection)')\n",
    "\n",
    "    # Add outlier boundaries\n",
    "    total_q1, total_q3 = sample_stats['Total_Expression'].quantile([0.25, 0.75])\n",
    "    genes_q1, genes_q3 = sample_stats['Expressed_Genes'].quantile([0.25, 0.75])\n",
    "    total_iqr = total_q3 - total_q1\n",
    "    genes_iqr = genes_q3 - genes_q1\n",
    "\n",
    "    axes[1,0].axvline((total_q1 - 1.5 * total_iqr)/1000, color='red', linestyle='--', alpha=0.5)\n",
    "    axes[1,0].axvline((total_q3 + 1.5 * total_iqr)/1000, color='red', linestyle='--', alpha=0.5)\n",
    "    axes[1,0].axhline(genes_q1 - 1.5 * genes_iqr, color='red', linestyle='--', alpha=0.5)\n",
    "    axes[1,0].axhline(genes_q3 + 1.5 * genes_iqr, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Plot 4: Gene Variability (Feature Selection)\n",
    "    # Filter out genes with very low expression for CV calculation\n",
    "    cv_data = gene_stats[(gene_stats['Mean_Expression'] > 1) & (gene_stats['CV'].notna())]['CV']\n",
    "    cv_filtered = cv_data[cv_data < 5]  # Remove extreme outliers for better visualization\n",
    "\n",
    "    axes[1,1].hist(cv_filtered, bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[1,1].set_xlabel('Coefficient of Variation (CV)')\n",
    "    axes[1,1].set_ylabel('Number of Genes')\n",
    "    axes[1,1].set_title('4. Gene Variability Distribution\\n(Identifies informative features)')\n",
    "    axes[1,1].axvline(1, color='red', linestyle='--', label='CV = 1')\n",
    "    axes[1,1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Summary statistics for decision making\n",
    "    print(\"\\nKEY STATISTICS FOR ML PREPROCESSING:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    summary_stats = pd.DataFrame({\n",
    "        'Metric': [\n",
    "            'Total genes',\n",
    "            'Genes with >50% zeros',\n",
    "            'Genes with >90% zeros',\n",
    "            'Low expression genes (mean < 1 TPM)',\n",
    "            'Highly variable genes (CV > 1)',\n",
    "            'Potential outlier samples'\n",
    "        ],\n",
    "        'Count': [\n",
    "            len(gene_stats),\n",
    "            (gene_stats['Zero_Percentage'] > 50).sum(),\n",
    "            (gene_stats['Zero_Percentage'] > 90).sum(),\n",
    "            (gene_stats['Mean_Expression'] < 1).sum(),\n",
    "            (gene_stats['CV'] > 1).sum(),\n",
    "            len(sample_stats[(sample_stats['Total_Expression'] < total_q1 - 1.5 * total_iqr) |\n",
    "                           (sample_stats['Total_Expression'] > total_q3 + 1.5 * total_iqr)])\n",
    "        ],\n",
    "        'Percentage': [\n",
    "            '100%',\n",
    "            f\"{(gene_stats['Zero_Percentage'] > 50).sum() / len(gene_stats) * 100:.1f}%\",\n",
    "            f\"{(gene_stats['Zero_Percentage'] > 90).sum() / len(gene_stats) * 100:.1f}%\",\n",
    "            f\"{(gene_stats['Mean_Expression'] < 1).sum() / len(gene_stats) * 100:.1f}%\",\n",
    "            f\"{(gene_stats['CV'] > 1).sum() / len(gene_stats) * 100:.1f}%\",\n",
    "            f\"{len(sample_stats[(sample_stats['Total_Expression'] < total_q1 - 1.5 * total_iqr) | (sample_stats['Total_Expression'] > total_q3 + 1.5 * total_iqr)]) / len(sample_stats) * 100:.1f}%\"\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    display(summary_stats)\n",
    "\n",
    "    return gene_stats, sample_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the files\n",
    "combine_gdc_files(TSV_DIRECTORY, METADATA_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify_dataset(OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate essential plots for ML preprocessing\n",
    "# gene_stats, sample_stats = essential_plots_for_ml(OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
