{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "\n",
    "def apply_tpm_normalization():\n",
    "    \"\"\"\n",
    "    Apply TPM normalization using your retrieved gene lengths.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"üß¨ TPM NORMALIZATION PIPELINE\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Step 1: Load the raw merged dataset\n",
    "    print(\"üìÇ Loading raw merged dataset...\")\n",
    "    merged_df = pd.read_parquet('data/merged_dataset.pq')\n",
    "\n",
    "    X = merged_df.drop(columns=['condition'])\n",
    "    y = merged_df['condition']\n",
    "\n",
    "    print(f\"‚úÖ Loaded dataset: {X.shape}\")\n",
    "    print(f\"   Value range: {X.min().min()} to {X.max().max():,}\")\n",
    "    print(f\"   Class distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "    # Step 2: Load gene lengths\n",
    "    print(\"\\nüìè Loading gene lengths...\")\n",
    "\n",
    "    #TODO: Update this to your actual gene lengths file from get_gene_lengths_mygene\n",
    "    gene_length_files = ''\n",
    "\n",
    "    gene_lengths = None\n",
    "    for filename in gene_length_files:\n",
    "        try:\n",
    "            gene_lengths_df = pd.read_csv(filename, index_col=0)\n",
    "            gene_lengths = gene_lengths_df.iloc[:, 0]  # First column should be lengths\n",
    "            print(f\"‚úÖ Loaded gene lengths from: {filename}\")\n",
    "            break\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "\n",
    "    if gene_lengths is None:\n",
    "        print(\"‚ùå Could not find gene lengths file. Please ensure it's saved as 'gene_lengths.csv'\")\n",
    "        print(\"Expected columns: gene_id (index), length\")\n",
    "        return None\n",
    "\n",
    "    print(f\"‚úÖ Gene lengths loaded: {len(gene_lengths)} genes\")\n",
    "    print(f\"   Length range: {gene_lengths.min():,.0f} - {gene_lengths.max():,.0f} bp\")\n",
    "    print(f\"   Median length: {gene_lengths.median():,.0f} bp\")\n",
    "\n",
    "    # Step 3: Align gene lengths with expression data\n",
    "    print(\"\\nüéØ Aligning gene lengths with expression data...\")\n",
    "\n",
    "    # Find common genes between expression data and gene lengths\n",
    "    common_genes = list(set(X.columns) & set(gene_lengths.index))\n",
    "    missing_genes = list(set(X.columns) - set(gene_lengths.index))\n",
    "\n",
    "    print(f\"   Common genes: {len(common_genes)}\")\n",
    "    print(f\"   Missing gene lengths: {len(missing_genes)}\")\n",
    "\n",
    "    if missing_genes:\n",
    "        print(f\"   First few missing: {missing_genes[:5]}\")\n",
    "\n",
    "        # Use median length for missing genes\n",
    "        median_length = gene_lengths.median()\n",
    "        print(f\"   Using median length ({median_length:,.0f} bp) for missing genes\")\n",
    "\n",
    "        for gene in missing_genes:\n",
    "            gene_lengths[gene] = median_length\n",
    "\n",
    "    # Subset to common genes and align order\n",
    "    gene_lengths_aligned = gene_lengths[X.columns]\n",
    "\n",
    "    print(f\"‚úÖ Aligned gene lengths: {len(gene_lengths_aligned)} genes\")\n",
    "\n",
    "    # Step 4: Calculate TPM\n",
    "    print(\"\\nüî¢ Calculating TPM...\")\n",
    "\n",
    "    # Convert gene lengths to kilobases\n",
    "    gene_lengths_kb = gene_lengths_aligned / 1000\n",
    "\n",
    "    print(f\"   Gene lengths in kb: {gene_lengths_kb.min():.2f} - {gene_lengths_kb.max():.2f}\")\n",
    "\n",
    "    # Calculate reads per kilobase (RPK)\n",
    "    print(\"   Calculating RPK (reads per kilobase)...\")\n",
    "    rpk = X.div(gene_lengths_kb, axis=1)\n",
    "\n",
    "    print(f\"   RPK range: {rpk.min().min():.2f} - {rpk.max().max():,.2f}\")\n",
    "\n",
    "    # Calculate scaling factor (total RPK per sample / 1M)\n",
    "    print(\"   Calculating scaling factors...\")\n",
    "    total_rpk_per_sample = rpk.sum(axis=1)\n",
    "    scaling_factors = total_rpk_per_sample / 1e6\n",
    "\n",
    "    print(f\"   Scaling factors range: {scaling_factors.min():.2f} - {scaling_factors.max():.2f}\")\n",
    "    print(f\"   Scaling factors mean: {scaling_factors.mean():.2f}\")\n",
    "\n",
    "    # Calculate TPM\n",
    "    print(\"   Calculating final TPM values...\")\n",
    "    tpm = rpk.div(scaling_factors, axis=0)\n",
    "\n",
    "    print(f\"‚úÖ TPM calculated!\")\n",
    "    print(f\"   TPM range: {tpm.min().min():.2f} - {tpm.max().max():,.2f}\")\n",
    "\n",
    "    # Verify TPM sums (should be ~1M per sample)\n",
    "    tpm_sums = tpm.sum(axis=1)\n",
    "    print(f\"   TPM sums per sample: {tpm_sums.min():,.0f} - {tpm_sums.max():,.0f}\")\n",
    "    print(f\"   Mean TPM sum: {tpm_sums.mean():,.0f} (should be ~1,000,000)\")\n",
    "\n",
    "    if abs(tpm_sums.mean() - 1e6) > 1000:\n",
    "        print(\"   ‚ö†Ô∏è Warning: TPM sums not close to 1M - check calculation\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ TPM sums look correct!\")\n",
    "\n",
    "    # Step 5: Log transformation\n",
    "    print(\"\\nüìà Applying log2 transformation...\")\n",
    "\n",
    "    # Add pseudocount to avoid log(0)\n",
    "    tpm_log = np.log2(tpm + 1)\n",
    "\n",
    "    print(f\"   Log2(TPM+1) range: {tpm_log.min().min():.2f} - {tpm_log.max().max():.2f}\")\n",
    "\n",
    "    # Step 6: Create final dataframe and save\n",
    "    print(\"\\nüíæ Saving TPM-normalized data...\")\n",
    "\n",
    "    # Add condition back\n",
    "    tpm_final = tpm_log.copy()\n",
    "    tpm_final['condition'] = y\n",
    "\n",
    "    # Save raw TPM (before log)\n",
    "    tpm_raw = tpm.copy()\n",
    "    tpm_raw['condition'] = y\n",
    "    tpm_raw.to_parquet('data/merged_dataset_tpm_raw.pq')\n",
    "\n",
    "    # Save log-transformed TPM\n",
    "    tpm_final.to_parquet('data/merged_dataset_tpm_normalized.pq')\n",
    "\n",
    "    # Save TPM calculation details for reference\n",
    "    tpm_info = {\n",
    "        'total_genes': len(X.columns),\n",
    "        'genes_with_lengths': len(common_genes),\n",
    "        'genes_missing_lengths': len(missing_genes),\n",
    "        'median_gene_length': gene_lengths.median(),\n",
    "        'mean_tpm_sum': tpm_sums.mean(),\n",
    "        'tpm_range': (tpm.min().min(), tpm.max().max()),\n",
    "        'log_tpm_range': (tpm_log.min().min(), tpm_log.max().max())\n",
    "    }\n",
    "\n",
    "    pd.Series(tpm_info).to_csv('data/tpm_normalization_info.csv')\n",
    "\n",
    "    print(f\"‚úÖ Files saved:\")\n",
    "    print(f\"   üìÅ Raw TPM: data/merged_dataset_tpm_raw.pq\")\n",
    "    print(f\"   üìÅ Log TPM: data/merged_dataset_tpm_normalized.pq\")\n",
    "    print(f\"   üìÅ TPM info: data/tpm_normalization_info.csv\")\n",
    "\n",
    "    # Step 7: Summary statistics\n",
    "    print(f\"\\nüìä FINAL TPM SUMMARY:\")\n",
    "    print(f\"   Dataset shape: {tpm_final.shape}\")\n",
    "    print(f\"   Genes processed: {len(X.columns):,}\")\n",
    "    print(f\"   Samples: {len(X):,}\")\n",
    "    print(f\"   Class distribution: {y.value_counts().to_dict()}\")\n",
    "    print(f\"   Memory usage: {tpm_final.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "    # Cleanup\n",
    "    del merged_df, X, rpk, tpm, tpm_raw\n",
    "    gc.collect()\n",
    "\n",
    "    return tpm_final, gene_lengths_aligned, tpm_info\n"
   ],
   "id": "4eb92da946326994"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_gene_lengths_mygene(gene_ids):\n",
    "    \"\"\"Get gene lengths using MyGene API.\"\"\"\n",
    "\n",
    "    import mygene\n",
    "    mg = mygene.MyGeneInfo()\n",
    "\n",
    "    print(f\"Getting lengths for {len(gene_ids)} genes...\")\n",
    "\n",
    "    # Query MyGene for gene lengths\n",
    "    results = mg.querymany(\n",
    "        gene_ids,\n",
    "        scopes='ensembl.gene',\n",
    "        fields='genomic_pos',  # Contains start/end positions\n",
    "        species='human',\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    gene_lengths = {}\n",
    "    default_fallback_genes = {}\n",
    "    for result in results:\n",
    "        gene_id = result['query']\n",
    "\n",
    "        if 'genomic_pos' in result:\n",
    "            # Calculate length from genomic positions\n",
    "            pos_info = result['genomic_pos']\n",
    "            if isinstance(pos_info, list):\n",
    "                pos_info = pos_info[0]  # Take first if multiple\n",
    "\n",
    "            if 'start' in pos_info and 'end' in pos_info:\n",
    "                length = abs(pos_info['end'] - pos_info['start'])\n",
    "                gene_lengths[gene_id] = length\n",
    "            else:\n",
    "                default_fallback_genes[gene_id] = pos_info\n",
    "                gene_lengths[gene_id] = 10000  # Default fallback\n",
    "        else:\n",
    "            default_fallback_genes = {gene_id: None}\n",
    "            gene_lengths[gene_id] = 10000  # Default fallback\n",
    "\n",
    "    print(f\"Found lengths for {len(gene_lengths)} genes.\")\n",
    "    if default_fallback_genes:\n",
    "        print(f\"Using default length for {len(default_fallback_genes)} genes: 10,000 bp\")\n",
    "    return pd.Series(gene_lengths)\n",
    "\n",
    "## Example usage:\n",
    "# TODO: Update this to your actual gene IDs\n",
    "if __name__ == \"__main__\":\n",
    "    # Load in dataset and get all gene IDs\n",
    "    # merged_df = pd.read_parquet('data/merged_dataset.pq')\n",
    "    # gene_ids = merged_df.drop(columns=['condition']).columns.tolist()\n"
   ],
   "id": "fd65adc3d16151b6"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
