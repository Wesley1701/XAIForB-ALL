{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Base methods and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_as_df(file_path):\n",
    "    \"\"\"\n",
    "    Loads a parquet file in true chunks to avoid memory issues.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the parquet file.\n",
    "        chunk_size (int): The number of rows per chunk.\n",
    "\n",
    "    Yields:\n",
    "        pandas.DataFrame: DataFrame chunks of the specified size.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Error: File not found at {file_path}\")\n",
    "            return None\n",
    "\n",
    "        return pd.read_parquet(file_path, engine='pyarrow')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading parquet file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    finally:\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def save_df_as_parquet(dataframe, output_parquet_path, preserve_index=True):\n",
    "    \"\"\"\n",
    "    Save a DataFrame as parquet using PyArrow for maximum efficiency.\n",
    "    Handles index preservation and duplicate detection.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): DataFrame to save\n",
    "        output_parquet_path (str): Path to save the parquet file\n",
    "        preserve_index (bool): Whether to preserve the DataFrame index\n",
    "    \"\"\"\n",
    "\n",
    "    if dataframe.empty:\n",
    "        print(\"No data to save!\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        if preserve_index and dataframe.index.name is not None:\n",
    "            duplicate_indices = dataframe.index.duplicated()\n",
    "            if duplicate_indices.any():\n",
    "                num_duplicates = duplicate_indices.sum()\n",
    "                print(f\"Warning: Found {num_duplicates} duplicate index values\")\n",
    "                duplicate_values = dataframe.index[duplicate_indices].unique()[:5]\n",
    "                print(f\"First few duplicates: {list(duplicate_values)}\")\n",
    "\n",
    "        table = pa.Table.from_pandas(dataframe, preserve_index=preserve_index)\n",
    "\n",
    "        if preserve_index and dataframe.index.name:\n",
    "            print(f\"Preserving index: '{dataframe.index.name}' (dtype: {dataframe.index.dtype})\")\n",
    "\n",
    "        pq.write_table(table, output_parquet_path)\n",
    "\n",
    "        print(f\"Successfully saved {len(dataframe)} rows to parquet\")\n",
    "        if preserve_index:\n",
    "            unique_index_values = len(dataframe.index.unique())\n",
    "            print(f\"Total unique index values: {unique_index_values}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving DataFrame to parquet: {e}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        # Clean up\n",
    "        if 'table' in locals():\n",
    "            del table\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def transpose_df(dataframe, skip_rows=None, skip_columns=None, dtype='float32'):\n",
    "    \"\"\"\n",
    "    Transpose a DataFrame with optional row/column filtering.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): DataFrame to transpose\n",
    "        skip_rows: List of row indices/names to skip, or None\n",
    "        skip_columns: List of column indices/names to skip, or None\n",
    "        dtype: Data type for the transposed data (default: 'float32')\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Transposed DataFrame with sample_id column\n",
    "    \"\"\"\n",
    "\n",
    "    if dataframe.empty:\n",
    "        print(\"No data to transpose!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(\"Starting DataFrame transpose...\")\n",
    "\n",
    "    df_filtered = dataframe.copy()\n",
    "\n",
    "    original_rows, original_cols = df_filtered.shape\n",
    "\n",
    "    if skip_columns is not None:\n",
    "        columns_to_keep = [col for col in df_filtered.columns if col not in skip_columns]\n",
    "        df_filtered = df_filtered[columns_to_keep]\n",
    "        print(f\"Skipping columns: {skip_columns}\")\n",
    "\n",
    "    if skip_rows is not None:\n",
    "        if skip_rows and isinstance(skip_rows[0], str):\n",
    "            rows_to_keep = [idx for idx in df_filtered.index if idx not in skip_rows]\n",
    "        else:\n",
    "            rows_to_keep = [idx for i, idx in enumerate(df_filtered.index) if i not in skip_rows]\n",
    "\n",
    "        df_filtered = df_filtered.loc[rows_to_keep]\n",
    "        print(f\"Skipping rows: {skip_rows}\")\n",
    "\n",
    "    filtered_rows, filtered_cols = df_filtered.shape\n",
    "\n",
    "    print(f\"Dataset dimensions:\")\n",
    "    print(f\"    Original: {original_rows} rows x {original_cols} columns\")\n",
    "    print(f\"    After filtering: {filtered_rows} rows x {filtered_cols} columns\")\n",
    "    print(f\"    After transpose: {filtered_cols} rows x {filtered_rows} columns\")\n",
    "\n",
    "    try:\n",
    "        print(\"Converting data types and transposing...\")\n",
    "        df_numeric = df_filtered.astype(dtype)\n",
    "        transposed_df = df_numeric.T\n",
    "\n",
    "        final_df = pd.DataFrame(\n",
    "            data=transposed_df.values,\n",
    "            index=transposed_df.index,\n",
    "            columns=transposed_df.columns\n",
    "        )\n",
    "\n",
    "        # Reset index to create sample_id column\n",
    "        final_df = final_df.reset_index().rename(columns={'index': 'sample_id'})\n",
    "\n",
    "        print(f\"Transposition completed successfully!\")\n",
    "        print(f\"Final DataFrame shape: {final_df.shape}\")\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transposition: {e}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        # Clean up\n",
    "        if 'df_filtered' in locals():\n",
    "            del df_filtered\n",
    "        if 'df_numeric' in locals():\n",
    "            del df_numeric\n",
    "        if 'transposed_df' in locals():\n",
    "            del transposed_df\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "# def convert_gene_ids_to_symbols(dataset_chunk_iterator, mygene_client, gene_column_prefix=\"ENSG\"):\n",
    "#     \"\"\"\n",
    "#     Convert gene IDs to gene symbols using MyGene API in a memory-efficient way.\n",
    "#     Only calls MyGene API once for all gene columns from the first chunk.\n",
    "\n",
    "#     Args:\n",
    "#         dataset_chunk_iterator: Iterator yielding DataFrame chunks\n",
    "#         mygene_client: MyGene client instance\n",
    "#         gene_column_prefix (str): Prefix to identify gene columns\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: Complete dataset with gene symbols as column names\n",
    "#     \"\"\"\n",
    "#     print(\"Converting gene IDs to symbols (chunked processing)...\")\n",
    "\n",
    "#     # Get first chunk to determine gene columns and create mapping\n",
    "#     try:\n",
    "#         first_chunk = next(dataset_chunk_iterator)\n",
    "#         print(f\"Processing first chunk: {first_chunk.shape}\")\n",
    "#     except StopIteration:\n",
    "#         print(\"‚ùå Error: Dataset is empty\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     # Identify gene columns from first chunk\n",
    "#     gene_columns = [col for col in first_chunk.columns\n",
    "#                    if isinstance(col, str) and col.startswith(gene_column_prefix)]\n",
    "\n",
    "#     if not gene_columns:\n",
    "#         print(\"No gene columns found - returning original data\")\n",
    "#         # Concatenate all chunks and return\n",
    "#         all_chunks = [first_chunk]\n",
    "#         for chunk in dataset_chunk_iterator:\n",
    "#             all_chunks.append(chunk)\n",
    "#         return pd.concat(all_chunks, axis=0, ignore_index=False)\n",
    "\n",
    "#     print(f\"Found {len(gene_columns)} gene columns\")\n",
    "\n",
    "#     # Single MyGene API call for all gene IDs\n",
    "#     gene_id_to_symbol = {}\n",
    "#     symbols_found = 0\n",
    "\n",
    "#     try:\n",
    "#         print(\"Making single MyGene API call...\")\n",
    "#         import warnings\n",
    "#         with warnings.catch_warnings():\n",
    "#             warnings.simplefilter(\"ignore\")\n",
    "#             results = mygene_client.querymany(\n",
    "#                 gene_columns,\n",
    "#                 scopes='ensembl.gene',\n",
    "#                 fields='symbol',\n",
    "#                 species='human',\n",
    "#                 verbose=False,\n",
    "#                 silent=True\n",
    "#             )\n",
    "\n",
    "#         # Process API results\n",
    "#         for result in results:\n",
    "#             gene_id = result['query']\n",
    "#             if 'symbol' in result and result['symbol']:\n",
    "#                 gene_id_to_symbol[gene_id] = result['symbol']\n",
    "#                 symbols_found += 1\n",
    "#             else:\n",
    "#                 gene_id_to_symbol[gene_id] = gene_id\n",
    "\n",
    "#         print(f\"‚úÖ Successfully converted {symbols_found}/{len(gene_columns)} genes to symbols\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ö†Ô∏è MyGene API error: {e}. Using original gene IDs\")\n",
    "#         gene_id_to_symbol = {gene_id: gene_id for gene_id in gene_columns}\n",
    "\n",
    "#     # Create final column mapping (preserve non-gene columns like 'condition')\n",
    "#     final_column_mapping = {}\n",
    "#     for col in first_chunk.columns:\n",
    "#         if col == 'condition':\n",
    "#             final_column_mapping[col] = col\n",
    "#         elif col in gene_id_to_symbol:\n",
    "#             final_column_mapping[col] = gene_id_to_symbol[col]\n",
    "#         else:\n",
    "#             final_column_mapping[col] = col\n",
    "\n",
    "#     print(\"Applying gene symbol mapping to all chunks...\")\n",
    "\n",
    "#     # Process first chunk\n",
    "#     renamed_first_chunk = first_chunk.rename(columns=final_column_mapping)\n",
    "#     processed_chunks = [renamed_first_chunk]\n",
    "#     chunk_count = 1\n",
    "\n",
    "#     # Process remaining chunks with same mapping\n",
    "#     for chunk in dataset_chunk_iterator:\n",
    "#         renamed_chunk = chunk.rename(columns=final_column_mapping)\n",
    "#         processed_chunks.append(renamed_chunk)\n",
    "#         chunk_count += 1\n",
    "\n",
    "#         if chunk_count % 3 == 0:\n",
    "#             print(f\"  ‚úì Processed {chunk_count} chunks...\")\n",
    "\n",
    "#     print(f\"Concatenating {chunk_count} processed chunks...\")\n",
    "#     final_dataset = pd.concat(processed_chunks, axis=0, ignore_index=False)\n",
    "\n",
    "#     print(f\"‚úÖ Final dataset shape: {final_dataset.shape}\")\n",
    "#     print(f\"   Symbols converted: {symbols_found}/{len(gene_columns)}\")\n",
    "\n",
    "#     return final_dataset\n",
    "\n",
    "\n",
    "def add_condition_labels(dataframe, condition_label, dataset_name):\n",
    "    \"\"\"\n",
    "    Add condition labels to a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): DataFrame to add labels to\n",
    "        condition_label (int): Binary label (0 for healthy, 1 for unhealthy)\n",
    "        dataset_name (str): Name for logging purposes\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with condition column added\n",
    "    \"\"\"\n",
    "    if dataframe.empty:\n",
    "        print(f\"Warning: {dataset_name} dataset is empty!\")\n",
    "        return dataframe\n",
    "\n",
    "    labeled_df = dataframe.copy()\n",
    "\n",
    "    labeled_df['condition'] = condition_label\n",
    "    labeled_df['condition'] = labeled_df['condition'].astype(np.int8)\n",
    "\n",
    "    print(f\"‚úÖ Successfully added condition label to {len(labeled_df)} samples in {dataset_name} dataset\")\n",
    "\n",
    "    return labeled_df\n",
    "\n",
    "\n",
    "def merge_labeled_datasets(healthy_dataframe, unhealthy_dataframe):\n",
    "    \"\"\"\n",
    "    Merge healthy and unhealthy dataset chunks into one DataFrame.\n",
    "\n",
    "    Args:\n",
    "        healthy_chunks (list): List of healthy DataFrame chunks\n",
    "        unhealthy_chunks (list): List of unhealthy DataFrame chunks\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Merged dataset\n",
    "    \"\"\"\n",
    "    if healthy_dataframe.empty and unhealthy_dataframe.empty:\n",
    "        print(\"Warning: Both datasets are empty!\")\n",
    "        return\n",
    "\n",
    "    if healthy_dataframe.empty:\n",
    "        print(\"Warning: Healthy dataset is empty, returning only unhealthy data\")\n",
    "        return\n",
    "    elif unhealthy_dataframe.empty:\n",
    "        print(\"Warning: Unhealthy dataset is empty, returning only healthy data\")\n",
    "        return\n",
    "    else:\n",
    "        merged_dataset = pd.concat([healthy_dataframe, unhealthy_dataframe], axis=0, ignore_index=False)\n",
    "\n",
    "    print(f\"‚úÖ Merged dataset: {len(merged_dataset)} samples, {len(merged_dataset.columns)} features\")\n",
    "\n",
    "    if 'condition' in merged_dataset.columns:\n",
    "        print(f\"   Healthy (0): {(merged_dataset['condition'] == 0).sum()}\")\n",
    "        print(f\"   Unhealthy (1): {(merged_dataset['condition'] == 1).sum()}\")\n",
    "    else:\n",
    "        print(\"   Note: No 'condition' column found for condition counting\")\n",
    "\n",
    "    return merged_dataset\n",
    "\n",
    "\n",
    "def clean_duplicate_nans(dataframe):\n",
    "    \"\"\"\n",
    "    Process a DataFrame to drop duplicates and NaNs.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): DataFrame to clean\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame with duplicates and NaNs removed\n",
    "    \"\"\"\n",
    "    print(\"Starting data cleaning (removing NaNs and duplicates)...\")\n",
    "\n",
    "    if dataframe.empty:\n",
    "        print(\"Warning: DataFrame is empty!\")\n",
    "        return dataframe\n",
    "\n",
    "    original_rows = len(dataframe)\n",
    "    print(f\"Original DataFrame shape: {dataframe.shape}\")\n",
    "\n",
    "    cleaned_df = dataframe.copy()\n",
    "\n",
    "    cleaned_df = cleaned_df.dropna()\n",
    "    rows_after_nan_drop = len(cleaned_df)\n",
    "\n",
    "    if rows_after_nan_drop < original_rows:\n",
    "        nan_dropped = original_rows - rows_after_nan_drop\n",
    "        print(f\"Dropped {nan_dropped} rows with null values\")\n",
    "    else:\n",
    "        print(\"No null values found\")\n",
    "\n",
    "    if cleaned_df.empty:\n",
    "        print(\"Warning: DataFrame is empty after dropping NaNs!\")\n",
    "        return cleaned_df\n",
    "\n",
    "    cleaned_df = cleaned_df.drop_duplicates()\n",
    "    final_rows = len(cleaned_df)\n",
    "\n",
    "    if final_rows < rows_after_nan_drop:\n",
    "        duplicates_dropped = rows_after_nan_drop - final_rows\n",
    "        print(f\"Dropped {duplicates_dropped} duplicate rows\")\n",
    "    else:\n",
    "        print(\"No duplicate rows found\")\n",
    "\n",
    "    if cleaned_df.empty:\n",
    "        print(\"Warning: DataFrame is empty after removing duplicates!\")\n",
    "        return cleaned_df\n",
    "\n",
    "    total_removed = original_rows - final_rows\n",
    "    print(f\"‚úÖ Cleaning complete: {final_rows} rows remaining ({total_removed} total rows removed)\")\n",
    "    print(f\"Final DataFrame shape: {cleaned_df.shape}\")\n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "def align_gene_columns(healthy_dataframe, unhealthy_dataframe, gene_column_prefix=\"ENSG\"):\n",
    "    \"\"\"\n",
    "    Align gene columns between healthy and unhealthy datasets.\n",
    "\n",
    "    Args:\n",
    "        healthy_dataframe (pd.DataFrame): Healthy dataset\n",
    "        unhealthy_dataframe (pd.DataFrame): Unhealthy dataset\n",
    "        gene_column_prefix (str): Prefix to identify gene columns (default: \"ENSG\")\n",
    "\n",
    "    Returns:\n",
    "        tuple: (aligned_healthy_df, aligned_unhealthy_df) with matching columns\n",
    "    \"\"\"\n",
    "    print(\"Starting gene column alignment...\")\n",
    "\n",
    "    if healthy_dataframe.empty or unhealthy_dataframe.empty:\n",
    "        print(\"ERROR: One or both datasets are empty!\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    print(\"\\nProcessing gene columns and stripping version suffixes...\")\n",
    "\n",
    "    def extract_gene_info(dataframe, dataset_name):\n",
    "        \"\"\"Extract gene columns and create mapping from original_column to base_id\"\"\"\n",
    "        print(f\"Processing {dataset_name} dataset...\")\n",
    "\n",
    "        original_to_base = {}\n",
    "        base_gene_ids = set()\n",
    "        total_gene_columns = 0\n",
    "        duplicate_count = 0\n",
    "\n",
    "        for column in dataframe.columns:\n",
    "            if isinstance(column, str) and column.startswith(gene_column_prefix):\n",
    "                total_gene_columns += 1\n",
    "                base_gene_id = column.split('.')[0]\n",
    "\n",
    "                if base_gene_id not in base_gene_ids:\n",
    "                    original_to_base[column] = base_gene_id\n",
    "                    base_gene_ids.add(base_gene_id)\n",
    "                else:\n",
    "                    duplicate_count += 1\n",
    "                    print(f\"Warning: Duplicate base gene {base_gene_id} found, skipping {column}\")\n",
    "\n",
    "        print(f\"Total gene columns found: {total_gene_columns}\")\n",
    "        print(f\"Unique base gene IDs: {len(base_gene_ids)}\")\n",
    "        if duplicate_count > 0:\n",
    "            print(f\"Duplicates removed: {duplicate_count}\")\n",
    "\n",
    "        return original_to_base, base_gene_ids\n",
    "\n",
    "    healthy_rename_mapping, healthy_base_genes = extract_gene_info(healthy_dataframe, \"HEALTHY\")\n",
    "    unhealthy_rename_mapping, unhealthy_base_genes = extract_gene_info(unhealthy_dataframe, \"UNHEALTHY\")\n",
    "\n",
    "    common_base_genes = healthy_base_genes & unhealthy_base_genes\n",
    "\n",
    "    if not common_base_genes:\n",
    "        print(\"ERROR: No common genes found between datasets!\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    print(f\"Common genes: {len(common_base_genes)}\")\n",
    "    print(f\"Genes exclusive to healthy dataset: {len(healthy_base_genes - common_base_genes)}\")\n",
    "    print(f\"Genes exclusive to unhealthy dataset: {len(unhealthy_base_genes - common_base_genes)}\")\n",
    "\n",
    "    renamed_healthy = healthy_dataframe.rename(columns=healthy_rename_mapping)\n",
    "    renamed_unhealthy = unhealthy_dataframe.rename(columns=unhealthy_rename_mapping)\n",
    "\n",
    "    healthy_non_gene_cols = [col for col in renamed_healthy.columns\n",
    "                            if not (isinstance(col, str) and col.startswith(gene_column_prefix))]\n",
    "    unhealthy_non_gene_cols = [col for col in renamed_unhealthy.columns\n",
    "                              if not (isinstance(col, str) and col.startswith(gene_column_prefix))]\n",
    "\n",
    "    if set(healthy_non_gene_cols) != set(unhealthy_non_gene_cols):\n",
    "        print(\"Warning: Non-gene columns differ between datasets!\")\n",
    "        print(f\"Healthy only: {set(healthy_non_gene_cols) - set(unhealthy_non_gene_cols)}\")\n",
    "        print(f\"Unhealthy only: {set(unhealthy_non_gene_cols) - set(healthy_non_gene_cols)}\")\n",
    "\n",
    "    non_gene_cols = healthy_non_gene_cols\n",
    "    common_gene_base_ids = sorted(common_base_genes)\n",
    "    final_columns = non_gene_cols + common_gene_base_ids\n",
    "\n",
    "    print(\"\\nAligning datasets to common columns...\")\n",
    "\n",
    "    try:\n",
    "        aligned_healthy = renamed_healthy[final_columns].copy()\n",
    "        aligned_unhealthy = renamed_unhealthy[final_columns].copy()\n",
    "    except KeyError as e:\n",
    "        print(f\"Error selecting columns: {e}\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    print(f\"‚úÖ Alignment complete!\")\n",
    "    print(f\"   Final shape - Healthy: {aligned_healthy.shape}, Unhealthy: {aligned_unhealthy.shape}\")\n",
    "    print(f\"   Final columns: {len(final_columns)} ({len(non_gene_cols)} non-gene + {len(common_gene_base_ids)} gene)\")\n",
    "\n",
    "    return aligned_healthy, aligned_unhealthy\n",
    "\n",
    "\n",
    "# def prepare_metadata_generator(aligned_pair_iterator, output_metadata_path=\"data/merged_metadata.pq\"):\n",
    "#     \"\"\"\n",
    "#     Generator that creates metadata from aligned chunk pairs and saves to parquet.\n",
    "\n",
    "#     Args:\n",
    "#         aligned_chunk_pairs_generator: Generator yielding (healthy_chunk, unhealthy_chunk) tuples\n",
    "#         output_metadata_path: Path to save metadata parquet file\n",
    "\n",
    "#     Yields:\n",
    "#         tuple: (healthy_chunk, unhealthy_chunk) - passes through the original chunks\n",
    "#     \"\"\"\n",
    "\n",
    "#     all_metadata = []\n",
    "\n",
    "#     for healthy_chunk, unhealthy_chunk in aligned_pair_iterator:\n",
    "#         chunk_metadata = []\n",
    "\n",
    "#         if healthy_chunk is not None:\n",
    "#             healthy_sample_ids = [str(sid) for sid in healthy_chunk.index.tolist()]\n",
    "#             healthy_metadata = [{'sample_id': sid, 'condition': 0} for sid in healthy_sample_ids]\n",
    "#             chunk_metadata.extend(healthy_metadata)\n",
    "\n",
    "#         if unhealthy_chunk is not None:\n",
    "#             unhealthy_sample_ids = [str(sid) for sid in unhealthy_chunk.index.tolist()]\n",
    "#             unhealthy_metadata = [{'sample_id': sid, 'condition': 1} for sid in unhealthy_sample_ids]\n",
    "#             chunk_metadata.extend(unhealthy_metadata)\n",
    "\n",
    "#         all_metadata.extend(chunk_metadata)\n",
    "\n",
    "#         # Yield the chunks unchanged (pass-through)\n",
    "#         yield healthy_chunk, unhealthy_chunk\n",
    "#         del healthy_chunk, unhealthy_chunk\n",
    "\n",
    "#     if all_metadata:\n",
    "#         metadata_df = pd.DataFrame(all_metadata)\n",
    "#         metadata_df.to_parquet(output_metadata_path, index=False)\n",
    "#         print(f\"Metadata saved to {output_metadata_path} with {len(metadata_df)} records\")\n",
    "#     else:\n",
    "#         print(\"No metadata to save\")\n",
    "\n",
    "#     # Clean up\n",
    "#     if 'all_metadata' in locals() or 'all_metadata' in globals():\n",
    "#         del all_metadata\n",
    "#     if 'metadata_df' in locals() or 'metadata_df' in globals():\n",
    "#         del metadata_df\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Unhealthy Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthy_dataset_file = 'data/unhealthy_data.pq'\n",
    "unhealthy_output_file = 'data/unhealthy_data_preprocessed.pq'\n",
    "\n",
    "unhealthy_df = load_parquet_as_df(unhealthy_dataset_file)\n",
    "print(f\"Starting preprocessing for Unhealthy Dataset: {unhealthy_dataset_file}...\")\n",
    "\n",
    "unhealthy_df = transpose_df(unhealthy_df, dtype='float32')\n",
    "unhealthy_df.index.name = 'sample_id'\n",
    "unhealthy_df.set_index('sample_id', drop=True, inplace=True)\n",
    "unhealthy_df = clean_duplicate_nans(unhealthy_df)\n",
    "unhealthy_df = add_condition_labels(unhealthy_df, condition_label=1, dataset_name='Unhealthy')\n",
    "\n",
    "save_df_as_parquet(unhealthy_df, unhealthy_output_file)\n",
    "\n",
    "# Clean up\n",
    "if 'unhealthy_df' in locals():\n",
    "    del unhealthy_df\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Display unhealthy dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthy_df = load_parquet_as_df('data/unhealthy_data_preprocessed.pq')\n",
    "\n",
    "print(f\"Shape: {unhealthy_df.shape}\")\n",
    "print(f\"Columns: {list(unhealthy_df.columns)}\")\n",
    "print(f\"Data types:\\n{unhealthy_df.dtypes}\")\n",
    "print(f\"Memory usage: {unhealthy_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "display(unhealthy_df.head())\n",
    "\n",
    "# Clean up\n",
    "if 'unhealthy_df' in locals():\n",
    "    del unhealthy_df\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Healthy Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_dataset_file = 'data/healthy_data.pq'\n",
    "healthy_output_file = 'data/healthy_data_preprocessed.pq'\n",
    "chunk_size = 1000\n",
    "\n",
    "healthy_df = load_parquet_as_df(healthy_dataset_file)\n",
    "print(f\"Starting preprocessing for Unhealthy Dataset: {healthy_dataset_file}...\")\n",
    "\n",
    "healthy_df = transpose_df(healthy_df, dtype='float32')\n",
    "healthy_df.index.name = 'sample_id'\n",
    "healthy_df.set_index('sample_id', drop=True, inplace=True)\n",
    "healthy_df = clean_duplicate_nans(healthy_df)\n",
    "healthy_df = add_condition_labels(healthy_df, condition_label=0, dataset_name='Healthy')\n",
    "\n",
    "save_df_as_parquet(healthy_df, healthy_output_file)\n",
    "\n",
    "# Clean up\n",
    "if 'healthy_df' in locals():\n",
    "    del healthy_df\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Display healthy dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_df = load_parquet_as_df('data/healthy_data_preprocessed.pq')\n",
    "\n",
    "print(f\"Shape: {healthy_df.shape}\")\n",
    "print(f\"Columns: {list(healthy_df.columns)}\")\n",
    "print(f\"Data types:\\n{healthy_df.dtypes}\")\n",
    "print(f\"Memory usage: {healthy_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "display(healthy_df.head())\n",
    "\n",
    "# Clean up\n",
    "if 'healthy_df' in locals():\n",
    "    del healthy_df\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_data_path = 'data/healthy_data_preprocessed.pq'\n",
    "unhealthy_data_path = 'data/unhealthy_data_preprocessed.pq'\n",
    "merged_data_path = 'data/merged_dataset.pq'\n",
    "\n",
    "healthy_df = load_parquet_as_df(healthy_data_path)\n",
    "unhealthy_df = load_parquet_as_df(unhealthy_data_path)\n",
    "\n",
    "healthy_df, unhealthy_df = align_gene_columns(healthy_df, unhealthy_df)\n",
    "merged_df = merge_labeled_datasets(healthy_df, unhealthy_df)\n",
    "\n",
    "save_df_as_parquet(merged_df, merged_data_path)\n",
    "\n",
    "# Clean up\n",
    "if 'healthy_df' in locals() or 'healthy_df' in globals():\n",
    "    del healthy_df\n",
    "if 'unhealthy_df' in locals() or 'unhealthy_df' in globals():\n",
    "    del unhealthy_df\n",
    "if 'merged_df' in locals() or 'merged_df' in globals():\n",
    "    del merged_df\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Display basic merged dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = load_parquet_as_df('data/merged_dataset.pq')\n",
    "\n",
    "print(f\"Shape: {merged_df.shape}\")\n",
    "print(f\"Columns: {list(merged_df.columns)}\")\n",
    "print(f\"Data types:\\n{merged_df.dtypes}\")\n",
    "print(f\"Memory usage: {merged_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\nFirst and Last 5 rows:\")\n",
    "display(merged_df)\n",
    "\n",
    "# Clean up\n",
    "if 'merged_df' in locals():\n",
    "    del merged_df\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_split(merged_dataset_path, output_dir=\"data/splits\",\n",
    "                                 test_size=0.2, random_state=42, create_validation=False):\n",
    "    \"\"\"\n",
    "    Create train-test splits from the merged dataset (before feature selection).\n",
    "\n",
    "    Args:\n",
    "        merged_dataset_path (str): Path to merged dataset (with condition column)\n",
    "        output_dir (str): Directory to save split datasets\n",
    "        test_size (float): Proportion of data for test set (default: 0.2)\n",
    "        random_state (int): Random state for reproducibility\n",
    "        create_validation (bool): Whether to create a validation set from training data\n",
    "\n",
    "    Returns:\n",
    "        dict: Paths to saved split files\n",
    "    \"\"\"\n",
    "    print(\"Loading merged dataset for early train-test splitting...\")\n",
    "\n",
    "    # Load merged dataset\n",
    "    df = pd.read_parquet(merged_dataset_path)\n",
    "\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "    # Check if condition column exists\n",
    "    if 'condition' not in df.columns:\n",
    "        raise ValueError(\"Condition column not found in merged dataset!\")\n",
    "\n",
    "    print(f\"Condition distribution: {df['condition'].value_counts()}\")\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Separate features and labels\n",
    "    X = df.drop('condition', axis=1)  # Remove condition column for features\n",
    "    y = df['condition']\n",
    "\n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "    # Initial train-test split (stratified)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTrain-Test Split Results:\")\n",
    "    print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"Training condition distribution: {y_train.value_counts()}\")\n",
    "    print(f\"Test condition distribution: {y_test.value_counts()}\")\n",
    "\n",
    "    # Create validation set if requested\n",
    "    if create_validation:\n",
    "        X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "            X_train, y_train,\n",
    "            test_size=0.25,  # 25% of training = 20% of total (if test_size=0.2)\n",
    "            random_state=random_state,\n",
    "            stratify=y_train\n",
    "        )\n",
    "\n",
    "        print(f\"\\nWith Validation Set:\")\n",
    "        print(f\"Final training: {X_train_final.shape[0]} samples ({X_train_final.shape[0]/len(X)*100:.1f}%)\")\n",
    "        print(f\"Validation: {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "        print(f\"Test: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "    # Save the splits\n",
    "    file_paths = {}\n",
    "\n",
    "    if create_validation:\n",
    "        # Save with validation split\n",
    "        X_train_final.to_parquet(f\"{output_dir}/X_train_raw.pq\", index=True)\n",
    "        X_val.to_parquet(f\"{output_dir}/X_val_raw.pq\", index=True)\n",
    "        X_test.to_parquet(f\"{output_dir}/X_test_raw.pq\", index=True)\n",
    "\n",
    "        y_train_final.to_frame('condition').to_parquet(f\"{output_dir}/y_train.pq\", index=True)\n",
    "        y_val.to_frame('condition').to_parquet(f\"{output_dir}/y_val.pq\", index=True)\n",
    "        y_test.to_frame('condition').to_parquet(f\"{output_dir}/y_test.pq\", index=True)\n",
    "\n",
    "        file_paths = {\n",
    "            'X_train': f\"{output_dir}/X_train_raw.pq\",\n",
    "            'X_val': f\"{output_dir}/X_val_raw.pq\",\n",
    "            'X_test': f\"{output_dir}/X_test_raw.pq\",\n",
    "            'y_train': f\"{output_dir}/y_train.pq\",\n",
    "            'y_val': f\"{output_dir}/y_val.pq\",\n",
    "            'y_test': f\"{output_dir}/y_test.pq\"\n",
    "        }\n",
    "\n",
    "        print(f\"\\nSaved train/validation/test splits to {output_dir}/\")\n",
    "\n",
    "        # Create visualization with validation set\n",
    "        create_split_visualization(X_train_final, X_test, y_train_final, y_test, X_val, y_val)\n",
    "\n",
    "    else:\n",
    "        # Save without validation split\n",
    "        X_train.to_parquet(f\"{output_dir}/X_train_raw.pq\", index=True)\n",
    "        X_test.to_parquet(f\"{output_dir}/X_test_raw.pq\", index=True)\n",
    "\n",
    "        y_train.to_frame('condition').to_parquet(f\"{output_dir}/y_train.pq\", index=True)\n",
    "        y_test.to_frame('condition').to_parquet(f\"{output_dir}/y_test.pq\", index=True)\n",
    "\n",
    "        file_paths = {\n",
    "            'X_train': f\"{output_dir}/X_train_raw.pq\",\n",
    "            'X_test': f\"{output_dir}/X_test_raw.pq\",\n",
    "            'y_train': f\"{output_dir}/y_train.pq\",\n",
    "            'y_test': f\"{output_dir}/y_test.pq\"\n",
    "        }\n",
    "\n",
    "        print(f\"\\nSaved train/test splits to {output_dir}/\")\n",
    "\n",
    "        # Create visualization without validation set\n",
    "        create_split_visualization(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "def create_split_visualization(X_train, X_test, y_train, y_test, X_val=None, y_val=None):\n",
    "    \"\"\"\n",
    "    Create visualization of the train-test split.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nCreating split visualization...\")\n",
    "\n",
    "    # Combine data for consistent PCA\n",
    "    if X_val is not None:\n",
    "        X_combined = pd.concat([X_train, X_val, X_test])\n",
    "        y_combined = pd.concat([y_train, y_val, y_test])\n",
    "        split_labels = (['Train'] * len(X_train) +\n",
    "                       ['Validation'] * len(X_val) +\n",
    "                       ['Test'] * len(X_test))\n",
    "    else:\n",
    "        X_combined = pd.concat([X_train, X_test])\n",
    "        y_combined = pd.concat([y_train, y_test])\n",
    "        split_labels = ['Train'] * len(X_train) + ['Test'] * len(X_test)\n",
    "\n",
    "    # Perform PCA\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_combined)\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(X_scaled)\n",
    "\n",
    "    # Create plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Plot 1: Colored by split\n",
    "    ax1 = axes[0]\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c'] if X_val is not None else ['#1f77b4', '#ff7f0e']\n",
    "    for i, split_type in enumerate(set(split_labels)):\n",
    "        mask = [label == split_type for label in split_labels]\n",
    "        ax1.scatter(pca_result[mask, 0], pca_result[mask, 1],\n",
    "                   c=colors[i], label=split_type, alpha=0.7, s=50)\n",
    "\n",
    "    ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    ax1.set_title('Data Split Visualization')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Colored by condition\n",
    "    ax2 = axes[1]\n",
    "    condition_colors = {0: '#2E8B57', 1: '#DC143C'}  # Green for healthy, red for unhealthy\n",
    "    for condition in [0, 1]:\n",
    "        mask = y_combined == condition\n",
    "        label = 'Healthy' if condition == 0 else 'Unhealthy'\n",
    "        ax2.scatter(pca_result[mask, 0], pca_result[mask, 1],\n",
    "                   c=condition_colors[condition], label=label, alpha=0.7, s=50)\n",
    "\n",
    "    ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    ax2.set_title('Condition Distribution Across Splits')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Split visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_paths = create_train_test_split(\n",
    "    merged_dataset_path=\"data/merged_dataset_tpm_normalized.pq\",\n",
    "    output_dir=\"data/splits\",\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    create_validation=False\n",
    ")\n",
    "\n",
    "print(\"Split files created:\")\n",
    "for split_name, path in split_paths.items():\n",
    "    print(f\"  {split_name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# Log2 Tansformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_log_transformation_to_splits(split_paths, output_dir=\"data/splits\"):\n",
    "    \"\"\"\n",
    "    Apply log2(x + 1) transformation to train/test splits.\n",
    "\n",
    "    Args:\n",
    "        split_paths (dict): Paths to raw split files\n",
    "        output_dir (str): Directory to save transformed splits\n",
    "\n",
    "    Returns:\n",
    "        dict: Paths to transformed split files\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    transformed_paths = {}\n",
    "\n",
    "    for split_name, file_path in split_paths.items():\n",
    "        if split_name.startswith('X_'):\n",
    "            print(f\"Transforming {split_name}...\")\n",
    "\n",
    "            df = pd.read_parquet(file_path)\n",
    "\n",
    "            if 'condition' in df.columns:\n",
    "                condition_col = df['condition']\n",
    "                feature_cols = df.drop('condition', axis=1)\n",
    "\n",
    "                transformed_features = np.log2(feature_cols + 1)\n",
    "\n",
    "                transformed_df = pd.DataFrame(\n",
    "                    transformed_features,\n",
    "                    index=df.index,\n",
    "                    columns=feature_cols.columns\n",
    "                )\n",
    "                transformed_df['condition'] = condition_col\n",
    "            else:\n",
    "                transformed_df = pd.DataFrame(\n",
    "                    np.log2(df + 1),\n",
    "                    index=df.index,\n",
    "                    columns=df.columns\n",
    "                )\n",
    "\n",
    "            filename = os.path.basename(file_path).replace('_raw.pq', '_log2.pq')\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            transformed_df.to_parquet(output_path, index=True)\n",
    "            transformed_paths[split_name] = output_path\n",
    "\n",
    "            print(f\"  Saved to: {output_path}\")\n",
    "\n",
    "            del df, transformed_df\n",
    "\n",
    "        else:\n",
    "            transformed_paths[split_name] = file_path\n",
    "\n",
    "    print(\"‚úÖ Log transformation complete!\")\n",
    "    return transformed_paths\n",
    "\n",
    "\n",
    "def visualize_log_transformation(raw_split_paths, sample_size=100000):\n",
    "    \"\"\"\n",
    "    Create visualization comparing raw vs log-transformed data using training data.\n",
    "\n",
    "    Args:\n",
    "        raw_split_paths (dict): Paths to raw split files\n",
    "        sample_size (int): Number of values to sample for plotting\n",
    "    \"\"\"\n",
    "    print(\"Creating log transformation visualization...\")\n",
    "\n",
    "    # Load training data for visualization\n",
    "    X_train_raw = pd.read_parquet(raw_split_paths['X_train'])\n",
    "\n",
    "    print(f\"Loaded training data: {X_train_raw.shape}\")\n",
    "\n",
    "    # Sample data for plotting\n",
    "    raw_values = X_train_raw.values.flatten()\n",
    "    if len(raw_values) > sample_size:\n",
    "        raw_sample = np.random.choice(raw_values, size=sample_size, replace=False)\n",
    "    else:\n",
    "        raw_sample = raw_values\n",
    "\n",
    "    # Apply log transformation\n",
    "    log_sample = np.log2(raw_sample + 1)\n",
    "\n",
    "    print(f\"Sampled {len(raw_sample)} values for visualization\")\n",
    "\n",
    "    # Create plots\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Raw data distribution\n",
    "    axes[0].hist(raw_sample, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0].set_title('Distribution of Raw TPM Values (Training Data)')\n",
    "    axes[0].set_xlabel('Raw TPM Value')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_yscale('log')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Log-transformed data distribution\n",
    "    axes[1].hist(log_sample, bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[1].set_title('Distribution of Log2(TPM + 1) Values (Training Data)')\n",
    "    axes[1].set_xlabel('Log2(TPM + 1) Value')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(f\"\\nSummary Statistics:\")\n",
    "    print(f\"Raw data - Mean: {np.mean(raw_sample):.2f}, Std: {np.std(raw_sample):.2f}\")\n",
    "    print(f\"Log-transformed - Mean: {np.mean(log_sample):.2f}, Std: {np.std(log_sample):.2f}\")\n",
    "\n",
    "    # Clean up\n",
    "    del X_train_raw, raw_values, raw_sample, log_sample\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_log_transformation(split_paths, sample_size=50000)\n",
    "\n",
    "transformed_paths = apply_log_transformation_to_splits(split_paths)\n",
    "\n",
    "for split_name, path in transformed_paths.items():\n",
    "    print(f\"  {split_name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# Batch effect correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from inmoose.pycombat import pycombat_norm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_batch_correction_to_splits(transformed_paths, output_dir=\"data/splits\"):\n",
    "    \"\"\"\n",
    "    Apply batch effect correction using pycombat_norm to train/test splits.\n",
    "    Assumes batch effects are present but no specific batch information is available.\n",
    "\n",
    "    Args:\n",
    "        transformed_paths (dict): Paths to log-transformed split files\n",
    "        output_dir (str): Directory to save batch-corrected splits\n",
    "\n",
    "    Returns:\n",
    "        dict: Paths to batch-corrected split files\n",
    "    \"\"\"\n",
    "    print(\"=== BATCH EFFECT CORRECTION ===\")\n",
    "    print(\"Using pycombat_norm without covariates\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load training and test data\n",
    "    X_train = pd.read_parquet(transformed_paths['X_train'])\n",
    "    X_test = pd.read_parquet(transformed_paths['X_test'])\n",
    "\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "    # Check if we should apply batch correction\n",
    "    # For genomics data, batch correction is often beneficial even without explicit batch info\n",
    "\n",
    "    # Combine train and test for batch correction\n",
    "    # This ensures consistent correction across splits\n",
    "    combined_data = pd.concat([X_train, X_test], axis=0)\n",
    "    print(f\"Combined data shape: {combined_data.shape}\")\n",
    "\n",
    "    # Create simple batch labels based on dataset source\n",
    "    # Alternative: you could create batches based on sample ID patterns or other metadata\n",
    "    batch_labels = (['train'] * len(X_train) + ['test'] * len(X_test))\n",
    "\n",
    "    try:\n",
    "        print(\"\\nüîÑ Applying pycombat batch correction...\")\n",
    "        print(\"   Method: Combat without covariates\")\n",
    "        print(\"   This may take a few minutes for large datasets...\")\n",
    "\n",
    "        # Prepare data for pycombat (features as rows, samples as columns)\n",
    "        # pycombat expects: genes/features as rows, samples as columns\n",
    "        data_for_combat = combined_data.T  # Transpose: features as rows, samples as columns\n",
    "\n",
    "        # Apply batch correction\n",
    "        corrected_data = pycombat_norm(\n",
    "            data_for_combat,\n",
    "            batch=batch_labels,\n",
    "            mod=None  # No covariates\n",
    "        )\n",
    "\n",
    "        # Transpose back to original format (samples as rows, features as columns)\n",
    "        corrected_combined = pd.DataFrame(\n",
    "            corrected_data.T,\n",
    "            index=combined_data.index,\n",
    "            columns=combined_data.columns\n",
    "        )\n",
    "\n",
    "        print(\"‚úÖ Batch correction completed successfully!\")\n",
    "\n",
    "        # Split back into train and test\n",
    "        n_train = len(X_train)\n",
    "        corrected_X_train = corrected_combined.iloc[:n_train]\n",
    "        corrected_X_test = corrected_combined.iloc[n_train:]\n",
    "\n",
    "        print(f\"   Corrected training shape: {corrected_X_train.shape}\")\n",
    "        print(f\"   Corrected test shape: {corrected_X_test.shape}\")\n",
    "\n",
    "        # Save corrected splits\n",
    "        batch_corrected_paths = {}\n",
    "\n",
    "        for split_name, file_path in transformed_paths.items():\n",
    "            if split_name == 'X_train':\n",
    "                filename = os.path.basename(file_path).replace('_log2.pq', '_batch_corrected.pq')\n",
    "                output_path = os.path.join(output_dir, filename)\n",
    "                corrected_X_train.to_parquet(output_path, index=True)\n",
    "                batch_corrected_paths[split_name] = output_path\n",
    "                print(f\"   Saved corrected training data to: {output_path}\")\n",
    "\n",
    "            elif split_name == 'X_test':\n",
    "                filename = os.path.basename(file_path).replace('_log2.pq', '_batch_corrected.pq')\n",
    "                output_path = os.path.join(output_dir, filename)\n",
    "                corrected_X_test.to_parquet(output_path, index=True)\n",
    "                batch_corrected_paths[split_name] = output_path\n",
    "                print(f\"   Saved corrected test data to: {output_path}\")\n",
    "\n",
    "            else:\n",
    "                # Keep y_train, y_test unchanged\n",
    "                batch_corrected_paths[split_name] = file_path\n",
    "\n",
    "        # Save batch correction info\n",
    "        save_batch_correction_info(output_dir, method=\"pycombat_norm\", covariates=None)\n",
    "\n",
    "        return batch_corrected_paths\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Batch correction failed: {e}\")\n",
    "        print(\"   Returning original paths without batch correction\")\n",
    "        return transformed_paths\n",
    "\n",
    "    finally:\n",
    "        # Clean up memory\n",
    "        if 'combined_data' in locals():\n",
    "            del combined_data\n",
    "        if 'data_for_combat' in locals():\n",
    "            del data_for_combat\n",
    "        if 'corrected_data' in locals():\n",
    "            del corrected_data\n",
    "        if 'corrected_combined' in locals():\n",
    "            del corrected_combined\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def save_batch_correction_info(output_dir, method=\"pycombat_norm\", covariates=None):\n",
    "    \"\"\"\n",
    "    Save information about batch correction applied.\n",
    "    \"\"\"\n",
    "    info_path = os.path.join(output_dir, 'batch_correction_info.txt')\n",
    "\n",
    "    with open(info_path, 'w') as f:\n",
    "        f.write(\"Batch Effect Correction Summary\\n\")\n",
    "        f.write(\"==============================\\n\\n\")\n",
    "        f.write(f\"Method: {method}\\n\")\n",
    "        f.write(f\"Covariates: {covariates if covariates else 'None'}\\n\")\n",
    "        f.write(f\"Batch definition: Simple train/test split\\n\")\n",
    "        f.write(\"Note: Applied to remove potential technical batch effects\\n\")\n",
    "        f.write(\"      between training and test data\\n\")\n",
    "\n",
    "    print(f\"   üìÑ Batch correction info saved to: {info_path}\")\n",
    "\n",
    "\n",
    "def visualize_batch_effects(before_paths, after_paths, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Create simple visualization to compare data before and after batch correction.\n",
    "    \"\"\"\n",
    "    print(\"üìä Creating batch correction visualization...\")\n",
    "\n",
    "    # Load data before and after correction\n",
    "    X_train_before = pd.read_parquet(before_paths['X_train'])\n",
    "    X_train_after = pd.read_parquet(after_paths['X_train'])\n",
    "\n",
    "    # Sample data for visualization\n",
    "    if len(X_train_before) > sample_size:\n",
    "        sample_idx = np.random.choice(len(X_train_before), sample_size, replace=False)\n",
    "        train_before_sample = X_train_before.iloc[sample_idx]\n",
    "        train_after_sample = X_train_after.iloc[sample_idx]\n",
    "    else:\n",
    "        train_before_sample = X_train_before\n",
    "        train_after_sample = X_train_after\n",
    "\n",
    "    # Create comparison plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    pca = PCA(n_components=2)\n",
    "\n",
    "    # Before correction\n",
    "    before_scaled = scaler.fit_transform(train_before_sample)\n",
    "    before_pca = pca.fit_transform(before_scaled)\n",
    "\n",
    "    axes[0].scatter(before_pca[:, 0], before_pca[:, 1], alpha=0.6, s=30)\n",
    "    axes[0].set_title('Before Batch Correction')\n",
    "    axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # After correction\n",
    "    after_scaled = scaler.fit_transform(train_after_sample)\n",
    "    after_pca = pca.fit_transform(after_scaled)\n",
    "\n",
    "    axes[1].scatter(after_pca[:, 0], after_pca[:, 1], alpha=0.6, s=30, color='orange')\n",
    "    axes[1].set_title('After Batch Correction')\n",
    "    axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(f\"\\nBatch Correction Summary:\")\n",
    "    print(f\"   Data points visualized: {len(train_before_sample):,}\")\n",
    "    print(f\"   Features: {train_before_sample.shape[1]:,}\")\n",
    "\n",
    "    # Clean up\n",
    "    del X_train_before, X_train_after, train_before_sample, train_after_sample\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply batch correction\n",
    "print(\"=== APPLYING BATCH EFFECT CORRECTION ===\")\n",
    "\n",
    "batch_corrected_paths = apply_batch_correction_to_splits(transformed_paths)\n",
    "\n",
    "# Visualize batch effects after correction (if correction was applied)\n",
    "if batch_corrected_paths != transformed_paths:\n",
    "    print(\"\\nüìä Batch correction was applied - creating visualization...\")\n",
    "    visualize_batch_effects(transformed_paths, batch_corrected_paths)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Batch correction was skipped - using original transformed data\")\n",
    "\n",
    "print(\"\\nBatch-corrected (or original) file paths:\")\n",
    "for split_name, path in batch_corrected_paths.items():\n",
    "    print(f\"  {split_name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_comprehensive_feature_selection(transformed_paths, variance_threshold=0.01,\n",
    "                                        k_best=5000, output_dir=\"data/splits\"):\n",
    "    \"\"\"\n",
    "    Apply both variance-based and k-best feature selection in sequence.\n",
    "\n",
    "    Args:\n",
    "        transformed_paths (dict): Paths to log-transformed split files\n",
    "        variance_threshold (float): Minimum variance threshold (default: 0.01)\n",
    "        k_best (int): Number of top features to keep after variance filtering\n",
    "        output_dir (str): Directory to save feature-selected splits\n",
    "\n",
    "    Returns:\n",
    "        tuple: (selected_paths, comprehensive_feature_info)\n",
    "    \"\"\"\n",
    "    from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(\"=== COMPREHENSIVE FEATURE SELECTION ===\")\n",
    "    print(\"Step 1: Variance Threshold (Unsupervised)\")\n",
    "    print(\"Step 2: SelectKBest (Supervised)\")\n",
    "    print()\n",
    "\n",
    "    # Load training data and labels\n",
    "    X_train = pd.read_parquet(transformed_paths['X_train'])\n",
    "    y_train = pd.read_parquet(transformed_paths['y_train'])['condition']\n",
    "\n",
    "    print(f\"Original features: {X_train.shape[1]:,}\")\n",
    "\n",
    "    # Step 1: Variance thresholding (unsupervised)\n",
    "    print(f\"\\nüîÑ Step 1: Applying variance threshold ({variance_threshold})...\")\n",
    "    variance_selector = VarianceThreshold(threshold=variance_threshold)\n",
    "    variance_selector.fit(X_train)\n",
    "\n",
    "    # Get features that pass variance threshold\n",
    "    variance_mask = variance_selector.get_support()\n",
    "    features_after_variance = X_train.columns[variance_mask].tolist()\n",
    "    variance_removed = X_train.columns[~variance_mask].tolist()\n",
    "\n",
    "    print(f\"‚úÖ Variance filtering complete:\")\n",
    "    print(f\"   Kept: {len(features_after_variance):,} features\")\n",
    "    print(f\"   Removed: {len(variance_removed):,} features\")\n",
    "\n",
    "    # Step 2: SelectKBest (supervised) - only if we have more than k_best features\n",
    "    if len(features_after_variance) > k_best:\n",
    "        print(f\"\\nüîÑ Step 2: Applying SelectKBest (k={k_best:,})...\")\n",
    "\n",
    "        # Apply variance selection to get intermediate dataset\n",
    "        X_train_var = X_train[features_after_variance]\n",
    "\n",
    "        # Apply SelectKBest\n",
    "        k_best_selector = SelectKBest(f_classif, k=k_best)\n",
    "        k_best_selector.fit(X_train_var, y_train)\n",
    "\n",
    "        # Get final selected features\n",
    "        kbest_mask = k_best_selector.get_support()\n",
    "        final_features = np.array(features_after_variance)[kbest_mask].tolist()\n",
    "        kbest_removed = np.array(features_after_variance)[~kbest_mask].tolist()\n",
    "\n",
    "        print(f\"‚úÖ SelectKBest complete:\")\n",
    "        print(f\"   Final features: {len(final_features):,}\")\n",
    "        print(f\"   Additional removed: {len(kbest_removed):,}\")\n",
    "\n",
    "        # Save both selectors\n",
    "        joblib.dump(variance_selector, os.path.join(output_dir, 'variance_selector.pkl'))\n",
    "        joblib.dump(k_best_selector, os.path.join(output_dir, 'kbest_selector.pkl'))\n",
    "\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Skipping SelectKBest: Only {len(features_after_variance):,} features after variance filtering (less than k={k_best:,})\")\n",
    "        final_features = features_after_variance\n",
    "        k_best_selector = None\n",
    "        kbest_removed = []\n",
    "\n",
    "        # Save only variance selector\n",
    "        joblib.dump(variance_selector, os.path.join(output_dir, 'variance_selector.pkl'))\n",
    "\n",
    "    print(f\"\\nüìä FINAL RESULTS:\")\n",
    "    print(f\"   Original features: {X_train.shape[1]:,}\")\n",
    "    print(f\"   Final features: {len(final_features):,}\")\n",
    "    print(f\"   Total reduction: {((X_train.shape[1] - len(final_features)) / X_train.shape[1] * 100):.1f}%\")\n",
    "\n",
    "    # Apply final feature selection to all splits\n",
    "    print(f\"\\nüîÑ Applying feature selection to all splits...\")\n",
    "    selected_paths = {}\n",
    "\n",
    "    for split_name, file_path in transformed_paths.items():\n",
    "        if split_name.startswith('X_'):\n",
    "            print(f\"   Processing {split_name}...\")\n",
    "\n",
    "            df = pd.read_parquet(file_path)\n",
    "            df_selected = df[final_features]  # Apply same selection to all splits\n",
    "\n",
    "            filename = os.path.basename(file_path).replace('_log2.pq', '_selected.pq')\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            df_selected.to_parquet(output_path, index=True)\n",
    "            selected_paths[split_name] = output_path\n",
    "\n",
    "            print(f\"     {df.shape} -> {df_selected.shape}\")\n",
    "\n",
    "            del df, df_selected\n",
    "        else:\n",
    "            selected_paths[split_name] = file_path\n",
    "\n",
    "    # Create comprehensive feature info\n",
    "    comprehensive_feature_info = {\n",
    "        'final_selected_features': final_features,\n",
    "        'variance_removed_features': variance_removed,\n",
    "        'kbest_removed_features': kbest_removed,\n",
    "        'variance_threshold': variance_threshold,\n",
    "        'k_best': k_best if k_best_selector else None,\n",
    "        'n_features_original': X_train.shape[1],\n",
    "        'n_features_after_variance': len(features_after_variance),\n",
    "        'n_features_final': len(final_features),\n",
    "        'total_reduction_pct': ((X_train.shape[1] - len(final_features)) / X_train.shape[1] * 100),\n",
    "        'used_kbest': k_best_selector is not None\n",
    "    }\n",
    "\n",
    "    # Save detailed results\n",
    "    with open(os.path.join(output_dir, 'comprehensive_feature_selection_info.txt'), 'w') as f:\n",
    "        f.write(\"Comprehensive Feature Selection Results\\n\")\n",
    "        f.write(\"======================================\\n\\n\")\n",
    "        f.write(f\"Original features: {comprehensive_feature_info['n_features_original']:,}\\n\")\n",
    "        f.write(f\"After variance threshold ({variance_threshold}): {comprehensive_feature_info['n_features_after_variance']:,}\\n\")\n",
    "        if comprehensive_feature_info['used_kbest']:\n",
    "            f.write(f\"After SelectKBest ({k_best:,}): {comprehensive_feature_info['n_features_final']:,}\\n\")\n",
    "        f.write(f\"Final features: {comprehensive_feature_info['n_features_final']:,}\\n\")\n",
    "        f.write(f\"Total reduction: {comprehensive_feature_info['total_reduction_pct']:.1f}%\\n\\n\")\n",
    "        f.write(\"Feature Selection Steps:\\n\")\n",
    "        f.write(\"1. Variance Threshold (Unsupervised)\\n\")\n",
    "        if comprehensive_feature_info['used_kbest']:\n",
    "            f.write(\"2. SelectKBest with F-classification (Supervised)\\n\")\n",
    "        else:\n",
    "            f.write(\"2. SelectKBest skipped (insufficient features)\\n\")\n",
    "\n",
    "    print(\"‚úÖ Comprehensive feature selection complete!\")\n",
    "    print(f\"üìÅ Results saved to: {output_dir}/\")\n",
    "\n",
    "    return selected_paths, comprehensive_feature_info\n",
    "\n",
    "\n",
    "def visualize_comprehensive_feature_selection(comprehensive_feature_info, X_train_path):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for the two-step feature selection process.\n",
    "\n",
    "    Args:\n",
    "        comprehensive_feature_info (dict): Results from comprehensive feature selection\n",
    "        X_train_path (str): Path to training data before selection\n",
    "    \"\"\"\n",
    "    print(\"Creating comprehensive feature selection visualizations...\")\n",
    "\n",
    "    # Load training data to analyze variances\n",
    "    X_train = pd.read_parquet(X_train_path)\n",
    "    variances = X_train.var()\n",
    "\n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    # Plot 1: Variance distribution with threshold\n",
    "    axes[0, 0].hist(variances, bins=100, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].axvline(comprehensive_feature_info['variance_threshold'],\n",
    "                      color='red', linestyle='--', linewidth=2,\n",
    "                      label=f\"Threshold = {comprehensive_feature_info['variance_threshold']}\")\n",
    "    axes[0, 0].set_xlabel('Feature Variance')\n",
    "    axes[0, 0].set_ylabel('Number of Features')\n",
    "    axes[0, 0].set_title('Step 1: Variance Distribution')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Feature selection pipeline\n",
    "    if comprehensive_feature_info['used_kbest']:\n",
    "        stages = ['Original', 'After\\nVariance', 'Final\\n(K-Best)']\n",
    "        counts = [\n",
    "            comprehensive_feature_info['n_features_original'],\n",
    "            comprehensive_feature_info['n_features_after_variance'],\n",
    "            comprehensive_feature_info['n_features_final']\n",
    "        ]\n",
    "        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    else:\n",
    "        stages = ['Original', 'Final\\n(Variance Only)']\n",
    "        counts = [\n",
    "            comprehensive_feature_info['n_features_original'],\n",
    "            comprehensive_feature_info['n_features_final']\n",
    "        ]\n",
    "        colors = ['#FF6B6B', '#45B7D1']\n",
    "\n",
    "    bars = axes[0, 1].bar(stages, counts, color=colors, alpha=0.8, edgecolor='black')\n",
    "    axes[0, 1].set_ylabel('Number of Features')\n",
    "    axes[0, 1].set_title('Feature Selection Pipeline')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Add count labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        height = bar.get_height()\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                       f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # Plot 3: Reduction percentages\n",
    "    if comprehensive_feature_info['used_kbest']:\n",
    "        variance_reduction = ((comprehensive_feature_info['n_features_original'] -\n",
    "                             comprehensive_feature_info['n_features_after_variance']) /\n",
    "                            comprehensive_feature_info['n_features_original'] * 100)\n",
    "        kbest_reduction = ((comprehensive_feature_info['n_features_after_variance'] -\n",
    "                          comprehensive_feature_info['n_features_final']) /\n",
    "                         comprehensive_feature_info['n_features_after_variance'] * 100)\n",
    "\n",
    "        reduction_stages = ['Variance\\nFiltering', 'K-Best\\nSelection']\n",
    "        reduction_pcts = [variance_reduction, kbest_reduction]\n",
    "        reduction_colors = ['#FF9F43', '#10AC84']\n",
    "    else:\n",
    "        reduction_stages = ['Variance\\nFiltering']\n",
    "        reduction_pcts = [comprehensive_feature_info['total_reduction_pct']]\n",
    "        reduction_colors = ['#10AC84']\n",
    "\n",
    "    bars = axes[1, 0].bar(reduction_stages, reduction_pcts,\n",
    "                         color=reduction_colors, alpha=0.8, edgecolor='black')\n",
    "    axes[1, 0].set_ylabel('Reduction Percentage (%)')\n",
    "    axes[1, 0].set_title('Feature Reduction by Stage')\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Add percentage labels\n",
    "    for bar, pct in zip(bars, reduction_pcts):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                       f'{pct:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # Plot 4: Final summary pie chart\n",
    "    if comprehensive_feature_info['used_kbest']:\n",
    "        labels = ['Selected', 'Removed by\\nVariance', 'Removed by\\nK-Best']\n",
    "        sizes = [\n",
    "            comprehensive_feature_info['n_features_final'],\n",
    "            len(comprehensive_feature_info['variance_removed_features']),\n",
    "            len(comprehensive_feature_info['kbest_removed_features'])\n",
    "        ]\n",
    "        colors = ['#2E8B57', '#DC143C', '#FF8C00']\n",
    "        explode = (0.05, 0, 0)\n",
    "    else:\n",
    "        labels = ['Selected', 'Removed by\\nVariance']\n",
    "        sizes = [\n",
    "            comprehensive_feature_info['n_features_final'],\n",
    "            len(comprehensive_feature_info['variance_removed_features'])\n",
    "        ]\n",
    "        colors = ['#2E8B57', '#DC143C']\n",
    "        explode = (0.05, 0)\n",
    "\n",
    "    wedges, texts, autotexts = axes[1, 1].pie(sizes, labels=labels, colors=colors,\n",
    "                                             autopct='%1.1f%%', explode=explode,\n",
    "                                             shadow=True, startangle=90)\n",
    "    axes[1, 1].set_title('Final Feature Distribution')\n",
    "\n",
    "    # Make percentage text bold\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print detailed summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"COMPREHENSIVE FEATURE SELECTION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"üìä Original features: {comprehensive_feature_info['n_features_original']:,}\")\n",
    "    print(f\"üîÑ After variance filtering: {comprehensive_feature_info['n_features_after_variance']:,}\")\n",
    "    if comprehensive_feature_info['used_kbest']:\n",
    "        print(f\"üéØ After K-Best selection: {comprehensive_feature_info['n_features_final']:,}\")\n",
    "    print(f\"‚úÖ Final features: {comprehensive_feature_info['n_features_final']:,}\")\n",
    "    print(f\"üìâ Total reduction: {comprehensive_feature_info['total_reduction_pct']:.1f}%\")\n",
    "    print(f\"üß¨ Features per sample ratio: {comprehensive_feature_info['n_features_final']:,} features\")\n",
    "\n",
    "    if comprehensive_feature_info['used_kbest']:\n",
    "        print(f\"\\nüî¨ Selection Methods Used:\")\n",
    "        print(f\"   1. Variance Threshold ({comprehensive_feature_info['variance_threshold']})\")\n",
    "        print(f\"   2. SelectKBest (k={comprehensive_feature_info['k_best']:,})\")\n",
    "    else:\n",
    "        print(f\"\\nüî¨ Selection Method Used:\")\n",
    "        print(f\"   ‚Ä¢ Variance Threshold only ({comprehensive_feature_info['variance_threshold']})\")\n",
    "\n",
    "    # Clean up\n",
    "    del X_train, variances\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_paths_conservative, feature_info_conservative = apply_comprehensive_feature_selection(\n",
    "    transformed_paths,\n",
    "    variance_threshold=0.005,\n",
    "    k_best=50,\n",
    "    output_dir=\"data/splits\"\n",
    ")\n",
    "\n",
    "# Visualize results\n",
    "visualize_comprehensive_feature_selection(feature_info_conservative, transformed_paths['X_train'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL PROCESSED FILES:\")\n",
    "for split_name, path in selected_paths_conservative.items():\n",
    "    print(f\"  {split_name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "# Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_basic_class_imbalance(selected_paths):\n",
    "    \"\"\"\n",
    "    Simple class imbalance analysis with basic class weight computation.\n",
    "\n",
    "    Args:\n",
    "        selected_paths (dict): Paths to processed split files\n",
    "\n",
    "    Returns:\n",
    "        dict: Basic class distribution and weights\n",
    "    \"\"\"\n",
    "    print(\"=== BASIC CLASS IMBALANCE ANALYSIS ===\")\n",
    "\n",
    "    # Load training labels\n",
    "    y_train = pd.read_parquet(selected_paths['y_train'])['condition']\n",
    "    y_test = pd.read_parquet(selected_paths['y_test'])['condition']\n",
    "\n",
    "    # Calculate distributions\n",
    "    train_counts = y_train.value_counts().sort_index()\n",
    "    test_counts = y_test.value_counts().sort_index()\n",
    "\n",
    "    healthy_train = train_counts[0]\n",
    "    unhealthy_train = train_counts[1]\n",
    "\n",
    "    # Calculate basic metrics\n",
    "    imbalance_ratio = healthy_train / unhealthy_train\n",
    "    minority_pct = (unhealthy_train / len(y_train)) * 100\n",
    "\n",
    "    print(f\"üìä Training Set:\")\n",
    "    print(f\"   Healthy: {healthy_train:,} ({healthy_train/len(y_train)*100:.1f}%)\")\n",
    "    print(f\"   Unhealthy: {unhealthy_train:,} ({unhealthy_train/len(y_train)*100:.1f}%)\")\n",
    "    print(f\"   Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "    print(f\"\\nüìä Test Set:\")\n",
    "    print(f\"   Healthy: {test_counts[0]:,} ({test_counts[0]/len(y_test)*100:.1f}%)\")\n",
    "    print(f\"   Unhealthy: {test_counts[1]:,} ({test_counts[1]/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "    # Compute class weights (sklearn balanced method)\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "    print(f\"\\n  Class Weights (sklearn 'balanced'):\")\n",
    "    print(f\"   Healthy (0): {class_weight_dict[0]:.3f}\")\n",
    "    print(f\"   Unhealthy (1): {class_weight_dict[1]:.3f}\")\n",
    "    print(f\"   Weight ratio: {class_weight_dict[1]/class_weight_dict[0]:.3f}\")\n",
    "\n",
    "    # LightGBM scale_pos_weight (from your existing code)\n",
    "    scale_pos_weight = healthy_train / unhealthy_train\n",
    "    print(f\"\\nüîß LightGBM scale_pos_weight: {scale_pos_weight:.3f}\")\n",
    "\n",
    "    # Simple recommendation\n",
    "    if imbalance_ratio > 2.0:\n",
    "        print(f\"   ‚ö†Ô∏è  Significant imbalance detected!\")\n",
    "        print(f\"   üí° Recommendation: Use class weights\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Relatively balanced dataset\")\n",
    "        print(f\"   üí° Class weights optional but can help\")\n",
    "\n",
    "    # Create simple visualization\n",
    "    create_simple_class_plots(train_counts, test_counts, class_weight_dict)\n",
    "\n",
    "    return {\n",
    "        'class_weight_dict': class_weight_dict,\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'imbalance_ratio': imbalance_ratio,\n",
    "        'minority_percentage': minority_pct,\n",
    "        'train_counts': train_counts,\n",
    "        'test_counts': test_counts\n",
    "    }\n",
    "\n",
    "\n",
    "def create_simple_class_plots(train_counts, test_counts, class_weights):\n",
    "    \"\"\"\n",
    "    Create simple bar plots for class distribution.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot 1: Class distribution\n",
    "    classes = ['Healthy', 'Unhealthy']\n",
    "    train_values = [train_counts[0], train_counts[1]]\n",
    "    test_values = [test_counts[0], test_counts[1]]\n",
    "\n",
    "    x = range(len(classes))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = axes[0].bar([i - width/2 for i in x], train_values, width,\n",
    "                       label='Training', color='#4CAF50', alpha=0.8)\n",
    "    bars2 = axes[0].bar([i + width/2 for i in x], test_values, width,\n",
    "                       label='Test', color='#FF9800', alpha=0.8)\n",
    "\n",
    "    axes[0].set_xlabel('Class')\n",
    "    axes[0].set_ylabel('Number of Samples')\n",
    "    axes[0].set_title('Class Distribution')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(classes)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Add count labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            axes[0].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                        f'{int(height):,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # Plot 2: Class weights\n",
    "    weight_values = [class_weights[0], class_weights[1]]\n",
    "    bars3 = axes[1].bar(classes, weight_values, color=['#2E8B57', '#DC143C'], alpha=0.8)\n",
    "\n",
    "    axes[1].set_xlabel('Class')\n",
    "    axes[1].set_ylabel('Class Weight')\n",
    "    axes[1].set_title('Computed Class Weights')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Add weight labels\n",
    "    for bar, weight in zip(bars3, weight_values):\n",
    "        height = bar.get_height()\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                    f'{weight:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_simple_class_weights(class_info, output_dir=\"data/splits\"):\n",
    "    \"\"\"\n",
    "    Save class weights in a simple format for easy loading.\n",
    "\n",
    "    Args:\n",
    "        class_info (dict): Class information from analyze_basic_class_imbalance\n",
    "        output_dir (str): Directory to save weights\n",
    "\n",
    "    Returns:\n",
    "        str: Path to saved weights file\n",
    "    \"\"\"\n",
    "    weights_path = os.path.join(output_dir, 'class_weights.pkl')\n",
    "\n",
    "    # Save just the essential info\n",
    "    weights_data = {\n",
    "        'class_weight_dict': class_info['class_weight_dict'],\n",
    "        'scale_pos_weight': class_info['scale_pos_weight'],\n",
    "        'imbalance_ratio': class_info['imbalance_ratio']\n",
    "    }\n",
    "\n",
    "    with open(weights_path, 'wb') as f:\n",
    "        pickle.dump(weights_data, f)\n",
    "\n",
    "    print(f\"‚úÖ Class weights saved to: {weights_path}\")\n",
    "\n",
    "    # Also save as simple text for reference\n",
    "    txt_path = os.path.join(output_dir, 'class_weights.txt')\n",
    "    with open(txt_path, 'w') as f:\n",
    "        f.write(\"Class Weights Summary\\n\")\n",
    "        f.write(\"===================\\n\\n\")\n",
    "        f.write(f\"sklearn class_weight='balanced':\\n\")\n",
    "        f.write(f\"  Healthy (0): {class_info['class_weight_dict'][0]:.6f}\\n\")\n",
    "        f.write(f\"  Unhealthy (1): {class_info['class_weight_dict'][1]:.6f}\\n\\n\")\n",
    "        f.write(f\"LightGBM scale_pos_weight: {class_info['scale_pos_weight']:.6f}\\n\")\n",
    "        f.write(f\"Imbalance ratio: {class_info['imbalance_ratio']:.2f}:1\\n\")\n",
    "\n",
    "    return weights_path\n",
    "\n",
    "\n",
    "def load_simple_class_weights(weights_path=\"data/splits/class_weights.pkl\"):\n",
    "    \"\"\"\n",
    "    Load class weights for model training.\n",
    "\n",
    "    Args:\n",
    "        weights_path (str): Path to saved class weights\n",
    "\n",
    "    Returns:\n",
    "        dict: Class weights ready for sklearn models\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(weights_path, 'rb') as f:\n",
    "            weights_data = pickle.load(f)\n",
    "\n",
    "        print(f\"‚úÖ Loaded class weights:\")\n",
    "        print(f\"   Healthy (0): {weights_data['class_weight_dict'][0]:.3f}\")\n",
    "        print(f\"   Unhealthy (1): {weights_data['class_weight_dict'][1]:.3f}\")\n",
    "        print(f\"   LightGBM scale_pos_weight: {weights_data['scale_pos_weight']:.3f}\")\n",
    "\n",
    "        return weights_data['class_weight_dict']\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Weights file not found: {weights_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class imbalance\n",
    "class_info = analyze_basic_class_imbalance(selected_paths_conservative)\n",
    "\n",
    "# Save weights for later use\n",
    "weights_path = save_simple_class_weights(class_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# Hyper Param Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_lightgbm_with_workflow(selected_paths, class_weights_path=\"data/splits/class_weights.pkl\",\n",
    "                                  n_trials=100, cv_folds=5, random_state=42,\n",
    "                                  output_dir=\"data/splits\"):\n",
    "    \"\"\"\n",
    "    Optimize LightGBM hyperparameters integrated with the full preprocessing workflow.\n",
    "\n",
    "    Args:\n",
    "        selected_paths (dict): Paths to processed split files\n",
    "        class_weights_path (str): Path to saved class weights\n",
    "        n_trials (int): Number of optimization trials\n",
    "        cv_folds (int): Number of cross-validation folds\n",
    "        random_state (int): Random seed\n",
    "        output_dir (str): Directory to save optimization results\n",
    "\n",
    "    Returns:\n",
    "        dict: Best hyperparameters, study results, and evaluation metrics\n",
    "    \"\"\"\n",
    "    print(\"=== LIGHTGBM HYPERPARAMETER OPTIMIZATION ===\")\n",
    "    print(\"Integrated with class weights and feature selection\")\n",
    "\n",
    "    # Load training data\n",
    "    print(\"\\nüìä Loading processed training data...\")\n",
    "    X_train = pd.read_parquet(selected_paths['X_train'])\n",
    "    y_train = pd.read_parquet(selected_paths['y_train'])['condition']\n",
    "\n",
    "    print(f\"   Training features shape: {X_train.shape}\")\n",
    "    print(f\"   Training labels shape: {y_train.shape}\")\n",
    "    print(f\"   Class distribution: {y_train.value_counts().to_dict()}\")\n",
    "\n",
    "    # Load class weights\n",
    "    print(\"\\n‚öñÔ∏è  Loading class imbalance information...\")\n",
    "    try:\n",
    "        with open(class_weights_path, 'rb') as f:\n",
    "            weights_data = pickle.load(f)\n",
    "        scale_pos_weight = weights_data['scale_pos_weight']\n",
    "        imbalance_ratio = weights_data['imbalance_ratio']\n",
    "        print(f\"   Scale_pos_weight: {scale_pos_weight:.3f}\")\n",
    "        print(f\"   Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"   ‚ö†Ô∏è  Class weights not found, using default (1.0)\")\n",
    "        scale_pos_weight = 1.0\n",
    "        imbalance_ratio = 1.0\n",
    "\n",
    "    # Define optimization objective\n",
    "    def objective(trial):\n",
    "        \"\"\"Optuna objective function for LightGBM optimization.\"\"\"\n",
    "\n",
    "        # Sample hyperparameters\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'binary_logloss',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'scale_pos_weight': scale_pos_weight,  # Use computed class weight\n",
    "            'random_state': random_state,\n",
    "            'verbose': -1,\n",
    "            'force_col_wise': True,  # Better for many features\n",
    "\n",
    "            # Core hyperparameters to optimize\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 10, 300),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 50),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        }\n",
    "\n",
    "        # Cross-validation with stratification\n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
    "        cv_scores = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "            X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "            # Create LightGBM datasets\n",
    "            train_data = lgb.Dataset(X_fold_train, label=y_fold_train)\n",
    "            val_data = lgb.Dataset(X_fold_val, label=y_fold_val, reference=train_data)\n",
    "\n",
    "            # Train with early stopping\n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                train_data,\n",
    "                valid_sets=[val_data],\n",
    "                valid_names=['eval'],\n",
    "                num_boost_round=1000,\n",
    "                callbacks=[\n",
    "                    lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "                    lgb.log_evaluation(0)  # Silent\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Predict and evaluate\n",
    "            val_pred = model.predict(X_fold_val, num_iteration=model.best_iteration)\n",
    "            auc_score = roc_auc_score(y_fold_val, val_pred)\n",
    "            cv_scores.append(auc_score)\n",
    "\n",
    "        return np.mean(cv_scores)\n",
    "\n",
    "    # Run optimization\n",
    "    print(f\"\\nüîß Starting optimization with {n_trials} trials and {cv_folds}-fold CV...\")\n",
    "    print(\"This may take several minutes depending on data size...\")\n",
    "\n",
    "    # Create study with reproducible sampler\n",
    "    sampler = optuna.samplers.TPESampler(seed=random_state)\n",
    "    study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "\n",
    "    # Add progress callback\n",
    "    def progress_callback(study, trial):\n",
    "        if trial.number % 10 == 0:\n",
    "            print(f\"   Trial {trial.number:3d}: Current best AUC = {study.best_value:.4f}\")\n",
    "\n",
    "    study.optimize(objective, n_trials=n_trials, callbacks=[progress_callback])\n",
    "\n",
    "    # Prepare final best parameters\n",
    "    best_params = study.best_params.copy()\n",
    "    best_params.update({\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'random_state': random_state,\n",
    "        'verbose': -1,\n",
    "        'force_col_wise': True\n",
    "    })\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"OPTIMIZATION RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"üéØ Best CV AUC Score: {study.best_value:.4f}\")\n",
    "    print(f\"üìä Number of trials: {len(study.trials)}\")\n",
    "    print(f\"‚öñÔ∏è  Used scale_pos_weight: {scale_pos_weight:.3f}\")\n",
    "\n",
    "    print(f\"\\nüîß Best Hyperparameters:\")\n",
    "    for param, value in study.best_params.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"   {param}: {value:.6f}\")\n",
    "        else:\n",
    "            print(f\"   {param}: {value}\")\n",
    "\n",
    "    # Create visualizations\n",
    "    create_optimization_visualizations(study, output_dir)\n",
    "\n",
    "    # Evaluate final model on test set\n",
    "    print(f\"\\nüìà Evaluating final model on test set...\")\n",
    "    test_results = evaluate_final_model(\n",
    "        best_params, selected_paths, X_train, y_train, output_dir\n",
    "    )\n",
    "\n",
    "    # Save optimization results\n",
    "    optimization_results = {\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': study.best_value,\n",
    "        'study': study,\n",
    "        'n_trials': n_trials,\n",
    "        'cv_folds': cv_folds,\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'imbalance_ratio': imbalance_ratio,\n",
    "        'test_results': test_results,\n",
    "        'feature_count': X_train.shape[1]\n",
    "    }\n",
    "\n",
    "    # Save results as pickle\n",
    "    results_path = os.path.join(output_dir, 'lightgbm_optimization_results.pkl')\n",
    "    with open(results_path, 'wb') as f:\n",
    "        pickle.dump(optimization_results, f)\n",
    "\n",
    "    # Save readable summary\n",
    "    save_optimization_summary(optimization_results, output_dir)\n",
    "\n",
    "    print(f\"\\n‚úÖ Optimization complete!\")\n",
    "    print(f\"üìÅ Results saved to: {output_dir}/\")\n",
    "\n",
    "    return optimization_results\n",
    "\n",
    "\n",
    "def evaluate_final_model(best_params, selected_paths, X_train, y_train, output_dir):\n",
    "    \"\"\"\n",
    "    Train final model with best parameters and evaluate on test set.\n",
    "    \"\"\"\n",
    "    # Load test data\n",
    "    X_test = pd.read_parquet(selected_paths['X_test'])\n",
    "    y_test = pd.read_parquet(selected_paths['y_test'])['condition']\n",
    "\n",
    "    print(f\"   Test data shape: {X_test.shape}\")\n",
    "\n",
    "    # Train final model on full training set\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "    final_model = lgb.train(\n",
    "        best_params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        callbacks=[lgb.log_evaluation(0)]\n",
    "    )\n",
    "\n",
    "    # Predict on test set\n",
    "    test_pred_proba = final_model.predict(X_test)\n",
    "    test_pred_binary = (test_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    # Calculate metrics\n",
    "    test_metrics = {\n",
    "        'auc': roc_auc_score(y_test, test_pred_proba),\n",
    "        'accuracy': accuracy_score(y_test, test_pred_binary),\n",
    "        'precision': precision_score(y_test, test_pred_binary),\n",
    "        'recall': recall_score(y_test, test_pred_binary),\n",
    "        'f1': f1_score(y_test, test_pred_binary)\n",
    "    }\n",
    "\n",
    "    # Print test results\n",
    "    print(f\"   üéØ Test AUC: {test_metrics['auc']:.4f}\")\n",
    "    print(f\"   üìä Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"   üéØ Test Precision: {test_metrics['precision']:.4f}\")\n",
    "    print(f\"   üìä Test Recall: {test_metrics['recall']:.4f}\")\n",
    "    print(f\"   üéØ Test F1-Score: {test_metrics['f1']:.4f}\")\n",
    "\n",
    "    # Save model\n",
    "    model_path = os.path.join(output_dir, 'best_lightgbm_model.txt')\n",
    "    final_model.save_model(model_path)\n",
    "    print(f\"   üíæ Model saved to: {model_path}\")\n",
    "\n",
    "    test_metrics['model_path'] = model_path\n",
    "    test_metrics['predictions'] = {\n",
    "        'y_true': y_test.tolist(),\n",
    "        'y_pred_proba': test_pred_proba.tolist(),\n",
    "        'y_pred_binary': test_pred_binary.tolist()\n",
    "    }\n",
    "\n",
    "    return test_metrics\n",
    "\n",
    "\n",
    "def create_optimization_visualizations(study, output_dir):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for the optimization process.\n",
    "    \"\"\"\n",
    "    print(f\"   üìä Creating optimization visualizations...\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # Plot 1: Optimization history\n",
    "    trials = study.trials\n",
    "    trial_numbers = [t.number for t in trials]\n",
    "    trial_values = [t.value for t in trials if t.value is not None]\n",
    "\n",
    "    axes[0, 0].plot(trial_numbers[:len(trial_values)], trial_values, alpha=0.7)\n",
    "    axes[0, 0].axhline(y=study.best_value, color='red', linestyle='--',\n",
    "                      label=f'Best: {study.best_value:.4f}')\n",
    "    axes[0, 0].set_xlabel('Trial Number')\n",
    "    axes[0, 0].set_ylabel('CV AUC Score')\n",
    "    axes[0, 0].set_title('Optimization History')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Parameter importance\n",
    "    try:\n",
    "        importance = optuna.importance.get_param_importances(study)\n",
    "        if importance:\n",
    "            params = list(importance.keys())\n",
    "            values = list(importance.values())\n",
    "\n",
    "            axes[0, 1].barh(params, values, alpha=0.8, color='skyblue')\n",
    "            axes[0, 1].set_xlabel('Importance')\n",
    "            axes[0, 1].set_title('Parameter Importance')\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[0, 1].text(0.5, 0.5, 'Not enough trials\\nfor importance',\n",
    "                           ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "    except:\n",
    "        axes[0, 1].text(0.5, 0.5, 'Parameter importance\\nnot available',\n",
    "                       ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "\n",
    "    # Plot 3: Learning rate vs num_leaves (if both exist)\n",
    "    if 'learning_rate' in study.best_params and 'num_leaves' in study.best_params:\n",
    "        lr_values = []\n",
    "        nl_values = []\n",
    "        scores = []\n",
    "\n",
    "        for trial in trials:\n",
    "            if trial.value is not None and 'learning_rate' in trial.params and 'num_leaves' in trial.params:\n",
    "                lr_values.append(trial.params['learning_rate'])\n",
    "                nl_values.append(trial.params['num_leaves'])\n",
    "                scores.append(trial.value)\n",
    "\n",
    "        if len(scores) > 0:\n",
    "            scatter = axes[1, 0].scatter(lr_values, nl_values, c=scores,\n",
    "                                       cmap='viridis', alpha=0.7)\n",
    "            axes[1, 0].set_xlabel('Learning Rate')\n",
    "            axes[1, 0].set_ylabel('Num Leaves')\n",
    "            axes[1, 0].set_title('Learning Rate vs Num Leaves')\n",
    "            axes[1, 0].set_xscale('log')\n",
    "            plt.colorbar(scatter, ax=axes[1, 0], label='CV AUC')\n",
    "\n",
    "    # Plot 4: Score distribution\n",
    "    valid_scores = [t.value for t in trials if t.value is not None]\n",
    "    if len(valid_scores) > 0:\n",
    "        axes[1, 1].hist(valid_scores, bins=min(20, len(valid_scores)//2),\n",
    "                       alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "        axes[1, 1].axvline(study.best_value, color='red', linestyle='--',\n",
    "                          label=f'Best: {study.best_value:.4f}')\n",
    "        axes[1, 1].set_xlabel('CV AUC Score')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].set_title('Score Distribution')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'lightgbm_optimization_plots.png'),\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_optimization_summary(results, output_dir):\n",
    "    \"\"\"\n",
    "    Save a human-readable summary of optimization results.\n",
    "    \"\"\"\n",
    "    summary_path = os.path.join(output_dir, 'lightgbm_optimization_summary.txt')\n",
    "\n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(\"LightGBM Hyperparameter Optimization Summary\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "        f.write(f\"Optimization Configuration:\\n\")\n",
    "        f.write(f\"  Number of trials: {results['n_trials']}\\n\")\n",
    "        f.write(f\"  CV folds: {results['cv_folds']}\\n\")\n",
    "        f.write(f\"  Features used: {results['feature_count']:,}\\n\")\n",
    "        f.write(f\"  Scale pos weight: {results['scale_pos_weight']:.6f}\\n\")\n",
    "        f.write(f\"  Imbalance ratio: {results['imbalance_ratio']:.2f}:1\\n\\n\")\n",
    "\n",
    "        f.write(f\"Best Results:\\n\")\n",
    "        f.write(f\"  CV AUC Score: {results['best_cv_score']:.6f}\\n\\n\")\n",
    "\n",
    "        f.write(f\"Test Set Performance:\\n\")\n",
    "        for metric, value in results['test_results'].items():\n",
    "            if metric not in ['model_path', 'predictions']:\n",
    "                f.write(f\"  {metric.upper()}: {value:.6f}\\n\")\n",
    "        f.write(f\"\\nModel saved to: {results['test_results']['model_path']}\\n\\n\")\n",
    "\n",
    "        f.write(f\"Best Hyperparameters:\\n\")\n",
    "        for param, value in results['best_params'].items():\n",
    "            if isinstance(value, float):\n",
    "                f.write(f\"  {param}: {value:.6f}\\n\")\n",
    "            else:\n",
    "                f.write(f\"  {param}: {value}\\n\")\n",
    "\n",
    "    print(f\"   üìÑ Summary saved to: {summary_path}\")\n",
    "\n",
    "\n",
    "def load_optimization_results(results_path=\"data/splits/lightgbm_optimization_results.pkl\"):\n",
    "    \"\"\"\n",
    "    Load previously saved optimization results.\n",
    "\n",
    "    Args:\n",
    "        results_path (str): Path to saved results\n",
    "\n",
    "    Returns:\n",
    "        dict: Optimization results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(results_path, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "\n",
    "        print(f\"‚úÖ Loaded optimization results from: {results_path}\")\n",
    "        print(f\"   Best CV AUC: {results['best_cv_score']:.4f}\")\n",
    "        print(f\"   Test AUC: {results['test_results']['auc']:.4f}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Results file not found: {results_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_results = optimize_lightgbm_with_workflow(\n",
    "    selected_paths_conservative,\n",
    "    class_weights_path=\"data/splits/class_weights.pkl\",\n",
    "    n_trials=50,\n",
    "    cv_folds=5,\n",
    "    random_state=42,\n",
    "    output_dir=\"data/splits\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüéâ Optimization completed!\")\n",
    "print(f\"üìä Best CV AUC: {optimization_results['best_cv_score']:.4f}\")\n",
    "print(f\"üéØ Test AUC: {optimization_results['test_results']['auc']:.4f}\")\n",
    "print(f\"üìÅ All results saved to: data/splits/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_suspicious_results(optimization_results, selected_paths_conservative):\n",
    "    \"\"\"\n",
    "    Investigate potential data leakage, batch effects, or other issues causing unrealistic performance.\n",
    "    \"\"\"\n",
    "    print(\"=== INVESTIGATING SUSPICIOUS RESULTS ===\")\n",
    "    print(\"Checking for potential data leakage, batch effects, or other issues...\")\n",
    "\n",
    "    # Load the data to investigate\n",
    "    X_train = pd.read_parquet(selected_paths_conservative['X_train'])\n",
    "    X_test = pd.read_parquet(selected_paths_conservative['X_test'])\n",
    "    y_train = pd.read_parquet(selected_paths_conservative['y_train'])['condition']\n",
    "    y_test = pd.read_parquet(selected_paths_conservative['y_test'])['condition']\n",
    "\n",
    "    print(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
    "\n",
    "    # 1. Check for identical samples between train/test\n",
    "    print(\"\\nüîç 1. CHECKING FOR IDENTICAL SAMPLES BETWEEN TRAIN/TEST\")\n",
    "\n",
    "    # Check if any training samples are identical to test samples\n",
    "    identical_samples = 0\n",
    "    sample_correlations = []\n",
    "\n",
    "    for i, test_sample in enumerate(X_test.values[:10]):  # Check first 10 test samples\n",
    "        for j, train_sample in enumerate(X_train.values):\n",
    "            correlation = np.corrcoef(test_sample, train_sample)[0, 1]\n",
    "            if correlation > 0.99:  # Very high correlation\n",
    "                identical_samples += 1\n",
    "                sample_correlations.append(correlation)\n",
    "                if identical_samples <= 3:  # Show first few\n",
    "                    print(f\"   High correlation ({correlation:.6f}) between test sample {i} and train sample {j}\")\n",
    "\n",
    "    if identical_samples > 0:\n",
    "        print(f\"   üö® FOUND {identical_samples} highly correlated sample pairs!\")\n",
    "        print(f\"   Average correlation: {np.mean(sample_correlations):.6f}\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ No identical samples found between train/test\")\n",
    "\n",
    "    # 2. Check feature distributions\n",
    "    print(\"\\nüîç 2. CHECKING FEATURE DISTRIBUTIONS\")\n",
    "\n",
    "    # Compare feature means between train/test\n",
    "    train_means = X_train.mean()\n",
    "    test_means = X_test.mean()\n",
    "    mean_differences = np.abs(train_means - test_means)\n",
    "\n",
    "    print(f\"   Mean absolute difference in feature means: {mean_differences.mean():.6f}\")\n",
    "    print(f\"   Max difference in feature means: {mean_differences.max():.6f}\")\n",
    "    print(f\"   Features with >0.1 difference: {(mean_differences > 0.1).sum()}\")\n",
    "\n",
    "    # 3. Check class distributions\n",
    "    print(\"\\nüîç 3. CHECKING CLASS DISTRIBUTIONS\")\n",
    "\n",
    "    train_class_dist = y_train.value_counts(normalize=True).sort_index()\n",
    "    test_class_dist = y_test.value_counts(normalize=True).sort_index()\n",
    "\n",
    "    print(f\"   Train class distribution: {train_class_dist.to_dict()}\")\n",
    "    print(f\"   Test class distribution: {test_class_dist.to_dict()}\")\n",
    "\n",
    "    class_diff = np.abs(train_class_dist - test_class_dist)\n",
    "    print(f\"   Class distribution difference: {class_diff.to_dict()}\")\n",
    "\n",
    "    # 4. Check for perfect separation features\n",
    "    print(\"\\nüîç 4. CHECKING FOR PERFECT SEPARATION FEATURES\")\n",
    "\n",
    "    # Find features that perfectly separate classes\n",
    "    perfect_features = []\n",
    "    for col in X_train.columns[:100]:  # Check first 100 features\n",
    "        train_healthy = X_train[y_train == 0][col]\n",
    "        train_unhealthy = X_train[y_train == 1][col]\n",
    "\n",
    "        # Check if ranges don't overlap\n",
    "        healthy_max = train_healthy.max()\n",
    "        healthy_min = train_healthy.min()\n",
    "        unhealthy_max = train_unhealthy.max()\n",
    "        unhealthy_min = train_unhealthy.min()\n",
    "\n",
    "        # Perfect separation if no overlap\n",
    "        if (healthy_max < unhealthy_min) or (unhealthy_max < healthy_min):\n",
    "            perfect_features.append(col)\n",
    "\n",
    "    if perfect_features:\n",
    "        print(f\"   üö® FOUND {len(perfect_features)} features with perfect class separation!\")\n",
    "        print(f\"   First few: {perfect_features[:5]}\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ No features with perfect class separation found\")\n",
    "\n",
    "    # 5. Simple baseline check\n",
    "    print(\"\\nüîç 5. SIMPLE BASELINE PERFORMANCE CHECK\")\n",
    "\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    # Majority class baseline\n",
    "    dummy_maj = DummyClassifier(strategy='most_frequent')\n",
    "    dummy_maj.fit(X_train, y_train)\n",
    "    dummy_pred = dummy_maj.predict(X_test)\n",
    "    dummy_acc = (dummy_pred == y_test).mean()\n",
    "\n",
    "    print(f\"   Majority class baseline accuracy: {dummy_acc:.4f}\")\n",
    "    print(f\"   Our model accuracy: {optimization_results['test_results']['accuracy']:.4f}\")\n",
    "    print(f\"   Improvement over baseline: {optimization_results['test_results']['accuracy'] - dummy_acc:.4f}\")\n",
    "\n",
    "    # 6. Check sample sizes vs feature count\n",
    "    print(\"\\nüîç 6. SAMPLE SIZE vs FEATURE COUNT\")\n",
    "\n",
    "    n_samples = len(X_train)\n",
    "    n_features = X_train.shape[1]\n",
    "    ratio = n_samples / n_features\n",
    "\n",
    "    print(f\"   Training samples: {n_samples}\")\n",
    "    print(f\"   Features: {n_features}\")\n",
    "    print(f\"   Sample-to-feature ratio: {ratio:.2f}\")\n",
    "\n",
    "    if ratio < 2:\n",
    "        print(\"   üö® WARNING: Very low sample-to-feature ratio! Risk of overfitting!\")\n",
    "    elif ratio < 5:\n",
    "        print(\"   ‚ö†Ô∏è  Low sample-to-feature ratio. Consider more regularization.\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Reasonable sample-to-feature ratio\")\n",
    "\n",
    "    # 7. Cross-validation consistency check\n",
    "    print(\"\\nüîç 7. CV vs TEST PERFORMANCE GAP\")\n",
    "\n",
    "    cv_auc = optimization_results['best_cv_score']\n",
    "    test_auc = optimization_results['test_results']['auc']\n",
    "    performance_gap = abs(cv_auc - test_auc)\n",
    "\n",
    "    print(f\"   CV AUC: {cv_auc:.6f}\")\n",
    "    print(f\"   Test AUC: {test_auc:.6f}\")\n",
    "    print(f\"   Performance gap: {performance_gap:.6f}\")\n",
    "\n",
    "    if performance_gap > 0.05:\n",
    "        print(\"   üö® Large performance gap! Possible overfitting or data leakage!\")\n",
    "    elif performance_gap > 0.02:\n",
    "        print(\"   ‚ö†Ô∏è  Moderate performance gap. Monitor for overfitting.\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Good CV-Test consistency\")\n",
    "\n",
    "    # Summary and recommendations\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INVESTIGATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    issues_found = []\n",
    "\n",
    "    if identical_samples > 0:\n",
    "        issues_found.append(\"Identical/highly similar samples between train/test\")\n",
    "\n",
    "    if len(perfect_features) > 0:\n",
    "        issues_found.append(\"Features with perfect class separation\")\n",
    "\n",
    "    if ratio < 2:\n",
    "        issues_found.append(\"Extremely low sample-to-feature ratio\")\n",
    "\n",
    "    if performance_gap > 0.05:\n",
    "        issues_found.append(\"Large CV-Test performance gap\")\n",
    "\n",
    "    if not issues_found:\n",
    "        print(\"‚úÖ No obvious data leakage issues detected\")\n",
    "        print(\"   The high performance might be legitimate for this dataset\")\n",
    "        print(\"   Consider:\")\n",
    "        print(\"   ‚Ä¢ Validating on external datasets\")\n",
    "        print(\"   ‚Ä¢ Checking biological plausibility of selected features\")\n",
    "        print(\"   ‚Ä¢ Running additional cross-validation schemes\")\n",
    "    else:\n",
    "        print(\"üö® POTENTIAL ISSUES DETECTED:\")\n",
    "        for issue in issues_found:\n",
    "            print(f\"   ‚Ä¢ {issue}\")\n",
    "        print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "        print(\"   ‚Ä¢ Re-examine data preprocessing pipeline\")\n",
    "        print(\"   ‚Ä¢ Check for sample ID overlaps in original data\")\n",
    "        print(\"   ‚Ä¢ Consider more aggressive feature selection\")\n",
    "        print(\"   ‚Ä¢ Validate with external datasets\")\n",
    "        print(\"   ‚Ä¢ Use nested cross-validation\")\n",
    "\n",
    "    return {\n",
    "        'identical_samples': identical_samples,\n",
    "        'perfect_features': perfect_features,\n",
    "        'sample_feature_ratio': ratio,\n",
    "        'performance_gap': performance_gap,\n",
    "        'issues_detected': len(issues_found) > 0\n",
    "    }\n",
    "\n",
    "\n",
    "def check_original_sample_ids():\n",
    "    \"\"\"\n",
    "    Check for potential sample ID overlaps in the original datasets.\n",
    "    \"\"\"\n",
    "    print(\"\\nüîç CHECKING ORIGINAL SAMPLE IDs\")\n",
    "\n",
    "    try:\n",
    "        # Load original processed data to check sample IDs\n",
    "        healthy_df = pd.read_parquet('data/healthy_data_preprocessed.pq')\n",
    "        unhealthy_df = pd.read_parquet('data/unhealthy_data_preprocessed.pq')\n",
    "\n",
    "        print(f\"Healthy samples: {len(healthy_df)}\")\n",
    "        print(f\"Unhealthy samples: {len(unhealthy_df)}\")\n",
    "\n",
    "        # Check for overlapping sample IDs\n",
    "        healthy_ids = set(healthy_df.index)\n",
    "        unhealthy_ids = set(unhealthy_df.index)\n",
    "\n",
    "        overlap = healthy_ids & unhealthy_ids\n",
    "\n",
    "        if overlap:\n",
    "            print(f\"üö® FOUND {len(overlap)} overlapping sample IDs!\")\n",
    "            print(f\"First few overlapping IDs: {list(overlap)[:5]}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚úÖ No overlapping sample IDs found\")\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not check sample IDs: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def simple_sanity_check():\n",
    "    \"\"\"\n",
    "    Perform a simple sanity check with a basic model.\n",
    "    \"\"\"\n",
    "    print(\"\\nüîç SIMPLE SANITY CHECK WITH BASIC MODEL\")\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    # Load the data\n",
    "    X_train = pd.read_parquet(selected_paths_conservative['X_train'])\n",
    "    X_test = pd.read_parquet(selected_paths_conservative['X_test'])\n",
    "    y_train = pd.read_parquet(selected_paths_conservative['y_train'])['condition']\n",
    "    y_test = pd.read_parquet(selected_paths_conservative['y_test'])['condition']\n",
    "\n",
    "    # Simple logistic regression\n",
    "    lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    lr_pred_proba = lr.predict_proba(X_test)[:, 1]\n",
    "    lr_auc = roc_auc_score(y_test, lr_pred_proba)\n",
    "\n",
    "    print(f\"   Simple Logistic Regression AUC: {lr_auc:.4f}\")\n",
    "    print(f\"   LightGBM AUC: {optimization_results['test_results']['auc']:.4f}\")\n",
    "\n",
    "    if lr_auc > 0.95:\n",
    "        print(\"   üö® Even simple model achieves very high performance!\")\n",
    "        print(\"   This suggests the problem might be too easy (possible data issues)\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Simple model shows more realistic performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive investigation\n",
    "investigation_results = investigate_suspicious_results(optimization_results, selected_paths_conservative)\n",
    "\n",
    "# Check sample ID overlaps\n",
    "sample_overlap = check_original_sample_ids()\n",
    "\n",
    "# Simple sanity check\n",
    "simple_sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
